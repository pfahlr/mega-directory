OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a7877-2b41-7ea0-ba91-749c2a0dbce4
--------
user
Implement the functionality described in codex/TASKS/35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml, then verify with any available tests.
ERROR: MCP client for `exa` failed to start: handshaking with MCP server failed: connection closed: initialize response
ERROR: MCP client for `vibe-check-mcp-server` failed to start: handshaking with MCP server failed: connection closed: initialize response
ERROR: MCP client for `mcp` failed to start: handshaking with MCP server failed: connection closed: initialize response

thinking
**Preparing to explore instructions and tasks**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 335ms:
AGENTS.md
apps
codex
codex.toml
codex.yaml
db
docker-compose.yml
docs
docs.tar
env.json
Makefile
package.json
packages
README.md
scripts
tests
tsconfig.json

thinking
**Opening task instructions file**
exec
bash -lc "sed -n '1,200p' codex/TASKS/35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml" in /var/home/rick/Development/mega-directory succeeded in 43ms:
id: 35
filename: 35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml
title: Implement Shared Post-Processing for Crawler and Text Import Scripts
description: >
Create shared Python module to:
- Enrich listings with AI-generated short/long descriptions
- Clean titles and normalize data
- To be reused across ingestion sources (crawler, manual import)
- additional information regarding entire development project this task is part of can be referenced in `codex/TASKS/codex_phase_1b_2.yaml`
dependencies: []

thinking
**Exploring repo structure and tasks**
exec
bash -lc 'ls codex/TASKS' in /var/home/rick/Development/mega-directory succeeded in 41ms:
00_prepare_repository.yaml
01_initialize_astro_frontend.yaml
02_setup_api_server_with_express_and_jwt_auth.yaml
03_design_postgresql_schema.yaml
04_implement_listing_ingestion_api_endpoint.yaml
05_admin_auth_route_and_middleware.yaml
06_frontend_directory_page_rendering.yaml
07_frontend_subcategory_ui_and_layout.yaml
08_setup_featured_listings_logic.yaml
09_add_category_meta_and_seo_controls.yaml
10_scaffold_python_crawler_agent.yaml
11_llm_field_generation_via_jinja2.yaml
12_post_listings_from_crawler_to_api_server.yaml
13_create_json_crawler_configuration_schema.yaml
14_scaffold_admin_ui_application.yaml
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml

thinking
**Exploring apps directory**
exec
bash -lc 'ls apps' in /var/home/rick/Development/mega-directory succeeded in 41ms:
admin
api
crawler
web

thinking
**Examining crawler directory**
exec
bash -lc 'ls apps/crawler' in /var/home/rick/Development/mega-directory succeeded in 40ms:
config
crawler.py
dev_runner.py
main.py
__pycache__
requirements-dev.txt
targets.example.json
tests

thinking
**Opening main files for inspection**
exec
bash -lc "sed -n '1,200p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 79ms:
from __future__ import annotations

from dataclasses import dataclass, field
import logging
import os
import re
from typing import Any, Callable, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup, Tag
from jinja2 import Environment, StrictUndefined, TemplateError

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

thinking
**Continuing to review large file**
exec
bash -lc "sed -n '200,400p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
    def _resolve_scrape_field(
        self,
        field_name: str,
        config: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Any:
        attribute = config.get("attribute") or field_name
        listing_data = context["listing"]
        value = self._lookup_value(listing_data, attribute)
        if value is not None:
            return value
        return self._lookup_value(context, attribute)

    def _generate_ai_field(
        self,
        field_name: str,
        config: Dict[str, Any],
        context: Dict[str, Any],
        target: Dict[str, Any],
    ) -> Any:

thinking
**Proceeding with task**
exec
bash -lc "sed -n '400,800p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 43ms:
                listing.categories = categories
                listing.category_slug = categories[0]
            listings.append(listing)

        return listings

    @staticmethod
    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
        text_parts = [part for part in parts if isinstance(part, str) and part]
        element_text = element.get_text(" ", strip=True)
        if element_text:
            text_parts.append(element_text)
        return " ".join(text_parts).strip()

    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
        selectors: Dict[str, str] = {}
        if isinstance(config, dict):
            for key, selector in config.items():
                if key not in LOCATION_FIELDS or not isinstance(selector, str):
                    continue

thinking
**Planning file continuation retrieval**
exec
bash -lc "sed -n '800,1200p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 41ms:

        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
        normalized_category = self._slugify(category_slug)
        if not normalized_category:
            raise ValueError("Unable to determine category slug for API payload")
        payload["categorySlug"] = normalized_category
        listing.category_slug = normalized_category
        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
            listing.categories.append(normalized_category)

        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
        if source_name:
            payload["sourceName"] = source_name

        location_payload = payload.get("location")
        if not isinstance(location_payload, dict):
            location_payload = listing.location
        normalized_location = self._normalize_location(location_payload)
        if normalized_location:
            payload["location"] = normalized_location

thinking
**Planning to read specific file section**
exec
bash -lc "sed -n '600,800p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 58ms:
        request = LLMRequest(
            provider=provider,
            model=model,
            prompt=prompt,
            field_name="category_classifier",
            options=classifier.get("options", {}),
            target=target,
            listing=context.get("listing"),
        )
        response = self.field_generator.llm_client(request)
        return self._resolve_classifier_slug(response, choices)

    def _resolve_classifier_slug(
        self,
        response: Any,
        choices: List[str],
    ) -> Optional[str]:
        normalized = self._normalize_string(response)
        if not normalized:
            return None

thinking
**Searching for text import script**
exec
bash -lc 'ls scripts' in /var/home/rick/Development/mega-directory succeeded in 41ms:
dev-bootstrap.sh
dev_bootstrap.sh

thinking
**Searching repository for text import**
exec
bash -lc 'rg -n "text import" -n' in /var/home/rick/Development/mega-directory succeeded in 789ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:235:**Searching repository for text import**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:237:bash -lc 'rg -n "text import" -n' in /var/home/rick/Development/mega-directory

thinking
**Exploring logs and test scripts**
exec
bash -lc 'ls apps/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 43ms:
__pycache__
test_config_schema.py
test_crawler.py

thinking
**Checking test_crawler.py for features**
exec
bash -lc "sed -n '1,200p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 42ms:
from pathlib import Path
from types import SimpleNamespace
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

import crawler
from crawler import run_crawler


SAMPLE_HTML = """
<section>
  <article data-listing>
    <a class="listing-link" href="https://example.com/acme">
      <span class="listing-title">ACME Electric</span>
    </a>

thinking
**Reviewing remaining tests**
exec
bash -lc "sed -n '200,400p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 43ms:


def test_parse_listings_extracts_addresses_and_category_rules() -> None:
    target = {
        "category": "Services",
        "locations": ["Midwest"],
        "address_selectors": {
            "addressLine1": ".address .street",
            "city": ".address .city",
            "region": ".address .region",
        },
        "address_patterns": [
            {
                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
            }
        ],
        "category_rules": [
            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
        ],

thinking
**Continuing process**
exec
bash -lc "sed -n '400,800p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
        "api_endpoint": "https://api.example.com/v1/crawler/listings",
        "api_token": "crawler-token",
        "targets": [
            {
                "category": "Services",
                "locations": ["Remote"],
                "category_rules": [
                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
                ],
                "category_classifier": {
                    "provider": "openai",
                    "model": "gpt-4o-mini",
                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
                    "choices": ["photographers", "builders"],
                },
            }
        ],
    }
    session = DummySession(ADDRESS_HTML)

thinking
**Continuing to review test file**
exec
bash -lc "sed -n '800,1200p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 42ms:


thinking
**Planning shared post-processing module**
exec
bash -lc 'rg -n "text" apps -g "*.py"' in /var/home/rick/Development/mega-directory succeeded in 47ms:
apps/crawler/dev_runner.py:11:import textwrap
apps/crawler/dev_runner.py:20:SAMPLE_DIRECTORY_HTML = textwrap.dedent(
apps/crawler/dev_runner.py:50:        self.text = html
apps/crawler/dev_runner.py:102:    return fixture_path.read_text(encoding="utf-8")
apps/crawler/tests/test_crawler.py:68:    def __init__(self, text: str) -> None:
apps/crawler/tests/test_crawler.py:69:        self.text = text
apps/crawler/tests/test_crawler.py:76:    def __init__(self, text: str) -> None:
apps/crawler/tests/test_crawler.py:77:        self.text = text
apps/crawler/tests/test_crawler.py:83:        return DummyResponse(self.text)
apps/crawler/tests/test_crawler.py:290:                    "display_link": {"source": "scrape", "attribute": "link_text"},
apps/crawler/tests/test_crawler.py:295:                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
apps/crawler/crawler.py:123:        context = self.build_context(listing, target, batch)
apps/crawler/crawler.py:129:                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
apps/crawler/crawler.py:131:                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
apps/crawler/crawler.py:137:    def build_context(
apps/crawler/crawler.py:143:        return self._build_context(listing, target, batch)
apps/crawler/crawler.py:145:    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
apps/crawler/crawler.py:146:        return self._render_template(label, template, context)
apps/crawler/crawler.py:148:    def _build_context(
apps/crawler/crawler.py:154:        listing_context: Dict[str, Any] = {

thinking
**Locating text import script**
exec
bash -lc "sed -n '1,200p' apps/crawler/main.py" in /var/home/rick/Development/mega-directory succeeded in 43ms:
import json
from crawler import run_crawler

if __name__ == "__main__":
    with open("config/targets.json", "r") as f:
        config = json.load(f)
    run_crawler(config)

thinking
**Searching for text import script**
exec
bash -lc 'rg -n "text import" -g "*.md"' in /var/home/rick/Development/mega-directory exited 1 in 51ms:


thinking
**Searching for import scripts**
exec
bash -lc 'rg -n "import script"' in /var/home/rick/Development/mega-directory succeeded in 52ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:238:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:357:**Locating text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:369:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:375:**Searching for import scripts**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:377:bash -lc 'rg -n "import script"' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log:243:I’m mapping out how to add location tables and related seeding functionality, starting with analyzing the repo’s frameworks and schema. I want to understand how `apps/api` might use Express, then check for existing location models or services, and look into Prisma migrations to plan database updates and data import scripts. I'll explore references to `Country` and `Location` to connect the dots before implementing and testing the changes.
codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log:723:codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log:243:I’m mapping out how to add location tables and related seeding functionality, starting with analyzing the repo’s frameworks and schema. I want to understand how `apps/api` might use Express, then check for existing location models or services, and look into Prisma migrations to plan database updates and data import scripts. I'll explore references to `Country` and `Location` to connect the dots before implementing and testing the changes.

thinking
**Checking task log for details**
exec
bash -lc "sed -n '200,360p' codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log" in /var/home/rick/Development/mega-directory succeeded in 40ms:
            payload["location"] = normalized_location

thinking
**Planning to read specific file section**
exec
bash -lc "sed -n '600,800p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 58ms:
        request = LLMRequest(
            provider=provider,
            model=model,
            prompt=prompt,
            field_name="category_classifier",
            options=classifier.get("options", {}),
            target=target,
            listing=context.get("listing"),
        )
        response = self.field_generator.llm_client(request)
        return self._resolve_classifier_slug(response, choices)

    def _resolve_classifier_slug(
        self,

thinking
**Searching for text import script**
exec
bash -lc "sed -n '1,200p' apps/crawler/dev_runner.py" in /var/home/rick/Development/mega-directory succeeded in 42ms:
#!/usr/bin/env python3

"""Developer-friendly crawler loop that replays sample HTML into the local API."""

from __future__ import annotations

import argparse
import logging
import os
from pathlib import Path
import textwrap
import time
from typing import Any, Dict

import requests

from crawler import run_crawler


SAMPLE_DIRECTORY_HTML = textwrap.dedent(

thinking
**Searching for text import script**
exec
bash -lc 'rg -n "text import" -g "*.ts"' in /var/home/rick/Development/mega-directory exited 1 in 48ms:


thinking
**Searching for data import script instructions**
exec
bash -lc "sed -n '1,200p' codex/TASKS/36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml" in /var/home/rick/Development/mega-directory succeeded in 46ms:
id: 36
filename: 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
title: Create Text-Based Data Import Script with LLM and HTML Parsing Modes
description: >
- Develop CLI script that accepts data inputs via file/stdin and mode flag (--format html|text|json|csv).
- Route parsing through BS4 (html), or LLM-based extractors (text).
- Output intermediate JSON file for manual review or later enrichment.
- when --formmat is json|csv then this data should be ingested, enriched, and stored to the database via the api
- additional information regarding entire development project this task is part of can be referenced in `codex/TASKS/codex_phase_1b_2.yaml`
dependencies: []

thinking
**Searching for text import scripts**
exec
bash -lc 'rg -n "text-based" -n' in /var/home/rick/Development/mega-directory succeeded in 48ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:446:bash -lc "sed -n '1,200p' codex/TASKS/36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml" in /var/home/rick/Development/mega-directory succeeded in 46ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:448:filename: 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:461:bash -lc 'rg -n "text-based" -n' in /var/home/rick/Development/mega-directory
codex/TASKS/work_list:14:36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
codex/TASKS/36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml:2:filename: 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
codex/LOGS/codex_run_2025-11-12_06-13-21.log:7:Tasks: 24_setup_project_scaffolding_and_configuration 25a_initialize_astro_frontend_with_base_layouts 25_define_directory_page_model_and_metadata_fields 26_initialize_geographic_location_tables_and_seed_data 27_implement_directory_pages_table_and_metadata_fields 28_build_admin_ui_for_directory_page_creation_and_editing 29_implement_listings_table_enhancements_for_multiple_categories_and_locations 30_set_up_geocoding_integration_with_fallback_support 31_build_map_widget_for_directory_pages_with_location-aware_listings 32_create_admin_api_endpoints_for_listings_addresses_categories_directories 33_add_directory_routing_logic_subdomain_subdirectory_handling 34_extend_crawler_to_handle_address_and_category_assignment 35_implement_shared_post-processing_for_crawler_and_text_import_scripts 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes 37_generate_admin_interface_for_reviewing_imported_json_listing_data 39_render_canonical_meta_tags_from_directory_configuration 40_document_full_development_setup_and_env_var_usage_makefile_sops_etc
codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log:575:codex/LOGS/codex_run_2025-11-12_06-13-21.log:7:Tasks: 24_setup_project_scaffolding_and_configuration 25a_initialize_astro_frontend_with_base_layouts 25_define_directory_page_model_and_metadata_fields 26_initialize_geographic_location_tables_and_seed_data 27_implement_directory_pages_table_and_metadata_fields 28_build_admin_ui_for_directory_page_creation_and_editing 29_implement_listings_table_enhancements_for_multiple_categories_and_locations 30_set_up_geocoding_integration_with_fallback_support 31_build_map_widget_for_directory_pages_with_location-aware_listings 32_create_admin_api_endpoints_for_listings_addresses_categories_directories 33_add_directory_routing_logic_subdomain_subdirectory_handling 34_extend_crawler_to_handle_address_and_category_assignment 35_implement_shared_post-processing_for_crawler_and_text_import_scripts 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes 37_generate_admin_interface_for_reviewing_imported_json_listing_data 39_render_canonical_meta_tags_from_directory_configuration 40_document_full_development_setup_and_env_var_usage_makefile_sops_etc

thinking
**Investigating text import script presence**
exec
bash -lc 'rg -n "import" apps -g "*text*"' in /var/home/rick/Development/mega-directory exited 1 in 46ms:


thinking
**Searching for text-related files**
exec
bash -lc "rg --files -g '*text*'" in /var/home/rick/Development/mega-directory succeeded in 45ms:
codex/TASKS/35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml
codex/TASKS/36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log

thinking
**Evaluating location for shared post-processing module**
exec
bash -lc 'rg -n "ingest" -n' in /var/home/rick/Development/mega-directory succeeded in 49ms:
codex.yaml:12:    path: codex/TASKS/04_implement_listing_ingestion_api_endpoint.yaml
docs/monitoring.md:23:- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
apps/crawler/dev_runner.py:125:        help="Crawler ingestion endpoint (defaults to http://localhost:3001/v1/crawler/listings).",
docs/deployment/railway.md:53:| `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
apps/crawler/crawler.py:741:        payloads = self._build_ingestion_payloads(batch, target_config)
apps/crawler/crawler.py:767:    def _build_ingestion_payloads(
db/migrations/001_core_schema.sql:130:  "ingestedAt" TIMESTAMPTZ DEFAULT NOW(),
db/schema.prisma:293:  ingestedAt       DateTime?        @default(now())
db/seed.ts:521:      ingestedAt: new Date(),
db/seed.ts:612:      ingestedAt: new Date(),
db/seed.ts:676:      ingestedAt: new Date(),
README.md:118:  /api        - TypeScript Express API, health checks, ingestion endpoints
apps/api/src/server.ts:46:  ingestedAt: string;
apps/api/src/server.ts:170:  ingestionStore: ListingStore;
apps/api/src/server.ts:251:  locals.ingestionStore = createListingStore();
apps/api/src/server.ts:416:        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
apps/api/src/server.ts:444:          event: 'listings.ingest.invalid',
apps/api/src/server.ts:476:    const store = locals.ingestionStore;
apps/api/src/server.ts:483:        event: 'listings.ingested',
apps/api/src/server.ts:484:        ingestedCount: savedRecords.length,

thinking
**Locating text import and ingestion logic**
exec
bash -lc "sed -n '1,240p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 41ms:
import express, { type Express, type Request, type RequestHandler, type Response } from 'express';
import jwt, { type JwtPayload } from 'jsonwebtoken';
import { DEFAULT_PORTS, PROJECT_NAME } from '@mega-directory/config';
import { geocodeListingLocation, type GeocodingAddress } from './geocoding';
import { createLogger, createRequestLogger, type Logger } from './logger';

const DEFAULT_PORT = DEFAULT_PORTS.api;
const DEFAULT_LISTING_STATUS: ListingStatus = 'INACTIVE';
const MAX_SLUG_LENGTH = 80;
const DEFAULT_ADMIN_TOKEN_TTL_SECONDS = 60 * 15;

type ExpressApp = ReturnType<typeof express>;

type ListingStatus = 'INACTIVE' | 'ACTIVE' | 'PENDING' | 'APPROVED' | 'REJECTED';

type DirectoryStatus = 'DRAFT' | 'ACTIVE' | 'ARCHIVED';

type NullableString = string | null;

type ListingLocation =

thinking
**Searching for ingestion endpoint**
exec
bash -lc 'rg -n "crawler/listings" -n apps/api/src/server.ts' in /var/home/rick/Development/mega-directory succeeded in 41ms:
284:  app.post('/v1/crawler/listings', crawlerAuth, createListingIngestionHandler(app));

thinking
**Extracting specific lines for review**
exec
bash -lc "sed -n '280,520p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 43ms:
  app.post('/v1/crawler/ping', crawlerAuth, (_req, res) => {
    res.json({ status: 'crawler-ok' });
  });

  app.post('/v1/crawler/listings', crawlerAuth, createListingIngestionHandler(app));

  registerAdminRoutes(app, adminAuth);

  return app;
}

export function startServer() {
  const app = createServer();
  const { port } = getAppLocals(app).config;
  app.listen(port, () => {
    getAppLocals(app).logger.info(
      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
      `API server running at http://localhost:${port}`
    );
  });

thinking
**Designing post-processing module**
exec
bash -lc 'rg -n "shortDescription" -n' in /var/home/rick/Development/mega-directory succeeded in 51ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:568:bash -lc 'rg -n "shortDescription" -n' in /var/home/rick/Development/mega-directory

thinking
**Inspecting database schema for Listing model**
exec
bash -lc 'rg -n "model Listing" -n db/schema.prisma' in /var/home/rick/Development/mega-directory succeeded in 41ms:
269:model Listing {
327:model ListingCategory {
339:model ListingAddress {
370:model ListingSubcategory {

thinking
**Extracting schema.prisma lines**
exec
bash -lc "sed -n '269,340p' db/schema.prisma" in /var/home/rick/Development/mega-directory succeeded in 40ms:
model Listing {
  id               Int              @id @default(autoincrement())
  title            String
  slug             String           @unique
  tagline          String?
  summary          String?
  description      String?
  websiteUrl       String?
  contactEmail     String?
  contactPhone     String?
  priceRange       String?
  rating           Float?           @default(0)
  reviewCount      Int?             @default(0)
  score            Float?           @default(0)
  status           ListingStatus    @default(PENDING)
  isClaimed        Boolean          @default(false)
  isSponsored      Boolean          @default(false)
  sourceName       String?
  sourceUrl        String?
  sourceId         String?          @unique

thinking
**Mapping description fields and inspecting API validation**
exec
bash -lc "sed -n '520,820p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 40ms:
    return { valid: false, errors: ['Each listing must be an object'] };
  }

  const title = sanitizeNullableString(payload.title);
  if (!title) {
    errors.push('title is required');
  }

  const categorySlug = sanitizeNullableString(payload.categorySlug);
  if (!categorySlug) {
    errors.push('categorySlug is required');
  }

  const slugSource = sanitizeNullableString(payload.slug) ?? title;
  const slug = slugify(slugSource || '');
  if (!slug) {
    errors.push('slug is required');
  }

  const websiteUrl = optionalUrl(payload.websiteUrl, 'websiteUrl', errors);
Reconnecting... 1/5

thinking
**Preparing to design new module**
exec
bash -lc 'ls apps/api/src' in /var/home/rick/Development/mega-directory succeeded in 40ms:
geocoding.ts
logger.ts
server.ts

thinking
**Checking the packages directory**
exec
bash -lc 'ls packages' in /var/home/rick/Development/mega-directory succeeded in 40ms:
shared-config

thinking
**Identifying contents**
exec
bash -lc 'ls packages/shared-config' in /var/home/rick/Development/mega-directory succeeded in 41ms:
index.d.ts
index.js
package.json
README.md

thinking
**Planning post-processing module creation**
exec
bash -lc "sed -n '700,880p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
                if not endpoint or not token:
                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
                timeout = self._coerce_positive_timeout(entry.get("timeout"))
                resolved.append(
                    APITarget(
                        name=str(entry.get("name") or endpoint),
                        endpoint=endpoint,
                        token=token,
                        timeout=timeout,
                    )
                )
        else:
            endpoint = config.get("api_endpoint")
            token = config.get("api_token")
            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
            if endpoint and token:
                resolved.append(
                    APITarget(
                        name="default",

thinking
**Designing shared post-processing module**

I’m thinking through how to create a reusable post-processing module that cleans and enriches listing data by generating AI-powered descriptions. It needs to work smoothly for both the crawler and text import scripts without causing import conflicts. I’m planning to build it under `apps/crawler/post_processing.py` with a class like `ListingPostProcessor` and some utility functions, carefully balancing reuse and independence. I’ll also explore existing prompts and possibly create new LLM prompt templates to handle short and long descriptions. Let’s see what specs or examples I can find in the repo next!
exec
bash -lc 'rg -n "post-processing" -n codex/TASKS/codex_phase_1b_2.yaml' in /var/home/rick/Development/mega-directory exited 1 in 43ms:


thinking
**Inspecting file start for task context**
exec
bash -lc "sed -n '1,200p' codex/TASKS/codex_phase_1b_2.yaml" in /var/home/rick/Development/mega-directory succeeded in 40ms:
project: mega-directory
version: 1.0
phases:
  - phase: 1
    description: Core system buildout — listings pipeline, directory structure, admin tools, crawler, API, and frontend delivery

    components:
      api_server:
        language: TypeScript
        framework: Node/Express
        deployment: Railway
        endpoints:
          - listings: [GET, POST, PUT, DELETE]
          - addresses: [GET, POST, PUT, DELETE]
          - categories: [GET, POST, PUT, DELETE]
          - directory_pages: [GET, POST, PUT, DELETE]
          - directory_metadata: [GET, PUT]
        notes:
          - Postal, city, state/province, and country tables are loaded from external datasets.
          - Listings reference postal_code_id or city_id depending on geography.

thinking
**Checking related task for context**
exec
bash -lc "sed -n '1,200p' codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml" in /var/home/rick/Development/mega-directory succeeded in 43ms:
id: 34
filename: 34_extend_crawler_to_handle_address_and_category_assignment.yaml
title: Extend Crawler to Handle Address and Category Assignment
description: >
- Extend Python crawler logic to support tagging listings with categories and addresses.
- Parse and infer location and category from HTML/CSS selectors or text patterns.
- Attach LLM-assisted classification for edge cases.
- additional information regarding entire development project this task is part of can be referenced in `codex/TASKS/codex_phase_1b_2.yaml`
dependencies: []

thinking
**Checking test_config_schema.py for config details**
exec
bash -lc "sed -n '1,200p' apps/crawler/tests/test_config_schema.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
from __future__ import annotations

import json
from pathlib import Path

import jsonschema
import pytest


AGENT_DIR = Path(__file__).resolve().parents[1]
SCHEMA_PATH = AGENT_DIR / "config" / "targets.schema.json"
EXAMPLE_PATH = AGENT_DIR / "targets.example.json"


@pytest.fixture(scope="module")
def schema() -> dict:
    """Load the crawler configuration schema once per test module."""
    with SCHEMA_PATH.open("r", encoding="utf-8") as fp:
        return json.load(fp)


thinking
**Designing new listing post-processing module**
exec
bash -lc "sed -n '360,720p' codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log" in /var/home/rick/Development/mega-directory succeeded in 43ms:
import json
from crawler import run_crawler

if __name__ == "__main__":
    with open("config/targets.json", "r") as f:
        config = json.load(f)
    run_crawler(config)

thinking
**Searching for text import script**
exec
bash -lc 'rg -n "text import" -g "*.md"' in /var/home/rick/Development/mega-directory exited 1 in 51ms:


thinking
**Searching for import scripts**
exec
bash -lc 'rg -n "import script"' in /var/home/rick/Development/mega-directory succeeded in 52ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:238:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**

thinking
**Designing listing post-processing module**

I’m planning a new module to clean and enrich listing data with optional AI-generated summaries and descriptions, defining clear interfaces and integrating it into the existing crawler workflow while restructuring code to avoid circular imports and maintain test coverage.
exec
bash -lc 'rg -n "post" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 43ms:
24:      "description": "Override timeout (seconds) when posting batches to the API."
180:        "postalCode": { "$ref": "#/$defs/nonEmptyString" },

thinking
**Searching for post-processing references**
exec
bash -lc 'rg -n "post-processing" -n' in /var/home/rick/Development/mega-directory succeeded in 49ms:
codex/TASKS/work_list:13:35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:13:Implement the functionality described in codex/TASKS/35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml, then verify with any available tests.
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:43:bash -lc "sed -n '1,200p' codex/TASKS/35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml" in /var/home/rick/Development/mega-directory succeeded in 43ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:45:filename: 35_implement_shared_post-processing_for_crawler_and_text_import_scripts.yaml
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:238:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:239:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:235:**Searching repository for text import**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:240:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:237:bash -lc 'rg -n "text import" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:332:**Planning shared post-processing module**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:378:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:379:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:238:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:228:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:380:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:357:**Locating text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:381:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:369:**Searching for text import script**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:382:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:375:**Searching for import scripts**
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:383:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:377:bash -lc 'rg -n "import script"' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:390:bash -lc "sed -n '200,360p' codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log" in /var/home/rick/Development/mega-directory succeeded in 40ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:462:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:446:bash -lc "sed -n '1,200p' codex/TASKS/36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml" in /var/home/rick/Development/mega-directory succeeded in 46ms:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:463:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:448:filename: 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes.yaml
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:464:codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:461:bash -lc 'rg -n "text-based" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:467:codex/LOGS/codex_run_2025-11-12_06-13-21.log:7:Tasks: 24_setup_project_scaffolding_and_configuration 25a_initialize_astro_frontend_with_base_layouts 25_define_directory_page_model_and_metadata_fields 26_initialize_geographic_location_tables_and_seed_data 27_implement_directory_pages_table_and_metadata_fields 28_build_admin_ui_for_directory_page_creation_and_editing 29_implement_listings_table_enhancements_for_multiple_categories_and_locations 30_set_up_geocoding_integration_with_fallback_support 31_build_map_widget_for_directory_pages_with_location-aware_listings 32_create_admin_api_endpoints_for_listings_addresses_categories_directories 33_add_directory_routing_logic_subdomain_subdirectory_handling 34_extend_crawler_to_handle_address_and_category_assignment 35_implement_shared_post-processing_for_crawler_and_text_import_scripts 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes 37_generate_admin_interface_for_reviewing_imported_json_listing_data 39_render_canonical_meta_tags_from_directory_configuration 40_document_full_development_setup_and_env_var_usage_makefile_sops_etc
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:468:codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log:575:codex/LOGS/codex_run_2025-11-12_06-13-21.log:7:Tasks: 24_setup_project_scaffolding_and_configuration 25a_initialize_astro_frontend_with_base_layouts 25_define_directory_page_model_and_metadata_fields 26_initialize_geographic_location_tables_and_seed_data 27_implement_directory_pages_table_and_metadata_fields 28_build_admin_ui_for_directory_page_creation_and_editing 29_implement_listings_table_enhancements_for_multiple_categories_and_locations 30_set_up_geocoding_integration_with_fallback_support 31_build_map_widget_for_directory_pages_with_location-aware_listings 32_create_admin_api_endpoints_for_listings_addresses_categories_directories 33_add_directory_routing_logic_subdomain_subdirectory_handling 34_extend_crawler_to_handle_address_and_category_assignment 35_implement_shared_post-processing_for_crawler_and_text_import_scripts 36_create_text-based_data_import_script_with_llm_and_html_parsing_modes 37_generate_admin_interface_for_reviewing_imported_json_listing_data 39_render_canonical_meta_tags_from_directory_configuration 40_document_full_development_setup_and_env_var_usage_makefile_sops_etc

thinking
**Planning post-processing integration**

I’m outlining a module to clean and normalize listing data—removing HTML from titles, unifying keys, and optionally using AI to generate summaries. I’m also thinking through how the crawler will plug in the post-processor, how to manage shared LLM clients, and how to handle config schemas to support customizable prompts. Next, I’ll inspect the targets schema file to see if it needs updating.
exec
bash -lc "sed -n '1,200p' apps/crawler/config/targets.schema.json" in /var/home/rick/Development/mega-directory succeeded in 41ms:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://mega-directory.dev/schemas/crawler-targets.json",
  "title": "Mega Directory Crawler Configuration",
  "type": "object",
  "description": "Schema describing crawler API targets, scraping targets, and optional LLM field generation rules.",
  "additionalProperties": true,
  "properties": {
    "$schema": {
      "type": "string",
      "description": "Optional pointer for tooling to locate this schema definition."
    },
    "api_endpoint": {
      "type": "string",
      "format": "uri",
      "description": "Single API endpoint to POST crawler batches to when api_targets is omitted."
    },
    "api_token": {
      "$ref": "#/$defs/nonEmptyString",
      "description": "Bearer token paired with api_endpoint."

thinking
**Continuing to scroll through config definitions**
exec
bash -lc "sed -n '200,400p' apps/crawler/config/targets.schema.json" in /var/home/rick/Development/mega-directory succeeded in 40ms:
      "required": ["pattern"]
    },
    "categoryRuleList": {
      "type": "array",
      "items": {
        "$ref": "#/$defs/categoryRule"
      }
    },
    "categoryRule": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "slug": { "$ref": "#/$defs/nonEmptyString" },
        "selector": { "$ref": "#/$defs/nonEmptyString" },
        "text": { "$ref": "#/$defs/nonEmptyString" },
        "pattern": { "$ref": "#/$defs/nonEmptyString" },
        "flags": {
          "type": "string",
          "pattern": "^[imsIMS]*$"
        }

thinking
**Designing post-processing schema and tests**

I'm working out the detailed schema additions for a new `post_processing` config to support AI-generated descriptions and title normalization. I’m also thinking through a thorough test-driven plan: first writing failing tests for the new module and the config schema, then implementing the module including integration with the crawler, updating configs and tests, and finally running everything to verify correctness. This should keep things systematic and maintainable.
Plan update
Need coordination between schema, crawler, and new shared module. Starting with tests keeps development aligned with required behavior per Codex instructions.
  → Add failing tests covering shared post-processing behaviors (module unit tests, crawler integration expectations, config schema validation).
  • Implement shared post-processing module plus schema/config updates and integrate into crawler.
  • Run and verify relevant test suites, adjusting as needed.


thinking
**Designing post_processing tests and configs**
thinking
**Preparing new test file for post_processing**
file update
A /var/home/rick/Development/mega-directory/apps/crawler/tests/test_post_processing.py
from __future__ import annotations

from pathlib import Path
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]


def test_processor_normalizes_existing_fields_without_llm() -> None:
    processor = ListingPostProcessor()
    context = PostProcessingContext(
        title="  ACME   Electric  ",
        snippet="Licensed and insured electricians serving the lakeshore.",
        category="Electricians",
        location="Holland, MI",
    )
    payload = {
        "title": "  ACME   Electric  \n",
        "summary": " Trusted  residential   crews.  ",
    }

    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})

    assert processed["title"] == "ACME Electric"
    assert processed["summary"] == "Trusted residential crews."


def test_processor_populates_summary_from_snippet_when_missing() -> None:
    processor = ListingPostProcessor()
    context = PostProcessingContext(
        title="Bright Sparks",
        snippet="Emergency electricians available 24/7 with EV charger installs.",
        category="Electricians",
        location="Grand Rapids, MI",
    )

    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})

    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."


def test_processor_invokes_llm_for_summary_and_description() -> None:
    requests = []

    def fake_llm(request):  # type: ignore[no-untyped-def]
        requests.append(request)
        return f"LLM::{request.field_name}::{request.prompt}"

    processor = ListingPostProcessor(llm_client=fake_llm)
    context = PostProcessingContext(
        title="Bright Sparks",
        snippet="Emergency electricians available 24/7",
        category="Electricians",
        location="Grand Rapids, MI",
    )
    config = {
        "summary": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
        },
        "description": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
        },
    }

    processed = processor.process({"title": "Bright Sparks"}, context, config=config)

    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
    assert processed["description"].startswith("LLM::description::Write a detailed overview")
    assert len(requests) == 2
    assert all(req.provider == "openai" for req in requests)
    assert any("Grand Rapids" in req.prompt for req in requests)


def test_processor_requires_llm_when_configured() -> None:
    processor = ListingPostProcessor(llm_client=None)
    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
    config = {
        "summary": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "prompt_template": "Summarize {{ title }}.",
        }
    }

    with pytest.raises(RuntimeError):
        processor.process({"title": "Bright Sparks"}, context, config=config)
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
A apps/crawler/tests/test_post_processing.py
file update:
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to add schema tests**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_config_schema.py
@@ -91 +91,57 @@
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M apps/crawler/tests/test_config_schema.py
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Adding new integration test for crawler post-processing**

I'll prepare a new test in `test_crawler.py` that checks how crawler merging of root-level and target-level post-processing configs applies for summary and description fields, including title normalization. I’ll set up a DummySession with crafted HTML to verify the results and place the test near existing ones using apply_patch.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_crawler.py
@@ -514 +514,56 @@
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/tests/test_crawler.py
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to run failing tests**
exec
bash -lc 'pytest apps/crawler/tests/test_post_processing.py' in /var/home/rick/Development/mega-directory exited 2 in 13.33s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error

==================================== ERRORS ====================================
_________ ERROR collecting apps/crawler/tests/test_post_processing.py __________
ImportError while importing test module '/var/home/rick/Development/mega-directory/apps/crawler/tests/test_post_processing.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib64/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
apps/crawler/tests/test_post_processing.py:12: in <module>
    from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ModuleNotFoundError: No module named 'post_processing'
=========================== short test summary info ============================
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Updating test plan progress**
Plan update
  ✓ Add failing tests covering shared post-processing behaviors (module unit tests, crawler integration expectations, config schema validation).
  → Implement shared post-processing module plus schema/config updates and integrate into crawler.
  • Run and verify relevant test suites, adjusting as needed.
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Implementing module and schema updates**

I'm planning to add a new LLM-related module, create post-processing logic, and update the crawler to integrate these features. This includes updating method signatures, managing context setup for post-processing, and revising config schemas and examples to reflect the new capabilities. I'll check the example JSON to align the config additions properly.
exec
bash -lc "sed -n '1,200p' apps/crawler/targets.example.json" in /var/home/rick/Development/mega-directory succeeded in 40ms:
{
  "$schema": "./config/targets.schema.json",
  "api_targets": [
    {
      "name": "dev",
      "endpoint": "http://localhost:3001/v1/crawler/listings",
      "token": "dev-token"
    },
    {
      "name": "prod",
      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
      "token": "prod-token"
    }
  ],
  "targets": [
    {
      "category": "Real Estate",
      "locations": ["New York", "San Francisco"],
      "subdomain": "realestate.local",
      "keywords": ["ocean view", "downtown"],
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)



thinking
**Designing post-processing integration**

I'm mapping out a comprehensive plan to add post-processing features: moving LLMRequest to its own module, implementing a new ListingPostProcessor with detailed normalization and generation logic using Jinja templates and LLM calls, updating crawler to support post-processing configs merged from root and target levels, and extending schema and example configs to reflect these changes. This will let the crawler run fine-grained post-processing on listings with flexible AI-powered summary and description generation.
thinking
**Detailing listing post-processing mechanics**

I'm ironing out how ListingPostProcessor normalizes titles, trims summaries and descriptions, decides when to call the LLM for generation, and merges results back. This includes helper methods for template rendering, whitespace collapsing, string truncation, and building prompt context with extra tokens. I’m also planning the integration points into the crawler to pass contextual info like location and categories, ensuring smooth config merging and fallback handling.
file update
A /var/home/rick/Development/mega-directory/apps/crawler/llm.py
"""Shared LLM helper types for crawler-adjacent tools."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Callable, Dict, Optional


@dataclass
class LLMRequest:
    provider: str
    model: str
    prompt: str
    field_name: str
    options: Dict[str, Any] = field(default_factory=dict)
    target: Optional[Dict[str, Any]] = None
    listing: Optional[Dict[str, Any]] = None


LLMClient = Callable[[LLMRequest], str]


__all__ = ["LLMRequest", "LLMClient"]
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
A apps/crawler/llm.py
file update:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing crawler.py imports and cleanup**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -6,3 +6,3 @@
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -13,2 +13,5 @@
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
@@ -91,16 +94,2 @@
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Searching for Callable usage**
exec
bash -lc 'rg -n "Callable" apps/crawler/crawler.py' in /var/home/rick/Development/mega-directory exited 1 in 40ms:

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Designing post_processing module**

I'm planning to create the `post_processing.py` module with `PostProcessingContext` and `ListingPostProcessor` classes, implementing careful default handling, normalization, truncation, and conditional LLM-based generation. I'll make sure config is shallow-copied, handle title/summary/description fields with fallbacks, and prepare methods like `_coerce_mapping`, `_should_generate`, and `_generate_field`. Also, I'll cover error handling when LLM is requested but unavailable and ensure tests with fake LLM pass as expected.
file update
A /var/home/rick/Development/mega-directory/apps/crawler/post_processing.py
from __future__ import annotations

from dataclasses import dataclass, field
import re
from typing import Any, Dict, Optional

from jinja2 import Environment, StrictUndefined, TemplateError

from llm import LLMClient, LLMRequest


DEFAULT_SUMMARY_MAX_CHARS = 280
DEFAULT_DESCRIPTION_MAX_CHARS = 1200
_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
_WHITESPACE_PATTERN = re.compile(r"\s+")


@dataclass
class PostProcessingContext:
    title: Optional[str] = None
    snippet: Optional[str] = None
    summary: Optional[str] = None
    description: Optional[str] = None
    category: Optional[str] = None
    category_slug: Optional[str] = None
    location: Optional[str] = None
    keyword: Optional[str] = None
    source_name: Optional[str] = None
    source_url: Optional[str] = None
    extras: Dict[str, Any] = field(default_factory=dict)


class ListingPostProcessor:
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
    ) -> None:
        self.llm_client = llm_client
        self.summary_max_chars = summary_max_chars
        self.description_max_chars = description_max_chars
        self._env = Environment(
            autoescape=False,
            undefined=StrictUndefined,
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def process(
        self,
        payload: Dict[str, Any],
        context: PostProcessingContext,
        config: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        normalized = dict(payload or {})
        config_map = dict(config or {}) if isinstance(config, dict) else {}
        normalize_titles = config_map.get("normalize_titles", True)
        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
        description_limit = (
            self._coerce_positive_int(config_map.get("description_max_chars"))
            or self.description_max_chars
        )

        title_value = normalized.get("title") or context.title
        if isinstance(title_value, str):
            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)

        summary_value = normalized.get("summary") or context.summary or context.snippet
        if isinstance(summary_value, str):
            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)

        description_value = normalized.get("description") or context.description
        if isinstance(description_value, str):
            normalized["description"] = self._truncate(
                self._collapse_whitespace(description_value),
                description_limit,
            )

        summary_config = self._coerce_mapping(config_map.get("summary"))
        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
            normalized["summary"] = self._generate_field(
                "summary",
                summary_config,
                normalized,
                context,
                summary_limit,
            )

        description_config = self._coerce_mapping(config_map.get("description"))
        if description_config and self._should_generate(normalized.get("description"), description_config):
            normalized["description"] = self._generate_field(
                "description",
                description_config,
                normalized,
                context,
                description_limit,
            )

        return normalized

    def _generate_field(
        self,
        field_name: str,
        config: Dict[str, Any],
        payload: Dict[str, Any],
        context: PostProcessingContext,
        limit: Optional[int],
    ) -> str:
        if not self.llm_client:
            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")

        prompt_template = config.get("prompt_template")
        provider = config.get("provider")
        model = config.get("model")
        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
            raise ValueError(f"Invalid post-processing config for '{field_name}'")

        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
        prompt = self._render_template(field_name, prompt_template, render_context)
        request = LLMRequest(
            provider=provider,
            model=model,
            prompt=prompt,
            field_name=field_name,
            options=self._coerce_mapping(config.get("options")) or {},
            target=config,
            listing=
            {
                "title": payload.get("title") or context.title,
                "summary": payload.get("summary") or context.summary,
                "description": payload.get("description") or context.description,
                "snippet": context.snippet,
                "category": context.category,
                "location": context.location,
            },
        )
        response = self.llm_client(request)
        cleaned = self._collapse_whitespace(response)
        return self._truncate(cleaned, limit)

    def _build_prompt_context(
        self,
        payload: Dict[str, Any],
        context: PostProcessingContext,
        extra_tokens: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        resolved_title = payload.get("title") or context.title or ""
        resolved_summary = payload.get("summary") or context.summary or ""
        resolved_description = payload.get("description") or context.description or ""
        resolved_snippet = context.snippet or resolved_summary
        resolved_location = context.location or ""
        resolved_category = context.category or ""
        base_context: Dict[str, Any] = {
            "title": resolved_title,
            "summary": resolved_summary,
            "description": resolved_description,
            "snippet": resolved_snippet,
            "category": resolved_category,
            "category_slug": context.category_slug or "",
            "location": resolved_location,
            "keyword": context.keyword or "",
            "source_name": context.source_name or "",
            "source_url": context.source_url or "",
            "extras": dict(context.extras or {}),
        }
        tokens: Dict[str, Any] = {
            "title": resolved_title,
            "summary": resolved_summary,
            "description": resolved_description,
            "snippet": resolved_snippet,
            "category": resolved_category,
            "category_slug": context.category_slug or "",
            "location": resolved_location,
            "keyword": context.keyword or "",
            "source_name": context.source_name or "",
            "source_url": context.source_url or "",
        }
        extras = context.extras or {}
        for key, value in extras.items():
            if isinstance(value, str):
                tokens[key] = value
        if isinstance(extra_tokens, dict):
            for key, value in extra_tokens.items():
                if isinstance(value, str):
                    tokens[key] = value
        base_context["tokens"] = tokens
        return base_context

    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
        try:
            compiled = self._env.from_string(template)
            return compiled.render(**context)
        except TemplateError as exc:  # pragma: no cover - defensive logging path
            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc

    @staticmethod
    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
        if isinstance(value, dict):
            return dict(value)
        return None

    @staticmethod
    def _coerce_positive_int(value: Any) -> Optional[int]:
        if isinstance(value, int) and value > 0:
            return value
        if isinstance(value, float) and value > 0:
            return int(value)
        return None

    @staticmethod
    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
        always = bool(config.get("always"))
        if always:
            return True
        return not isinstance(current_value, str) or not current_value.strip()

    @staticmethod
    def _truncate(value: str, limit: Optional[int]) -> str:
        if not limit or len(value) <= limit:
            return value
        return value[:limit].rstrip()

    @staticmethod
    def _collapse_whitespace(value: str) -> str:
        return _WHITESPACE_PATTERN.sub(" ", value).strip()

    @staticmethod
    def _normalize_title(value: str) -> str:
        collapsed = ListingPostProcessor._collapse_whitespace(value)
        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
        return ListingPostProcessor._collapse_whitespace(normalized)


__all__ = ["ListingPostProcessor", "PostProcessingContext"]
apply_patch(auto_approved=true) exited 0 in 13ms:
Success. Updated the following files:
A apps/crawler/post_processing.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..779dc9380c5ac8e33976b4a431dd34ba457d6608
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,235 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing=
+            {
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..779dc9380c5ac8e33976b4a431dd34ba457d6608
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,235 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing=
+            {
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Checking syntax around listing assignment**
exec
bash -lc "sed -n '1,260p' apps/crawler/post_processing.py" in /var/home/rick/Development/mega-directory succeeded in 43ms:
from __future__ import annotations

from dataclasses import dataclass, field
import re
from typing import Any, Dict, Optional

from jinja2 import Environment, StrictUndefined, TemplateError

from llm import LLMClient, LLMRequest


DEFAULT_SUMMARY_MAX_CHARS = 280
DEFAULT_DESCRIPTION_MAX_CHARS = 1200
_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
_WHITESPACE_PATTERN = re.compile(r"\s+")


@dataclass
class PostProcessingContext:
    title: Optional[str] = None
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..779dc9380c5ac8e33976b4a431dd34ba457d6608
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,235 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing=
+            {
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Correcting indentation in listing block**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/post_processing.py
@@ -127,4 +127,3 @@
             target=config,
-            listing=
-            {
+            listing={
                 "title": payload.get("title") or context.title,
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M apps/crawler/post_processing.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..1586401ba0b14d3a323a50faf47e175fca097998
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,20 +92,6 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
 
 
 class FieldGenerator:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Planning post-processing integration in crawler.py**

I’m carefully figuring out how to add ListingPostProcessor to Crawler by initializing it in __init__, updating payload-building methods to pass merged post-processing config, and adding helpers for merging configs and resolving location labels. I’m making sure context extras handle strings properly and that all method signatures align with the new parameters so everything fits smoothly into the existing crawler workflow.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -283,2 +283,3 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
apply_patch(auto_approved=true) exited 0 in 13ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..5e681b7f668eab657855d089d63f24012fd2538f
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
 
-LLMClient = Callable[[LLMRequest], str]
 
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..5e681b7f668eab657855d089d63f24012fd2538f
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
 
-LLMClient = Callable[[LLMRequest], str]
 
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Modifying _post_batch_to_api call**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -730,3 +730,3 @@
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
apply_patch(auto_approved=true) exited 0 in 14ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..40a94bb0927cac379fd303e54a0c55e72032d1ce
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..40a94bb0927cac379fd303e54a0c55e72032d1ce
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Modifying ingestion payload methods**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -760,2 +760,3 @@
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
@@ -764,3 +765,9 @@
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
@@ -773,3 +780,5 @@
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
apply_patch(auto_approved=true) exited 0 in 15ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..242f389f783283fbaa364e9594fd51ee08c01188
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..242f389f783283fbaa364e9594fd51ee08c01188
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Modifying payload assembly with post-processing**

I’m updating the `_build_listing_payload` method to incorporate post-processing by constructing a context and calling `post_processor.process` right after normalizing location but before the final return. I’ll also add helper methods like `_build_post_processing_extras` and `_resolve_post_processing_config` nearby to support this integration and cleanly handle extras, location labels, and configs. This should enhance payload enrichment consistently.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -819,2 +819,19 @@
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
apply_patch(auto_approved=true) exited 0 in 14ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..16f61392aeba65006e8289bef00f8762a938d45d
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..16f61392aeba65006e8289bef00f8762a938d45d
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Adding post-processing helper methods**

I'm preparing to implement several helper methods for post-processing configuration and location label handling, placing them near existing helpers like `_resolve_source_name`. These will handle merging mappings, coercing dicts, formatting addresses, and assembling extra metadata from listings and batches, ensuring clean and consistent data processing.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -857,2 +857,77 @@
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
apply_patch(auto_approved=true) exited 0 in 13ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying data handling and imports**
exec
bash -lc 'rg -n "_build_ingestion_payloads" -n apps/crawler/crawler.py' in /var/home/rick/Development/mega-directory succeeded in 42ms:
731:        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
757:    def _build_ingestion_payloads(
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Updating schema with post-processing config**

I'm preparing to add new post-processing configuration definitions to the JSON schema file for the crawler app. This involves inserting new properties for `post_processing` under root and targets, and defining new detailed schemas for `postProcessingConfig` and `llmGenerationConfig`, carefully placing them for readability and correctness. I'll watch the JSON syntax closely to ensure everything fits well and maintains schema validity.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/config/targets.schema.json
@@ -29,2 +29,6 @@
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
@@ -167,2 +171,5 @@
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
@@ -250,2 +257,46 @@
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
apply_patch(auto_approved=true) exited 0 in 15ms:
Success. Updated the following files:
M apps/crawler/config/targets.schema.json
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Updating JSON with post_processing examples**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/targets.example.json
@@ -14,2 +14,10 @@
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
@@ -48,2 +56,10 @@
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
apply_patch(auto_approved=true) exited 0 in 16ms:
Success. Updated the following files:
M apps/crawler/targets.example.json
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying integration and imports**
exec
bash -lc 'rg -n "LLMRequest" -n' in /var/home/rick/Development/mega-directory succeeded in 50ms:
apps/crawler/post_processing.py:9:from llm import LLMClient, LLMRequest
apps/crawler/post_processing.py:121:        request = LLMRequest(
apps/crawler/llm.py:10:class LLMRequest:
apps/crawler/llm.py:20:LLMClient = Callable[[LLMRequest], str]
apps/crawler/llm.py:23:__all__ = ["LLMRequest", "LLMClient"]
apps/crawler/crawler.py:14:from llm import LLMClient, LLMRequest
apps/crawler/crawler.py:222:        request = LLMRequest(
apps/crawler/crawler.py:590:        request = LLMRequest(
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:206:        request = LLMRequest(
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:397:        request = LLMRequest(
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2874:I'm mapping out a comprehensive plan to add post-processing features: moving LLMRequest to its own module, implementing a new ListingPostProcessor with detailed normalization and generation logic using Jinja templates and LLM calls, updating crawler to support post-processing configs merged from root and target levels, and extending schema and example configs to reflect these changes. This will let the crawler run fine-grained post-processing on listings with flexible AI-powered summary and description generation.
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2890:class LLMRequest:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2900:LLMClient = Callable[[LLMRequest], str]
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2903:__all__ = ["LLMRequest", "LLMClient"]
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2923:+class LLMRequest:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2933:+LLMClient = Callable[[LLMRequest], str]
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:2936:+__all__ = ["LLMRequest", "LLMClient"]
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:3183:+class LLMRequest:
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:3193:+LLMClient = Callable[[LLMRequest], str]
codex/LOGS/tasklog_35_implement_shared_post-processing_for_crawler_and_text_import_scripts_1762957666.log:3196:+__all__ = ["LLMRequest", "LLMClient"]
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying post-processing changes and preparing tests**

I'm making sure all the type hints, imports, and helper functions related to post-processing are correctly set up and consistent, especially handling Optional types and default values. I’m also confirming that the ListingPostProcessor config logic is solid and tests can properly import the post_processing module before I run pytest on its test suite.
exec
bash -lc 'pytest apps/crawler/tests/test_post_processing.py' in /var/home/rick/Development/mega-directory exited 1 in 1.48s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 4 items

apps/crawler/tests/test_post_processing.py ..FF                          [100%]

=================================== FAILURES ===================================
____________ test_processor_invokes_llm_for_summary_and_description ____________

    def test_processor_invokes_llm_for_summary_and_description() -> None:
        requests = []
    
        def fake_llm(request):  # type: ignore[no-untyped-def]
            requests.append(request)
            return f"LLM::{request.field_name}::{request.prompt}"
    
        processor = ListingPostProcessor(llm_client=fake_llm)
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..f619262d0e900932e982bb65770550fc5aa80cff
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_value = normalized.get("summary") or context.summary or context.snippet
+        if isinstance(summary_value, str):
+            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_value = normalized.get("description") or context.description
+        if isinstance(description_value, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(description_value),
+                description_limit,
+            )
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        if description_config and self._should_generate(normalized.get("description"), description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Reorganizing summary and description processing**

I'm reorganizing the `ListingPostProcessor` to evaluate whether to generate LLM summaries before falling back to snippet data, ensuring that snippet is only used if LLM generation isn't triggered. This involves capturing existing summary before fallback, updating `_should_generate` logic to ignore snippet fallbacks, and similarly handling descriptions. I’ll also ensure proper error raising when LLM config is given but no client is set, and clean summaries/descriptions consistently before storing.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/post_processing.py
@@ -68,15 +68,5 @@
 
-        summary_value = normalized.get("summary") or context.summary or context.snippet
-        if isinstance(summary_value, str):
-            normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
-
-        description_value = normalized.get("description") or context.description
-        if isinstance(description_value, str):
-            normalized["description"] = self._truncate(
-                self._collapse_whitespace(description_value),
-                description_limit,
-            )
-
         summary_config = self._coerce_mapping(config_map.get("summary"))
-        if summary_config and self._should_generate(normalized.get("summary"), summary_config):
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
             normalized["summary"] = self._generate_field(
@@ -88,5 +78,10 @@
             )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
 
         description_config = self._coerce_mapping(config_map.get("description"))
-        if description_config and self._should_generate(normalized.get("description"), description_config):
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
             normalized["description"] = self._generate_field(
@@ -98,2 +93,7 @@
             )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
 
apply_patch(auto_approved=true) exited 0 in 15ms:
Success. Updated the following files:
M apps/crawler/post_processing.py
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying summary and description logic**
exec
bash -lc 'pytest apps/crawler/tests/test_post_processing.py' in /var/home/rick/Development/mega-directory succeeded in 1.19s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 4 items

apps/crawler/tests/test_post_processing.py ....                          [100%]

============================== 4 passed in 0.37s ===============================
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying post-processing logic and preparing tests**

I'm making sure the post-processing methods handle empty categories and summaries correctly, preserve original payloads by copying, and normalize keywords properly. Once these checks are in place, I'll run the crawler tests under `apps/crawler/tests` to confirm everything works as expected.
exec
bash -lc 'pytest apps/crawler/tests' in /var/home/rick/Development/mega-directory exited 1 in 7.27s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 22 items

apps/crawler/tests/test_config_schema.py .....                           [ 22%]
apps/crawler/tests/test_crawler.py ............F                         [ 81%]
apps/crawler/tests/test_post_processing.py ....                          [100%]

=================================== FAILURES ===================================
_____________ test_run_crawler_applies_post_processing_generation ______________

    def test_run_crawler_applies_post_processing_generation() -> None:
        html = """
        <section>
          <article data-listing>
            <a class=\"listing-link\" href=\"https://example.com/acme\">
              <span class=\"listing-title\">
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..c9aea3ae49b2dc955c7c14f4a2632906f1373574
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -89,22 +92,8 @@
     endpoint: str
     token: str
     timeout: Optional[int] = None
-
-
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
 
 
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -818,6 +817,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +855,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Ensuring correct post-processing and removing snippet fallback**

I’m figuring out why the summary generation isn’t triggering as expected and realizing that the snippet fallback in `_build_listing_payload` overrides summary too early, blocking the post-processor’s summary generation. By removing that snippet fallback block, I can let the `ListingPostProcessor` handle summary fallback properly, keeping behavior consistent and avoiding duplication. I’ll remove that code and consider adjusting tests accordingly.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -790,7 +790,2 @@
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
apply_patch(auto_approved=true) exited 0 in 17ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Verifying summary parameter handling**
exec
bash -lc 'pytest apps/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 2.46s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 22 items

apps/crawler/tests/test_config_schema.py .....                           [ 22%]
apps/crawler/tests/test_crawler.py .............                         [ 81%]
apps/crawler/tests/test_post_processing.py ....                          [100%]

============================== 22 passed in 1.41s ==============================
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Reviewing test completion and planning next steps**
Plan update
  ✓ Add failing tests covering shared post-processing behaviors (module unit tests, crawler integration expectations, config schema validation).
  ✓ Implement shared post-processing module plus schema/config updates and integrate into crawler.
  ✓ Run and verify relevant test suites, adjusting as needed.
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Checking modified files**
exec
bash -lc 'git status -sb' in /var/home/rick/Development/mega-directory succeeded in 130ms:
## main...origin/main [ahead 6]
 M .env.example
?? .editorconfig
?? .vscode/
?? apps/
?? codex/LOGS/codex_run_2025-11-12_06-13-21.log
?? codex/LOGS/tasklog_24_setup_project_scaffolding_and_configuration_1762946001.log
?? codex/LOGS/tasklog_24_setup_project_scaffolding_and_configuration_1762946001.log.done
?? codex/LOGS/tasklog_25_define_directory_page_model_and_metadata_fields_1762949245.log
?? codex/LOGS/tasklog_25_define_directory_page_model_and_metadata_fields_1762949245.log.done
?? codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log
?? codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log.done
?? codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log
?? codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log.done
?? codex/LOGS/tasklog_27_implement_directory_pages_table_and_metadata_fields_1762950694.log
?? codex/LOGS/tasklog_27_implement_directory_pages_table_and_metadata_fields_1762950694.log.done
?? codex/LOGS/tasklog_28_build_admin_ui_for_directory_page_creation_and_editing_1762951273.log
?? codex/LOGS/tasklog_28_build_admin_ui_for_directory_page_creation_and_editing_1762951273.log.done
?? codex/LOGS/tasklog_29_implement_listings_table_enhancements_for_multiple_categories_and_locations_1762952190.log
?? codex/LOGS/tasklog_29_implement_listings_table_enhancements_for_multiple_categories_and_locations_1762952190.log.done
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Organizing changed files and line references**
exec
bash -lc 'nl -ba apps/crawler/post_processing.py | head -n 220' in /var/home/rick/Development/mega-directory succeeded in 98ms:
     1	from __future__ import annotations
     2	
     3	from dataclasses import dataclass, field
     4	import re
     5	from typing import Any, Dict, Optional
     6	
     7	from jinja2 import Environment, StrictUndefined, TemplateError
     8	
     9	from llm import LLMClient, LLMRequest
    10	
    11	
    12	DEFAULT_SUMMARY_MAX_CHARS = 280
    13	DEFAULT_DESCRIPTION_MAX_CHARS = 1200
    14	_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
    15	_WHITESPACE_PATTERN = re.compile(r"\s+")
    16	
    17	
    18	@dataclass
    19	class PostProcessingContext:
    20	    title: Optional[str] = None
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Reviewing line references for llm.py**
exec
bash -lc 'nl -ba apps/crawler/llm.py' in /var/home/rick/Development/mega-directory succeeded in 39ms:
     1	"""Shared LLM helper types for crawler-adjacent tools."""
     2	
     3	from __future__ import annotations
     4	
     5	from dataclasses import dataclass, field
     6	from typing import Any, Callable, Dict, Optional
     7	
     8	
     9	@dataclass
    10	class LLMRequest:
    11	    provider: str
    12	    model: str
    13	    prompt: str
    14	    field_name: str
    15	    options: Dict[str, Any] = field(default_factory=dict)
    16	    target: Optional[Dict[str, Any]] = None
    17	    listing: Optional[Dict[str, Any]] = None
    18	
    19	
    20	LLMClient = Callable[[LLMRequest], str]
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Identifying code insertion points**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '180,280p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   180	            "location": batch_context.get("location") or "",
   181	            "keyword": batch_context.get("keyword") or "",
   182	            "subdomain": base_context.get("subdomain") or "",
   183	        }
   184	        for key, value in listing_context.items():
   185	            if isinstance(value, str):
   186	                tokens[key] = value
   187	        return tokens
   188	
   189	    def _resolve_scrape_field(
   190	        self,
   191	        field_name: str,
   192	        config: Dict[str, Any],
   193	        context: Dict[str, Any],
   194	    ) -> Any:
   195	        attribute = config.get("attribute") or field_name
   196	        listing_data = context["listing"]
   197	        value = self._lookup_value(listing_data, attribute)
   198	        if value is not None:
   199	            return value
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Capturing initialization area**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '276,340p'" in /var/home/rick/Development/mega-directory succeeded in 40ms:
   276	    ) -> None:
   277	        self.session = session or requests_cache.CachedSession(
   278	            cache_name=cache_name,
   279	            backend="sqlite",
   280	            expire_after=expire_after,
   281	        )
   282	        self.request_timeout = request_timeout
   283	        self.field_generator = FieldGenerator(llm_client=llm_client)
   284	        self.post_processor = ListingPostProcessor(llm_client=llm_client)
   285	        self.logger = logger or _build_default_logger()
   286	
   287	    def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
   288	        api_targets = self._resolve_api_targets(config)
   289	        target_count = len(config.get("targets") or [])
   290	        self.logger.info("Starting crawler run for %s targets", target_count)
   291	        batches: List[CrawlerBatch] = []
   292	        for target in config.get("targets", []):
   293	            for location_batch in self._iter_location_batches(target):
   294	                listings = self.fetch_listings(target, location_batch)
   295	                limit = target.get("listings_per_location")
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to view function code segment**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '720,880p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   720	        return resolved
   721	
   722	    def _post_batch_to_api(
   723	        self,
   724	        batch: CrawlerBatch,
   725	        target_config: Dict[str, Any],
   726	        api_targets: List[APITarget],
   727	        root_config: Dict[str, Any],
   728	    ) -> None:
   729	        if not batch.listings or not api_targets:
   730	            return
   731	        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
   732	        if not payloads:
   733	            return
   734	        for api_target in api_targets:
   735	            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
   736	            self.logger.info(
   737	                "Posting %s listings to %s",
   738	                len(payloads),
   739	                api_target.name,
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Extracting remaining helper functions**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '880,1020p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   880	    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
   881	        extras: Dict[str, Any] = {}
   882	        keyword = self._normalize_string(listing.extras.get("keyword"))
   883	        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
   884	        search_url = self._normalize_string(batch.metadata.get("search_url"))
   885	        if keyword:
   886	            extras["keyword"] = keyword
   887	        elif batch_keyword:
   888	            extras["keyword"] = batch_keyword
   889	        if search_url:
   890	            extras["search_url"] = search_url
   891	        if batch.location:
   892	            extras["batch_location"] = batch.location
   893	        if listing.categories:
   894	            extras["categories"] = ", ".join(listing.categories)
   895	        return extras
   896	
   897	    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
   898	        location = getattr(listing, "location", None)
   899	        if isinstance(location, dict):
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to reference modifications**
exec
bash -lc "nl -ba apps/crawler/config/targets.schema.json | sed -n '1,120p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
     1	{
     2	  "$schema": "https://json-schema.org/draft/2020-12/schema",
     3	  "$id": "https://mega-directory.dev/schemas/crawler-targets.json",
     4	  "title": "Mega Directory Crawler Configuration",
     5	  "type": "object",
     6	  "description": "Schema describing crawler API targets, scraping targets, and optional LLM field generation rules.",
     7	  "additionalProperties": true,
     8	  "properties": {
     9	    "$schema": {
    10	      "type": "string",
    11	      "description": "Optional pointer for tooling to locate this schema definition."
    12	    },
    13	    "api_endpoint": {
    14	      "type": "string",
    15	      "format": "uri",
    16	      "description": "Single API endpoint to POST crawler batches to when api_targets is omitted."
    17	    },
    18	    "api_token": {
    19	      "$ref": "#/$defs/nonEmptyString",
    20	      "description": "Bearer token paired with api_endpoint."
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to highlight post_processing changes**
exec
bash -lc "nl -ba apps/crawler/config/targets.schema.json | sed -n '150,260p'" in /var/home/rick/Development/mega-directory succeeded in 40ms:
   150	          "$ref": "#/$defs/fieldMap"
   151	        },
   152	        "category_rules": {
   153	          "$ref": "#/$defs/categoryRuleList"
   154	        },
   155	        "category_classifier": {
   156	          "$ref": "#/$defs/categoryClassifier"
   157	        },
   158	        "listings_per_location": {
   159	          "$ref": "#/$defs/positiveInteger"
   160	        },
   161	        "request_timeout": {
   162	          "$ref": "#/$defs/positiveNumber"
   163	        },
   164	        "api_request_timeout": {
   165	          "$ref": "#/$defs/positiveNumber"
   166	        },
   167	        "source_name": {
   168	          "$ref": "#/$defs/nonEmptyString"
   169	        },
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Extending postProcessingConfig view**
exec
bash -lc "nl -ba apps/crawler/config/targets.schema.json | sed -n '258,360p'" in /var/home/rick/Development/mega-directory succeeded in 43ms:
   258	    "llmGenerationConfig": {
   259	      "type": "object",
   260	      "additionalProperties": false,
   261	      "properties": {
   262	        "provider": { "$ref": "#/$defs/nonEmptyString" },
   263	        "model": { "$ref": "#/$defs/nonEmptyString" },
   264	        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
   265	        "tokens": {
   266	          "type": "object",
   267	          "additionalProperties": {
   268	            "type": "string"
   269	          }
   270	        },
   271	        "options": {
   272	          "type": "object"
   273	        },
   274	        "always": {
   275	          "type": "boolean"
   276	        }
   277	      },
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Referencing new config lines**
exec
bash -lc 'nl -ba apps/crawler/targets.example.json' in /var/home/rick/Development/mega-directory succeeded in 41ms:
     1	{
     2	  "$schema": "./config/targets.schema.json",
     3	  "api_targets": [
     4	    {
     5	      "name": "dev",
     6	      "endpoint": "http://localhost:3001/v1/crawler/listings",
     7	      "token": "dev-token"
     8	    },
     9	    {
    10	      "name": "prod",
    11	      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
    12	      "token": "prod-token"
    13	    }
    14	  ],
    15	  "post_processing": {
    16	    "summary_max_chars": 240,
    17	    "summary": {
    18	      "provider": "openai",
    19	      "model": "gpt-4o-mini",
    20	      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing test file details**
exec
bash -lc "nl -ba apps/crawler/tests/test_config_schema.py | sed -n '1,220p'" in /var/home/rick/Development/mega-directory succeeded in 42ms:
     1	from __future__ import annotations
     2	
     3	import json
     4	from pathlib import Path
     5	
     6	import jsonschema
     7	import pytest
     8	
     9	
    10	AGENT_DIR = Path(__file__).resolve().parents[1]
    11	SCHEMA_PATH = AGENT_DIR / "config" / "targets.schema.json"
    12	EXAMPLE_PATH = AGENT_DIR / "targets.example.json"
    13	
    14	
    15	@pytest.fixture(scope="module")
    16	def schema() -> dict:
    17	    """Load the crawler configuration schema once per test module."""
    18	    with SCHEMA_PATH.open("r", encoding="utf-8") as fp:
    19	        return json.load(fp)
    20	
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing to reference new tests**
exec
bash -lc 'nl -ba apps/crawler/tests/test_post_processing.py' in /var/home/rick/Development/mega-directory succeeded in 44ms:
     1	from __future__ import annotations
     2	
     3	from pathlib import Path
     4	import sys
     5	
     6	import pytest
     7	
     8	AGENT_DIR = Path(__file__).resolve().parents[1]
     9	if str(AGENT_DIR) not in sys.path:
    10	    sys.path.insert(0, str(AGENT_DIR))
    11	
    12	from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
    13	
    14	
    15	def test_processor_normalizes_existing_fields_without_llm() -> None:
    16	    processor = ListingPostProcessor()
    17	    context = PostProcessingContext(
    18	        title="  ACME   Electric  ",
    19	        snippet="Licensed and insured electricians serving the lakeshore.",
    20	        category="Electricians",
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)


thinking
**Preparing detailed changelog summary**
codex
**Changes**
- Added a shared LLM-aware post-processing pipeline (`apps/crawler/post_processing.py:18`) and extracted the common `LLMRequest`/`LLMClient` types (`apps/crawler/llm.py:1`) so both the crawler and future import tools can normalize titles, trim summaries, and optionally invoke templated LLM prompts for short/long descriptions.
- Wired the crawler to the new processor (`apps/crawler/crawler.py:284`, `apps/crawler/crawler.py:777`) including config merging, richer prompt context, and helper utilities for extras/location labels (`apps/crawler/crawler.py:815`, `apps/crawler/crawler.py:880`); added an end-to-end regression that proves summary/description enrichment flows into API payloads (`apps/crawler/tests/test_crawler.py:520`).
- Documented configuration knobs via schema extensions (`apps/crawler/config/targets.schema.json:30`, `apps/crawler/config/targets.schema.json:258`) and the example config (`apps/crawler/targets.example.json:15`) so targets can opt into shared or per-target post-processing.
- Introduced unit tests covering the processor contract and schema validation (`apps/crawler/tests/test_post_processing.py:15`, `apps/crawler/tests/test_config_schema.py:94`).

**Tests**
- `pytest apps/crawler/tests`

**Next**
1. Wire the upcoming text-import CLI to `ListingPostProcessor` so manual ingests automatically reuse the normalization/LLM flow.
2. Consider exposing prompt/limit defaults in config docs or CLI flags to make tuning easier for ops teams.
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index f9fb82cc649ac807e622bb09f283ce06e5c52f09..8b8fa8fb18567cafb78f7ec5f0d3790d42a7391a
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -27,6 +27,10 @@
       "$ref": "#/$defs/positiveNumber",
       "description": "Override timeout (seconds) for crawling requests."
     },
+    "post_processing": {
+      "$ref": "#/$defs/postProcessingConfig",
+      "description": "Optional shared normalization + LLM enrichment settings applied to every target."
+    },
     "api_targets": {
       "type": "array",
       "minItems": 1,
@@ -165,6 +169,9 @@
         },
         "sourceName": {
           "$ref": "#/$defs/nonEmptyString"
+        },
+        "post_processing": {
+          "$ref": "#/$defs/postProcessingConfig"
         }
       },
       "required": ["category", "locations"]
@@ -248,6 +255,50 @@
       },
       "required": ["provider", "model", "prompt_template"]
     },
+    "llmGenerationConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        },
+        "always": {
+          "type": "boolean"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
+    "postProcessingConfig": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "normalize_titles": {
+          "type": "boolean",
+          "default": true
+        },
+        "summary_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "description_max_chars": {
+          "$ref": "#/$defs/positiveInteger"
+        },
+        "summary": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        },
+        "description": {
+          "$ref": "#/$defs/llmGenerationConfig"
+        }
+      }
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index b0de1d685de20f81ed6caa75edc8574ffbc0fb37..b03811c8d66bc3468f1f34ff0319123e51735aec
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -4,13 +4,16 @@
 import logging
 import os
 import re
-from typing import Any, Callable, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
+from llm import LLMClient, LLMRequest
+from post_processing import ListingPostProcessor, PostProcessingContext
+
 try:
     import requests_cache
 except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
@@ -91,20 +94,6 @@
     timeout: Optional[int] = None
 
 
-@dataclass
-class LLMRequest:
-    provider: str
-    model: str
-    prompt: str
-    field_name: str
-    options: Dict[str, Any] = field(default_factory=dict)
-    target: Optional[Dict[str, Any]] = None
-    listing: Optional[Dict[str, Any]] = None
-
-
-LLMClient = Callable[[LLMRequest], str]
-
-
 class FieldGenerator:
     def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
         self.llm_client = llm_client
@@ -292,6 +281,7 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.post_processor = ListingPostProcessor(llm_client=llm_client)
         self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
@@ -738,7 +728,7 @@
     ) -> None:
         if not batch.listings or not api_targets:
             return
-        payloads = self._build_ingestion_payloads(batch, target_config)
+        payloads = self._build_ingestion_payloads(batch, target_config, root_config)
         if not payloads:
             return
         for api_target in api_targets:
@@ -768,11 +758,18 @@
         self,
         batch: CrawlerBatch,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
     ) -> List[Dict[str, Any]]:
         category_slug = self._resolve_category_slug(target_config)
         payloads: List[Dict[str, Any]] = []
         for listing in batch.listings:
-            payload = self._build_listing_payload(listing, target_config, category_slug)
+            payload = self._build_listing_payload(
+                listing,
+                target_config,
+                root_config,
+                category_slug,
+                batch,
+            )
             if payload:
                 payloads.append(payload)
         return payloads
@@ -781,7 +778,9 @@
         self,
         listing: Listing,
         target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
         default_category_slug: str,
+        batch: CrawlerBatch,
     ) -> Optional[Dict[str, Any]]:
         payload: Dict[str, Any] = dict(listing.fields or {})
         title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
@@ -789,11 +788,6 @@
             return None
         payload["title"] = title
 
-        summary = self._normalize_string(payload.get("summary"))
-        snippet = self._normalize_string(listing.snippet)
-        if not summary and snippet:
-            payload["summary"] = snippet
-
         source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
         if source_url:
             payload["sourceUrl"] = source_url
@@ -818,6 +812,23 @@
         if normalized_location:
             payload["location"] = normalized_location
 
+        post_processing_config = self._resolve_post_processing_config(root_config, target_config)
+        context = PostProcessingContext(
+            title=listing.title,
+            snippet=listing.snippet,
+            summary=payload.get("summary"),
+            description=payload.get("description"),
+            category=target_config.get("category"),
+            category_slug=payload.get("categorySlug"),
+            location=self._resolve_location_label(listing, batch.location),
+            keyword=self._normalize_string(listing.extras.get("keyword"))
+            or self._normalize_string(batch.metadata.get("keyword")),
+            source_name=payload.get("sourceName"),
+            source_url=payload.get("sourceUrl") or listing.url,
+            extras=self._build_post_processing_extras(listing, batch),
+        )
+        payload = self.post_processor.process(payload, context, config=post_processing_config)
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -839,6 +850,81 @@
                 return value
         return None
 
+    def _resolve_post_processing_config(
+        self,
+        root_config: Dict[str, Any],
+        target_config: Dict[str, Any],
+    ) -> Optional[Dict[str, Any]]:
+        root_map = self._coerce_mapping(root_config.get("post_processing"))
+        target_map = self._coerce_mapping(target_config.get("post_processing"))
+        if root_map and target_map:
+            return self._merge_mappings(root_map, target_map)
+        return target_map or root_map
+
+    @classmethod
+    def _merge_mappings(cls, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
+        merged = dict(base)
+        for key, value in override.items():
+            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
+                merged[key] = cls._merge_mappings(merged[key], value)
+            else:
+                merged[key] = value
+        return merged
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    def _build_post_processing_extras(self, listing: Listing, batch: CrawlerBatch) -> Dict[str, Any]:
+        extras: Dict[str, Any] = {}
+        keyword = self._normalize_string(listing.extras.get("keyword"))
+        batch_keyword = self._normalize_string(batch.metadata.get("keyword"))
+        search_url = self._normalize_string(batch.metadata.get("search_url"))
+        if keyword:
+            extras["keyword"] = keyword
+        elif batch_keyword:
+            extras["keyword"] = batch_keyword
+        if search_url:
+            extras["search_url"] = search_url
+        if batch.location:
+            extras["batch_location"] = batch.location
+        if listing.categories:
+            extras["categories"] = ", ".join(listing.categories)
+        return extras
+
+    def _resolve_location_label(self, listing: Listing, fallback: Optional[str]) -> Optional[str]:
+        location = getattr(listing, "location", None)
+        if isinstance(location, dict):
+            line1 = self._normalize_string(location.get("addressLine1"))
+            line2 = self._normalize_string(location.get("addressLine2"))
+            city = self._normalize_string(location.get("city"))
+            region = self._normalize_string(location.get("region"))
+            postal = self._normalize_string(location.get("postalCode"))
+            country = self._normalize_string(location.get("country"))
+            parts: List[str] = []
+            if line1:
+                parts.append(line1)
+            if line2:
+                parts.append(line2)
+            city_region = ", ".join(part for part in (city, region) if part)
+            if postal:
+                city_region = f"{city_region} {postal}".strip() if city_region else postal
+            if city_region:
+                parts.append(city_region)
+            if country:
+                parts.append(country)
+            if parts:
+                return ", ".join(parts)
+        extras_location = listing.extras.get("location")
+        normalized_extras_location = self._normalize_string(extras_location)
+        if normalized_extras_location:
+            return normalized_extras_location
+        if fallback and fallback.strip():
+            return fallback.strip()
+        return None
+
     def _resolve_api_timeout(
         self,
         api_target: APITarget,
diff --git a/apps/crawler/llm.py b/apps/crawler/llm.py
new file mode 100644
index 0000000000000000000000000000000000000000..c0cfd4e62a81f46143d83b14e52277306240e976
--- /dev/null
+++ b/apps/crawler/llm.py
@@ -0,0 +1,23 @@
+"""Shared LLM helper types for crawler-adjacent tools."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Optional
+
+
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+__all__ = ["LLMRequest", "LLMClient"]
diff --git a/apps/crawler/post_processing.py b/apps/crawler/post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..37ba5d681eac1f560544faffe6fb0c576fb4245c
--- /dev/null
+++ b/apps/crawler/post_processing.py
@@ -0,0 +1,234 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+import re
+from typing import Any, Dict, Optional
+
+from jinja2 import Environment, StrictUndefined, TemplateError
+
+from llm import LLMClient, LLMRequest
+
+
+DEFAULT_SUMMARY_MAX_CHARS = 280
+DEFAULT_DESCRIPTION_MAX_CHARS = 1200
+_SEPARATOR_PATTERN = re.compile(r"\s*([|/•–—-])\s*")
+_WHITESPACE_PATTERN = re.compile(r"\s+")
+
+
+@dataclass
+class PostProcessingContext:
+    title: Optional[str] = None
+    snippet: Optional[str] = None
+    summary: Optional[str] = None
+    description: Optional[str] = None
+    category: Optional[str] = None
+    category_slug: Optional[str] = None
+    location: Optional[str] = None
+    keyword: Optional[str] = None
+    source_name: Optional[str] = None
+    source_url: Optional[str] = None
+    extras: Dict[str, Any] = field(default_factory=dict)
+
+
+class ListingPostProcessor:
+    def __init__(
+        self,
+        llm_client: Optional[LLMClient] = None,
+        summary_max_chars: int = DEFAULT_SUMMARY_MAX_CHARS,
+        description_max_chars: int = DEFAULT_DESCRIPTION_MAX_CHARS,
+    ) -> None:
+        self.llm_client = llm_client
+        self.summary_max_chars = summary_max_chars
+        self.description_max_chars = description_max_chars
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def process(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        config: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
+        normalized = dict(payload or {})
+        config_map = dict(config or {}) if isinstance(config, dict) else {}
+        normalize_titles = config_map.get("normalize_titles", True)
+        summary_limit = self._coerce_positive_int(config_map.get("summary_max_chars")) or self.summary_max_chars
+        description_limit = (
+            self._coerce_positive_int(config_map.get("description_max_chars"))
+            or self.description_max_chars
+        )
+
+        title_value = normalized.get("title") or context.title
+        if isinstance(title_value, str):
+            normalized["title"] = self._normalize_title(title_value) if normalize_titles else self._collapse_whitespace(title_value)
+
+        summary_config = self._coerce_mapping(config_map.get("summary"))
+        existing_summary = normalized.get("summary") or context.summary
+        if summary_config and self._should_generate(existing_summary, summary_config):
+            normalized["summary"] = self._generate_field(
+                "summary",
+                summary_config,
+                normalized,
+                context,
+                summary_limit,
+            )
+        else:
+            summary_value = existing_summary or context.snippet
+            if isinstance(summary_value, str):
+                normalized["summary"] = self._truncate(self._collapse_whitespace(summary_value), summary_limit)
+
+        description_config = self._coerce_mapping(config_map.get("description"))
+        existing_description = normalized.get("description") or context.description
+        if description_config and self._should_generate(existing_description, description_config):
+            normalized["description"] = self._generate_field(
+                "description",
+                description_config,
+                normalized,
+                context,
+                description_limit,
+            )
+        elif isinstance(existing_description, str):
+            normalized["description"] = self._truncate(
+                self._collapse_whitespace(existing_description),
+                description_limit,
+            )
+
+        return normalized
+
+    def _generate_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        limit: Optional[int],
+    ) -> str:
+        if not self.llm_client:
+            raise RuntimeError("Post-processing LLM generation requires an llm_client instance")
+
+        prompt_template = config.get("prompt_template")
+        provider = config.get("provider")
+        model = config.get("model")
+        if not (isinstance(prompt_template, str) and isinstance(provider, str) and isinstance(model, str)):
+            raise ValueError(f"Invalid post-processing config for '{field_name}'")
+
+        render_context = self._build_prompt_context(payload, context, config.get("tokens"))
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=self._coerce_mapping(config.get("options")) or {},
+            target=config,
+            listing={
+                "title": payload.get("title") or context.title,
+                "summary": payload.get("summary") or context.summary,
+                "description": payload.get("description") or context.description,
+                "snippet": context.snippet,
+                "category": context.category,
+                "location": context.location,
+            },
+        )
+        response = self.llm_client(request)
+        cleaned = self._collapse_whitespace(response)
+        return self._truncate(cleaned, limit)
+
+    def _build_prompt_context(
+        self,
+        payload: Dict[str, Any],
+        context: PostProcessingContext,
+        extra_tokens: Optional[Dict[str, Any]],
+    ) -> Dict[str, Any]:
+        resolved_title = payload.get("title") or context.title or ""
+        resolved_summary = payload.get("summary") or context.summary or ""
+        resolved_description = payload.get("description") or context.description or ""
+        resolved_snippet = context.snippet or resolved_summary
+        resolved_location = context.location or ""
+        resolved_category = context.category or ""
+        base_context: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+            "extras": dict(context.extras or {}),
+        }
+        tokens: Dict[str, Any] = {
+            "title": resolved_title,
+            "summary": resolved_summary,
+            "description": resolved_description,
+            "snippet": resolved_snippet,
+            "category": resolved_category,
+            "category_slug": context.category_slug or "",
+            "location": resolved_location,
+            "keyword": context.keyword or "",
+            "source_name": context.source_name or "",
+            "source_url": context.source_url or "",
+        }
+        extras = context.extras or {}
+        for key, value in extras.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        if isinstance(extra_tokens, dict):
+            for key, value in extra_tokens.items():
+                if isinstance(value, str):
+                    tokens[key] = value
+        base_context["tokens"] = tokens
+        return base_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive logging path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _coerce_mapping(value: Any) -> Optional[Dict[str, Any]]:
+        if isinstance(value, dict):
+            return dict(value)
+        return None
+
+    @staticmethod
+    def _coerce_positive_int(value: Any) -> Optional[int]:
+        if isinstance(value, int) and value > 0:
+            return value
+        if isinstance(value, float) and value > 0:
+            return int(value)
+        return None
+
+    @staticmethod
+    def _should_generate(current_value: Any, config: Dict[str, Any]) -> bool:
+        always = bool(config.get("always"))
+        if always:
+            return True
+        return not isinstance(current_value, str) or not current_value.strip()
+
+    @staticmethod
+    def _truncate(value: str, limit: Optional[int]) -> str:
+        if not limit or len(value) <= limit:
+            return value
+        return value[:limit].rstrip()
+
+    @staticmethod
+    def _collapse_whitespace(value: str) -> str:
+        return _WHITESPACE_PATTERN.sub(" ", value).strip()
+
+    @staticmethod
+    def _normalize_title(value: str) -> str:
+        collapsed = ListingPostProcessor._collapse_whitespace(value)
+        normalized = _SEPARATOR_PATTERN.sub(lambda match: f" {match.group(1)} ", collapsed)
+        return ListingPostProcessor._collapse_whitespace(normalized)
+
+
+__all__ = ["ListingPostProcessor", "PostProcessingContext"]
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3..ea0242ec6a7c83abac6d9063a358f8624ee7246d
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -12,6 +12,14 @@
       "token": "prod-token"
     }
   ],
+  "post_processing": {
+    "summary_max_chars": 240,
+    "summary": {
+      "provider": "openai",
+      "model": "gpt-4o-mini",
+      "prompt_template": "Summarize {{ title }} in {{ location }} using {{ snippet }}."
+    }
+  },
   "targets": [
     {
       "category": "Real Estate",
@@ -46,6 +54,14 @@
         "choices": ["real-estate", "property-management"]
       },
       "listings_per_location": 50,
+      "post_processing": {
+        "normalize_titles": true,
+        "description": {
+          "provider": "openai",
+          "model": "gpt-4o-mini",
+          "prompt_template": "Write a persuasive description for {{ title }} serving {{ location }}. Context: {{ snippet }}."
+        }
+      },
       "fields": {
         "title": { "source": "scrape" },
         "price": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 9394b54c2f91df2d58ee8e8fa1f0f88928784023..76ef01f0ce06ceeff76b719d72bde4d22de55b93
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -89,3 +89,59 @@
     assert errors, "expected schema to enforce slug requirement"
     messages = "\n".join(error.message for error in errors)
     assert "slug" in messages
+
+
+def test_schema_accepts_post_processing_config(schema: dict) -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}.",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, config)
+
+    assert not errors, f"post_processing config failed schema validation: {[e.message for e in errors]}"
+
+
+def test_schema_rejects_invalid_post_processing_llm(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Grand Rapids, MI"],
+                "post_processing": {
+                    "summary": {
+                        # Missing provider + model to trigger validation failure
+                        "prompt_template": "Summarize {{ title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to reject incomplete post_processing"
+    messages = "\n".join(error.message for error in errors)
+    assert "provider" in messages or "model" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index 8dd24bc16f6c9681368f7355a8fe8a8896885c11..47305c9c9bd13f9fcc417e2782484d81ae9a95aa
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -512,3 +512,58 @@
     assert any(
         "Posting 1 listings to dev" in record["message"] for record in logger.records
     ), "expected post log entry"
+
+
+def test_run_crawler_applies_post_processing_generation() -> None:
+    html = """
+    <section>
+      <article data-listing>
+        <a class=\"listing-link\" href=\"https://example.com/acme\">
+          <span class=\"listing-title\">
+            ACME   Electric   |   Lakeshore Crew
+          </span>
+        </a>
+        <p class=\"listing-description\">Licensed electricians for EV charging.</p>
+      </article>
+    </section>
+    """
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"{request.field_name}::{request.provider}::{request.model}"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "post_processing": {
+            "summary": {
+                "provider": "openai",
+                "model": "gpt-4o-mini",
+                "prompt_template": "Summarize {{ title }} for {{ location }}",
+            }
+        },
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "post_processing": {
+                    "normalize_titles": True,
+                    "description": {
+                        "provider": "openai",
+                        "model": "gpt-4o-mini",
+                        "prompt_template": "Describe {{ title }} located in {{ location }}",
+                    },
+                },
+            }
+        ],
+    }
+    session = DummySession(html)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["title"] == "ACME Electric | Lakeshore Crew"
+    assert payload["summary"].startswith("summary::openai::gpt-4o-mini")
+    assert payload["description"].startswith("description::openai::gpt-4o-mini")
+    assert len(llm_requests) == 2
diff --git a/apps/crawler/tests/test_post_processing.py b/apps/crawler/tests/test_post_processing.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a3bd80d4ef2f800549d25afa15b7b6bd8a27443
--- /dev/null
+++ b/apps/crawler/tests/test_post_processing.py
@@ -0,0 +1,96 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+import pytest
+
+AGENT_DIR = Path(__file__).resolve().parents[1]
+if str(AGENT_DIR) not in sys.path:
+    sys.path.insert(0, str(AGENT_DIR))
+
+from post_processing import ListingPostProcessor, PostProcessingContext  # type: ignore[import-not-found]
+
+
+def test_processor_normalizes_existing_fields_without_llm() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="  ACME   Electric  ",
+        snippet="Licensed and insured electricians serving the lakeshore.",
+        category="Electricians",
+        location="Holland, MI",
+    )
+    payload = {
+        "title": "  ACME   Electric  \n",
+        "summary": " Trusted  residential   crews.  ",
+    }
+
+    processed = processor.process(payload, context, config={"normalize_titles": True, "summary_max_chars": 80})
+
+    assert processed["title"] == "ACME Electric"
+    assert processed["summary"] == "Trusted residential crews."
+
+
+def test_processor_populates_summary_from_snippet_when_missing() -> None:
+    processor = ListingPostProcessor()
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7 with EV charger installs.",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config={"summary_max_chars": 64})
+
+    assert processed["summary"] == "Emergency electricians available 24/7 with EV charger installs."
+
+
+def test_processor_invokes_llm_for_summary_and_description() -> None:
+    requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    processor = ListingPostProcessor(llm_client=fake_llm)
+    context = PostProcessingContext(
+        title="Bright Sparks",
+        snippet="Emergency electricians available 24/7",
+        category="Electricians",
+        location="Grand Rapids, MI",
+    )
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }} for {{ location }} using {{ snippet }}.",
+        },
+        "description": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Write a detailed overview of {{ title }} in {{ location }}. Details: {{ snippet }}.",
+        },
+    }
+
+    processed = processor.process({"title": "Bright Sparks"}, context, config=config)
+
+    assert processed["summary"].startswith("LLM::summary::Summarize Bright Sparks")
+    assert processed["description"].startswith("LLM::description::Write a detailed overview")
+    assert len(requests) == 2
+    assert all(req.provider == "openai" for req in requests)
+    assert any("Grand Rapids" in req.prompt for req in requests)
+
+
+def test_processor_requires_llm_when_configured() -> None:
+    processor = ListingPostProcessor(llm_client=None)
+    context = PostProcessingContext(title="Bright Sparks", snippet="Emergency service")
+    config = {
+        "summary": {
+            "provider": "openai",
+            "model": "gpt-4o-mini",
+            "prompt_template": "Summarize {{ title }}.",
+        }
+    }
+
+    with pytest.raises(RuntimeError):
+        processor.process({"title": "Bright Sparks"}, context, config=config)

tokens used
143,999
**Changes**
- Added a shared LLM-aware post-processing pipeline (`apps/crawler/post_processing.py:18`) and extracted the common `LLMRequest`/`LLMClient` types (`apps/crawler/llm.py:1`) so both the crawler and future import tools can normalize titles, trim summaries, and optionally invoke templated LLM prompts for short/long descriptions.
- Wired the crawler to the new processor (`apps/crawler/crawler.py:284`, `apps/crawler/crawler.py:777`) including config merging, richer prompt context, and helper utilities for extras/location labels (`apps/crawler/crawler.py:815`, `apps/crawler/crawler.py:880`); added an end-to-end regression that proves summary/description enrichment flows into API payloads (`apps/crawler/tests/test_crawler.py:520`).
- Documented configuration knobs via schema extensions (`apps/crawler/config/targets.schema.json:30`, `apps/crawler/config/targets.schema.json:258`) and the example config (`apps/crawler/targets.example.json:15`) so targets can opt into shared or per-target post-processing.
- Introduced unit tests covering the processor contract and schema validation (`apps/crawler/tests/test_post_processing.py:15`, `apps/crawler/tests/test_config_schema.py:94`).

**Tests**
- `pytest apps/crawler/tests`

**Next**
1. Wire the upcoming text-import CLI to `ListingPostProcessor` so manual ingests automatically reuse the normalization/LLM flow.
2. Consider exposing prompt/limit defaults in config docs or CLI flags to make tuning easier for ops teams.
