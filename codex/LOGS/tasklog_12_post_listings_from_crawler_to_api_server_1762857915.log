OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a7285-13cc-7bf1-848a-95b71177d0d9
--------
user
Implement the functionality described in codex/TASKS/12_post_listings_from_crawler_to_api_server.yaml, then verify with any available tests.

thinking
**Preparing to inspect task instructions**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 43ms:
admin
agents
AGENTS.md
api
astro
codex
codex.toml
codex.yaml
db
docker-compose.yml
docs
README.md

thinking
**Viewing task file contents**
exec
bash -lc "sed -n '1,160p' codex/TASKS/12_post_listings_from_crawler_to_api_server.yaml" in /var/home/rick/Development/mega-directory succeeded in 49ms:
id: 12
title: POST Listings from Crawler to API Server
description: After field population, push data to API using HTTP and token. Validate
  integration against dev and prod targets.
dependencies:
- 4
- 10

thinking
**Exploring repo for crawler code**
exec
bash -lc 'ls agents' in /var/home/rick/Development/mega-directory succeeded in 105ms:
crawler

thinking
**Listing crawler**
exec
bash -lc 'ls agents/crawler' in /var/home/rick/Development/mega-directory succeeded in 41ms:
config
crawler.py
main.py
__pycache__
targets.example.json
tests

thinking
**Opening main crawler file**
exec
bash -lc "sed -n '1,200p' agents/crawler/main.py" in /var/home/rick/Development/mega-directory succeeded in 102ms:
import json
from crawler import run_crawler

if __name__ == "__main__":
    with open("config/targets.json", "r") as f:
        config = json.load(f)
    run_crawler(config)

thinking
**Checking crawler.py**
exec
bash -lc "sed -n '1,200p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 52ms:
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Callable, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup
from jinja2 import Environment, StrictUndefined, TemplateError

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

        def __init__(self, *args, **kwargs):
            super().__init__()

thinking
**Planning to relocate posting logic**
exec
bash -lc "sed -n '200,400p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 51ms:
            prompt=prompt,
            field_name=field_name,
            options=config.get("options", {}),
            target=target,
            listing=context.get("listing"),
        )
        return self.llm_client(request)

    def _prepare_render_context(
        self,
        context: Dict[str, Any],
        config: Dict[str, Any],
    ) -> Dict[str, Any]:
        render_context = dict(context)
        render_context["listing"] = context["listing"]
        render_context["batch"] = context["batch"]
        tokens = dict(context.get("tokens", {}))
        extra_tokens = config.get("tokens") or {}
        tokens.update(extra_tokens)
        render_context["tokens"] = tokens
exec
bash -lc "sed -n '400,800p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
        if not href:
            return ""
        if href.startswith("http://") or href.startswith("https://"):
            return href
        base = Crawler._normalize_subdomain(subdomain) or "https://"
        return urljoin(base + "/", href.lstrip("/"))


def run_crawler(
    config: Dict[str, Any],
    session: Optional[requests.Session] = None,
    llm_client: Optional[LLMClient] = None,
) -> List[CrawlerBatch]:
    crawler = Crawler(session=session, llm_client=llm_client)
    return crawler.run(config)

thinking
**Planning API data push implementation**
exec
bash -lc 'ls agents/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 38ms:
__pycache__
test_crawler.py
exec
bash -lc "sed -n '1,200p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
from pathlib import Path
from types import SimpleNamespace
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

import crawler
from crawler import run_crawler


SAMPLE_HTML = """
<section>
  <article data-listing>
    <a class="listing-link" href="https://example.com/acme">
      <span class="listing-title">ACME Electric</span>
    </a>
exec
bash -lc "sed -n '200,400p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 62ms:
                    }
                },
            }
        ],
    }

    with pytest.raises(RuntimeError):
        run_crawler(config, session=DummySession(SAMPLE_HTML))

thinking
**Searching for API endpoint usage**
exec
bash -lc 'rg -n "api_endpoint" -n' in /var/home/rick/Development/mega-directory succeeded in 257ms:
codex.yaml:12:    path: codex/TASKS/04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:182:bash -lc 'rg -n "api_endpoint" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/codex_run_2025-11-11_03-24-35.log:7:Tasks: 01_initialize_astro_frontend 02_initialize_api_server 03_initialize_database_schema 04_initialize_admin_app 05_initialize_python_crawler 23_docker_compose_local_dev 06_api_endpoints_listings 07_api_endpoints_categories 08_api_token_auth
codex/LOGS/tasklog_23_docker_compose_local_dev_1762853831.log:348:codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:299:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:69:    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,
codex/LOGS/tasklog_23_docker_compose_local_dev_1762853831.log:349:codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:302:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:77:    missing.push('ADMIN_JWT_SECRET');
codex/LOGS/tasklog_23_docker_compose_local_dev_1762853831.log:350:codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:303:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:2857:    13	    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:299:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:69:    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:300:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:70:    adminJwtIssuer: overrides.adminJwtIssuer ?? process.env.ADMIN_JWT_ISSUER ?? 'mega-directory',
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:301:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:71:    adminJwtAudience: overrides.adminJwtAudience ?? process.env.ADMIN_JWT_AUDIENCE ?? 'admin',
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:302:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:77:    missing.push('ADMIN_JWT_SECRET');
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:303:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:2857:    13	    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:304:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:2858:    14	    adminJwtIssuer: overrides.adminJwtIssuer ?? process.env.ADMIN_JWT_ISSUER ?? 'mega-directory',
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:305:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:2859:    15	    adminJwtAudience: overrides.adminJwtAudience ?? process.env.ADMIN_JWT_AUDIENCE ?? 'admin',
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:349:04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:560:    path: codex/TASKS/04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:580:04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_06_frontend_directory_page_rendering_1762853993.log:375:04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_07_frontend_subcategory_ui_and_layout_1762854681.log:489:04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_07_frontend_subcategory_ui_and_layout_1762854681.log:528:04_implement_listing_ingestion_api_endpoint.yaml
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",

thinking
**Narrowing search scope**
exec
bash -lc "rg -n --glob 'agents/**' \"api_endpoint\"" in /var/home/rick/Development/mega-directory succeeded in 199ms:
agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
agents/crawler/tests/test_crawler.py:53:        "api_endpoint": "https://api.example.com",
agents/crawler/tests/test_crawler.py:129:        "api_endpoint": "https://api.example.com",
agents/crawler/tests/test_crawler.py:154:        "api_endpoint": "https://api.example.com",
agents/crawler/tests/test_crawler.py:188:        "api_endpoint": "https://api.example.com",

thinking
**Inspecting configuration file**
exec
bash -lc "sed -n '1,200p' agents/crawler/config/targets.json" in /var/home/rick/Development/mega-directory succeeded in 41ms:
{
  "api_endpoint": "http://localhost:3000/api/new-listings",
  "api_token": "YOUR_API_TOKEN_HERE",
  "targets": [
    {
      "category": "Electricians",
      "locations": [
        "Holland MI"
      ],
      "subdomain": "electricians.example.com",
      "keywords": [
        "licensed",
        "residential",
        "commercial"
      ],
      "listings_per_location": 50,
      "fields": {
        "title": {
          "source": "scrape"
        },

thinking
**Searching for API endpoints**
exec
bash -lc 'rg -n "new-listings" -n' in /var/home/rick/Development/mega-directory succeeded in 66ms:
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:202:codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:208:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:209:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:220:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:243:bash -lc 'rg -n "new-listings" -n' in /var/home/rick/Development/mega-directory
agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:121:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:253:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:114:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:392:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:415:bash -lc 'rg -n "new-listings" -n' in /var/home/rick/Development/mega-directory succeeded in 45ms:
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:416:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:417:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:392:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:418:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:415:bash -lc 'rg -n "new-listings" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:419:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:426:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:449:Iâ€™m mapping out a new ingestion endpoint for listings, aiming for a consistent, restful route like `/v1/crawler/listings` despite crawler config using `/api/new-listings`. Iâ€™m planning to implement validation, batch support, default inactive status, and store listings in a modular ingestion store hooked to `app.locals`. Next, Iâ€™ll follow TDD with tests for auth, payload validation, and storage, and outline a multi-step plan before coding. Letâ€™s explore this carefully!

thinking
**Searching for API route definitions**
exec
bash -lc 'ls api' in /var/home/rick/Development/mega-directory succeeded in 53ms:
Dockerfile
index.js
node_modules
package.json
package-lock.json
test

thinking
**Opening api/index.js file**
exec
bash -lc "sed -n '1,200p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 112ms:
const express = require('express');
const jwt = require('jsonwebtoken');

const DEFAULT_PORT = 3001;
const DEFAULT_LISTING_STATUS = 'INACTIVE';
const MAX_SLUG_LENGTH = 80;
const DEFAULT_ADMIN_TOKEN_TTL_SECONDS = 60 * 15;

function resolveConfig(overrides = {}) {
  const parsedPort = process.env.PORT ? parseInt(process.env.PORT, 10) : undefined;
  const envPort = Number.isNaN(parsedPort) ? undefined : parsedPort;
  const parsedAdminTokenTtl =
    overrides.adminTokenTtlSeconds ??
    (process.env.ADMIN_TOKEN_TTL_SECONDS
      ? parseInt(process.env.ADMIN_TOKEN_TTL_SECONDS, 10)
      : undefined);

  const baseConfig = {
    port: overrides.port ?? envPort ?? DEFAULT_PORT,
    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,

thinking
**Continuing file inspection**
exec
bash -lc "sed -n '200,400p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 45ms:
    console.log(`API server running at http://localhost:${port}`);
  });
}

if (require.main === module) {
  startServer();
}

module.exports = {
  createServer
};

function createListingIngestionHandler(app) {
  return (req, res) => {
    const payloads = normalizeListingBatch(req.body);
    if (!payloads) {
      return res
        .status(400)
        .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
    }
exec
bash -lc "sed -n '400,800p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 40ms:
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  const email = value.trim();
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  if (!emailRegex.test(email)) {
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  return email.toLowerCase();
}

function sanitizeLocation(value) {
  if (!isPlainObject(value)) {
    return null;
  }
  const fields = ['addressLine1', 'addressLine2', 'city', 'region', 'postalCode', 'country'];
  const location = {};
  fields.forEach((field) => {
    const sanitized = sanitizeNullableString(value[field]);

thinking
**Planning crawler API posting**
exec
bash -lc "rg -n \"dev\" agents/crawler -g'*'" in /var/home/rick/Development/mega-directory exited 1 in 44ms:


thinking
**Planning API posting and tests**

I'm piecing together how to add API posting for each CrawlerBatch after field population, while keeping existing return behavior intact. Since TDD is emphasized, I'll also plan new tests that confirm HTTP POST requests are made correctly for both dev and prod targets, likely defined in config files, and ensure existing tests don't break. I need to verify details from task instructions and config samples to nail down the spec.
exec
bash -lc "sed -n '1,200p' codex/TASKS/10_scaffold_python_crawler_agent.yaml" in /var/home/rick/Development/mega-directory succeeded in 58ms:
id: 10
title: Scaffold Python Crawler Agent
description: Build Python module using requests, BeautifulSoup4, and requests-cache.
  Read from config and support category-location batches.
dependencies: []

thinking
**Checking for additional task instructions**
exec
bash -lc "sed -n '1,200p' codex/TASKS/11_llm_field_generation_via_jinja2.yaml" in /var/home/rick/Development/mega-directory succeeded in 62ms:
id: 11
title: LLM Field Generation via Jinja2
description: Add dynamic field generation with provider/model config, jinja2 templates,
  and token substitution.
dependencies:
- 10

thinking
**Investigating API target configuration**
exec
bash -lc "rg -n \"api_targets\" -g'*'" in /var/home/rick/Development/mega-directory exited 124 in 10.07s:
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:379:bash -lc "rg -n \"api_targets\" -g'*'" in /var/home/rick/Development/mega-directory

thinking
**Refining ripgrep search to exclude directories**
exec
bash -lc "rg -n --glob '"'!.git/**'"' --glob '"'!codex/LOGS/**'"' \"api_targets\"" in /var/home/rick/Development/mega-directory exited 1 in 42ms:


thinking
**Analyzing API payload requirements**
exec
bash -lc "sed -n '1,200p' README.md" in /var/home/rick/Development/mega-directory succeeded in 39ms:
# Mega Directory

Mega Directory is a server-rendered directory listing platform built with Astro, Express, and Prisma. It supports dynamic subdomains, AI-generated content enrichment, and moderation workflows.

## Features

- ðŸ›°ï¸ Subdomain-based regional directories (e.g. electricians.nyc.example.com)
- âš¡ Express API with JWT-based auth
- ðŸ§  AI agents for crawling, enrichment, and moderation (via OpenRouter)
- ðŸ§© Admin dashboard for approving or rejecting listings
- ðŸ’… Tailwind CSS and minimal JavaScript for accessibility and performance
- ðŸ³ Dockerized and deployable behind Cloudflare with SSR

## Tech Stack

- Astro (SSR)
- Express.js
- Prisma + PostgreSQL
- Tailwind CSS
- OpenRouter (AI Agents)

thinking
**Checking docs for crawler info**
exec
bash -lc "sed -n '1,200p' docs/AGENTS.md" in /var/home/rick/Development/mega-directory succeeded in 39ms:
# AGENTS.md

This document outlines the purpose and operation of each core agent in the **Mega Directory** project architecture.

---

## ðŸ§  Crawler Agent (Python)

**Purpose:**
Collect business listings from predefined online sources, optionally enhance them using LLMs, and send the data to the API server.

**Key Technologies:**

* Python 3.x
* `requests`, `beautifulsoup4`, `requests-cache`, `jinja2`
* External LLM APIs (OpenRouter, OpenAI, Google Gemini)

**Responsibilities:**

* Fetch listings using category/location/keyword-driven config

thinking
**Planning multi-step feature implementation**

I'm working through the complexity of adding new API posting functionality with TDD in mind. Iâ€™m outlining a multi-step plan to handle generating the data payload, slugifying categories, mapping fields properly, and validating integration against dev and prod environments. Iâ€™m digging into configs and existing docs to define the requirements before writing tests or code.
exec
bash -lc 'ls docs' in /var/home/rick/Development/mega-directory succeeded in 38ms:
AGENTS.md

thinking
**Planning API integration with multiple targets**

I'm figuring out how to implement HTTP posting to API endpoints with tokens and how to support both dev and prod targets, as the task suggests validating integration against both. Since the current config only has a single string endpoint, I want to check log files for any hidden spec or clues about handling multiple API targets before moving forward.
exec
bash -lc "sed -n '1,200p' codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log" in /var/home/rick/Development/mega-directory succeeded in 38ms:
OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a7285-13cc-7bf1-848a-95b71177d0d9
--------
user
Implement the functionality described in codex/TASKS/12_post_listings_from_crawler_to_api_server.yaml, then verify with any available tests.

thinking
**Preparing to inspect task instructions**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 43ms:
admin
agents

thinking
**Analyzing API integration and config ambiguities**

I'm piecing together log details and repo clues to understand how to push crawler data to dev and prod API targets. The config seems outdated or inconsistent, so I'll design a flexible approach supporting multiple API targets with a fallback to existing settings while figuring out the correct endpoints and tokens to use.
exec
bash -lc 'cat docker-compose.yml' in /var/home/rick/Development/mega-directory succeeded in 102ms:
version: '3.8'

services:
  api:
    build:
      context: ./api
    ports:
      - '3001:3001'
    environment:
      PORT: 3001
      DATABASE_URL: postgresql://postgres:password@db:5432/mega_directory
      ADMIN_JWT_SECRET: local-dev-secret
      ADMIN_LOGIN_EMAIL: admin@example.com
      ADMIN_LOGIN_PASSCODE: localpass
      CRAWLER_BEARER_TOKEN: crawler-dev-token
      ADMIN_TOKEN_TTL_SECONDS: 900
    depends_on:
      db:
        condition: service_healthy
    healthcheck:

thinking
**Verifying API endpoint and port usage**
exec
bash -lc 'rg -n "/api/new-listings" -n' in /var/home/rick/Development/mega-directory succeeded in 48ms:
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:202:codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:208:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:209:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:220:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:244:codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:202:codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:245:codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:208:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:246:codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:209:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:247:codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:220:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:249:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:250:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:251:codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:121:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:252:codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:253:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:253:codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:83:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:254:codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:114:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:255:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:392:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:257:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:416:agents/crawler/targets.example.json:2:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:258:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:417:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:392:  "api_endpoint": "http://localhost:3001/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:260:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:419:agents/crawler/config/targets.json:2:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:261:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:426:  "api_endpoint": "http://localhost:3000/api/new-listings",
codex/LOGS/tasklog_12_post_listings_from_crawler_to_api_server_1762857915.log:262:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:449:Iâ€™m mapping out a new ingestion endpoint for listings, aiming for a consistent, restful route like `/v1/crawler/listings` despite crawler config using `/api/new-listings`. Iâ€™m planning to implement validation, batch support, default inactive status, and store listings in a modular ingestion store hooked to `app.locals`. Next, Iâ€™ll follow TDD with tests for auth, payload validation, and storage, and outline a multi-step plan before coding. Letâ€™s explore this carefully!

thinking
**Analyzing API integration and testing strategy**

I'm piecing together that the crawler should send listing data to a new RESTful API endpoint, potentially supporting multiple targets like dev and prod based on configuration. Thereâ€™s a need to adapt payloads to the APIâ€™s expected format and implement tests verifying HTTP posts to both single and multiple endpoints, ensuring backward compatibility and proper integration validation. I want to check API test files next to confirm payload details and expected behavior.
exec
bash -lc 'ls api/test' in /var/home/rick/Development/mega-directory succeeded in 39ms:
server.test.js
exec
bash -lc "sed -n '1,200p' api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
const test = require('node:test');
const assert = require('node:assert');
const jwt = require('jsonwebtoken');
const { createServer } = require('..');

const baseConfig = Object.freeze({
  adminJwtSecret: 'test-admin-secret',
  adminJwtIssuer: 'mega-directory',
  adminJwtAudience: 'admin',
  adminLoginEmail: 'admin@example.com',
  adminLoginPasscode: 'letmein',
  crawlerBearerToken: 'crawler-token'
});

function findRoute(app, method, path) {
  const stack = (app._router && app._router.stack) || (app.router && app.router.stack) || [];
  const methodName = method.toLowerCase();
  return stack.find(
    (layer) => layer.route && layer.route.path === path && layer.route.methods[methodName]
  );

thinking
**Planning to check ingestion tests**
exec
bash -lc "sed -n '200,400p' api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
  const res = runRoute(
    route,
    createRequest({ method: 'POST', headers: { Authorization: 'Bearer wrong-token' } }),
    createResponse()
  );

  assert.strictEqual(res.statusCode, 401);
  assert.strictEqual(res.body.error, 'Crawler token missing or invalid');
});

test('crawler ping accepts matching bearer token', () => {
  const app = createServer(baseConfig);
  const route = findRoute(app, 'post', '/v1/crawler/ping');
  const req = createRequest({
    method: 'POST',
    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` }
  });
  const res = runRoute(route, req, createResponse());

  assert.strictEqual(res.statusCode, 200);


thinking
**Designing crawler API ingestion**
thinking
**Refining ingestion payload construction and slug logic**

Iâ€™m working out details to safely build listing payloads by copying and cleaning fields, ensuring required titles and category slugs are set by slugifying target data, and deciding which optional fields to include. Iâ€™m also planning the slugify helper and how to integrate HTTP posting with proper headers, while keeping existing tests and config compatible. Next, Iâ€™ll draft a detailed plan to add tests, implement posting, update configs if needed, and run tests.
Plan update
  â€¢ Add crawler tests covering API posting (single and multi-target/dev+prod).
  â€¢ Implement crawler ingestion client + payload mapping to satisfy new tests.
  â€¢ Update configs/examples if needed and run crawler test suite.

thinking
**Extending tests with POST support**

I'll update the crawler tests by enhancing `DummySession` to record POST calls alongside GETs, adding a `post` method that logs requests and returns a simple response with `raise_for_status`. Then, I'll add new tests to verify that the crawler posts batches correctly to configured API endpoints, ensuring headers, payload structure, and slugification behave as expected while keeping existing tests intact.
file update
M /var/home/rick/Development/mega-directory/agents/crawler/tests/test_crawler.py
@@ -44,2 +44,3 @@
         self.calls = []
+        self.post_calls = []
 
@@ -49,2 +50,13 @@
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
@@ -207 +219,65 @@
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M agents/crawler/tests/test_crawler.py
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Preparing to run tests**
exec
bash -lc 'pytest agents/crawler/tests' in /var/home/rick/Development/mega-directory exited 1 in 2.04s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 7 items

agents/crawler/tests/test_crawler.py .....FF                             [100%]

=================================== FAILURES ===================================
________________ test_run_crawler_posts_batches_to_api_endpoint ________________

    def test_run_crawler_posts_batches_to_api_endpoint() -> None:
        config = {
            "api_endpoint": "https://api.example.com/v1/crawler/listings",
            "api_token": "crawler-token",
            "targets": [
                {
                    "category": "Electricians",
                    "locations": ["Holland MI"],
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }



thinking
**Planning crawler API ingestion**
thinking
**Detailing API payload posting implementation**

I'm working on implementing methods in the crawler to build and send API ingestion payloads per batch. This includes resolving API targets from config, building listing payloads with proper defaults and slugification, handling timeouts from multiple config layers, and posting via the session with auth headers. Iâ€™m planning focused incremental code patches to update imports, add the APITarget dataclass, and tightly integrate all helper methods while ensuring proper error handling and compatibility with custom session types.
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -3,2 +3,3 @@
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..56c9774c1712f1320a0e281b1e0dbec6d02438cc
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..56c9774c1712f1320a0e281b1e0dbec6d02438cc
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Organizing imports and adding dataclass**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -73,2 +73,10 @@
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..0e352b5881b968fe5319596bab6e8f8b70262456
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -70,6 +71,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..0e352b5881b968fe5319596bab6e8f8b70262456
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -70,6 +71,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Preparing to enhance Crawler.run method**

Iâ€™m planning to update the `run` method to handle API targets by resolving them upfront and posting batches to the API when applicable. I need to implement `_resolve_api_targets` to raise if no API endpoints are found, ensuring config correctness, and to carefully call `_post_batch_to_api` only when relevant. Iâ€™ll apply these changes with a patch in mind.
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -268,2 +268,3 @@
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
@@ -290,2 +291,8 @@
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..748ee4c74d06597ff06e6b929bcb7ea1900620cc
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -70,6 +71,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +266,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +289,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..748ee4c74d06597ff06e6b929bcb7ea1900620cc
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -70,6 +71,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +266,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +289,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Preparing helper methods and slug constants**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -43,2 +43,4 @@
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..cec57fc2d2b8f0e2f93819e9afd67fd70fa9b2ad
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..cec57fc2d2b8f0e2f93819e9afd67fd70fa9b2ad
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Adding multiple helper methods to Crawler class**

I'm preparing to insert a block of helper methods after the existing `_normalize_url` method in the `Crawler` class using `apply_patch`. These methods will handle API target resolution, API requests, payload construction, string normalization, slugifying, and timeouts, ensuring proper handling of configuration, inputs, and edge cases. I'll keep the indentation clean and ensure static methods and constants are used appropriately.
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -424,2 +424,175 @@
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Verifying payload construction and config handling**

Iâ€™m making sure `_build_listing_payload` correctly normalizes strings, preserves fields like `notes` and `categorySlug`, and handles edge cases like blank snippets or URLs with spaces. Iâ€™m also confirming that API target resolution properly raises on misconfiguration and that the payload mutations donâ€™t affect original listings. Before running tests, Iâ€™m double-checking typings and imports to keep the code clean and consistent.
exec
bash -lc 'pytest agents/crawler/tests' in /var/home/rick/Development/mega-directory exited 1 in 1.60s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 7 items

agents/crawler/tests/test_crawler.py ..F....                             [100%]

=================================== FAILURES ===================================
_____________________ test_run_crawler_uses_cached_session _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fe275da0b00>

    def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
        captured = {}
    
        class FakeCachedSession:
            def __init__(self, *args, **kwargs):
                captured["args"] = args
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..ee383ed2cf01e16f19b13c24ab71384d0280088e
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -205,3 +217,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Updating test to include post method**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/tests/test_crawler.py
@@ -133,2 +133,5 @@
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/tests/test_crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Rerunning tests**
exec
bash -lc 'pytest agents/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 1.68s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 7 items

agents/crawler/tests/test_crawler.py .......                             [100%]

============================== 7 passed in 0.81s ===============================
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Updating plan step statuses**
Plan update
  âœ“ Add crawler tests covering API posting (single and multi-target/dev+prod).
  âœ“ Implement crawler ingestion client + payload mapping to satisfy new tests.
  â†’ Update configs/examples if needed and run crawler test suite.
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Planning updates to crawler target configs**
exec
bash -lc 'cat agents/crawler/config/targets.json' in /var/home/rick/Development/mega-directory succeeded in 41ms:
{
  "api_endpoint": "http://localhost:3000/api/new-listings",
  "api_token": "YOUR_API_TOKEN_HERE",
  "targets": [
    {
      "category": "Electricians",
      "locations": [
        "Holland MI"
      ],
      "subdomain": "electricians.example.com",
      "keywords": [
        "licensed",
        "residential",
        "commercial"
      ],
      "listings_per_location": 50,
      "fields": {
        "title": {
          "source": "scrape"
        },
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Updating API target configuration with new endpoints**

I'm restructuring the config to replace old `api_endpoint` and `api_token` keys with a new `api_targets` array illustrating dev and prod endpoints with placeholder tokens. This avoids confusion and aligns with the new specification while ensuring backward compatibility checks remain unaffected. I'll update the `targets.json` file accordingly and confirm JSON validity.
file update
M /var/home/rick/Development/mega-directory/agents/crawler/config/targets.json
@@ -1,4 +1,14 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
@@ -29,2 +39,2 @@
   ]
-}
\ No newline at end of file
+}
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/config/targets.json
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Reviewing target example JSON**
exec
bash -lc 'cat agents/crawler/targets.example.json' in /var/home/rick/Development/mega-directory succeeded in 38ms:
{
  "api_endpoint": "http://localhost:3001/api/new-listings",
  "api_token": "changeme",
  "targets": [
    {
      "category": "Real Estate",
      "locations": ["New York", "San Francisco"],
      "subdomain": "realestate.local",
      "keywords": ["ocean view", "downtown"],
      "listings_per_location": 50,
      "fields": {
        "title": { "source": "scrape" },
        "price": { "source": "scrape" },
        "description": {
          "source": "ai",
          "provider": "openai",
          "model": "gpt-4",
          "prompt_template": "Write a friendly listing description for a {{ category }} in {{ location }}. Details: {{ details }}."
        }
      }
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Updating API targets and route**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/targets.example.json
@@ -1,4 +1,14 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M agents/crawler/targets.example.json
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Verifying test coverage after config changes**
exec
bash -lc 'pytest agents/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 1.58s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 7 items

agents/crawler/tests/test_crawler.py .......                             [100%]

============================== 7 passed in 0.78s ===============================
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Updating final step status**
Plan update
  âœ“ Add crawler tests covering API posting (single and multi-target/dev+prod).
  âœ“ Implement crawler ingestion client + payload mapping to satisfy new tests.
  âœ“ Update configs/examples if needed and run crawler test suite.
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Gathering changed files from git**
exec
bash -lc 'git status -sb' in /var/home/rick/Development/mega-directory succeeded in 271ms:
## main...origin/main [ahead 3]
 M agents/crawler/config/targets.json
 M agents/crawler/crawler.py
 M agents/crawler/targets.example.json
 M codex/LOGS/codex_run_2025-11-11_04-26-54.log
 M codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log
?? agents/crawler/tests/
?? astro/src/components/DirectoryListing.astro
?? astro/src/components/DirectoryPage.astro
?? astro/src/data/
?? astro/src/lib/
?? astro/src/pages/listings/
?? astro/test/
?? codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log.done
?? codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log
?? codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log.done
?? codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log
?? codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log.done
?? codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log
?? codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log.done
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Noting untracked files and preparing line references**
exec
bash -lc "nl -ba agents/crawler/crawler.py | sed -n '250,520p'" in /var/home/rick/Development/mega-directory succeeded in 49ms:
   250	        return current
   251	
   252	
   253	class Crawler:
   254	    def __init__(
   255	        self,
   256	        session: Optional[requests.Session] = None,
   257	        cache_name: str = "crawler_cache",
   258	        expire_after: int = DEFAULT_CACHE_EXPIRE,
   259	        request_timeout: int = DEFAULT_TIMEOUT,
   260	        llm_client: Optional[LLMClient] = None,
   261	    ) -> None:
   262	        self.session = session or requests_cache.CachedSession(
   263	            cache_name=cache_name,
   264	            backend="sqlite",
   265	            expire_after=expire_after,
   266	        )
   267	        self.request_timeout = request_timeout
   268	        self.field_generator = FieldGenerator(llm_client=llm_client)
   269	
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Capturing specific line ranges**
exec
bash -lc "nl -ba agents/crawler/crawler.py | sed -n '430,700p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   430	            for entry in targets_config:
   431	                if not isinstance(entry, dict):
   432	                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
   433	                endpoint = entry.get("endpoint")
   434	                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
   435	                if not endpoint or not token:
   436	                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
   437	                timeout = self._coerce_positive_timeout(entry.get("timeout"))
   438	                resolved.append(
   439	                    APITarget(
   440	                        name=str(entry.get("name") or endpoint),
   441	                        endpoint=endpoint,
   442	                        token=token,
   443	                        timeout=timeout,
   444	                    )
   445	                )
   446	        else:
   447	            endpoint = config.get("api_endpoint")
   448	            token = config.get("api_token")
   449	            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Preparing line number references for summary**
exec
bash -lc "nl -ba agents/crawler/tests/test_crawler.py | sed -n '1,320p'" in /var/home/rick/Development/mega-directory succeeded in 40ms:
     1	from pathlib import Path
     2	from types import SimpleNamespace
     3	import sys
     4	
     5	import pytest
     6	
     7	AGENT_DIR = Path(__file__).resolve().parents[1]
     8	if str(AGENT_DIR) not in sys.path:
     9	    sys.path.insert(0, str(AGENT_DIR))
    10	
    11	import crawler
    12	from crawler import run_crawler
    13	
    14	
    15	SAMPLE_HTML = """
    16	<section>
    17	  <article data-listing>
    18	    <a class="listing-link" href="https://example.com/acme">
    19	      <span class="listing-title">ACME Electric</span>
    20	    </a>
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Reviewing lines for new tests**
exec
bash -lc "nl -ba agents/crawler/tests/test_crawler.py | sed -n '120,320p'" in /var/home/rick/Development/mega-directory succeeded in 40ms:
   120	    assert parsed[0].snippet == "Great service."
   121	
   122	
   123	def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
   124	    captured = {}
   125	
   126	    class FakeCachedSession:
   127	        def __init__(self, *args, **kwargs):
   128	            captured["args"] = args
   129	            captured["kwargs"] = kwargs
   130	
   131	        def get(self, *_args, **_kwargs):
   132	            return DummyResponse(SAMPLE_HTML)
   133	
   134	        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
   135	            return DummyResponse("")
   136	
   137	    monkeypatch.setattr(
   138	        crawler,
   139	        "requests_cache",
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }


thinking
**Summarizing final message structure**

I'm organizing the final response to clearly reference single start lines per file for code, tests, and config changes. I plan to use bullet points highlighting key updates with inline line references separated by commas. I'll include test runs and note the ValueError on missing API config, multi-target support, and updated configs. Also, I'll add a concise next steps section for running against real APIs and updating tokens. The format will respect instructions on header placement and avoiding line ranges.
codex
**Crawler API Posting**
- `agents/crawler/crawler.py:270` now resolves required API targets before crawling and posts each generated batch immediately, so misconfigured runs fail fast instead of silently skipping ingestion. 
- `agents/crawler/crawler.py:430` and `agents/crawler/crawler.py:491` add the ingestion client: multi-target Bearer auth, timeout resolution, and payload builders that default title/summary/sourceUrl, normalize category slugs, and attach source metadata for each listing before hitting `/v1/crawler/listings`.
- `agents/crawler/tests/test_crawler.py:123`, `agents/crawler/tests/test_crawler.py:225`, and `agents/crawler/tests/test_crawler.py:252` add coverage for posting behavior (including dev+prod fan-out) and ensure the cached session shim exposes `post`, enforcing the new contract.
- `agents/crawler/config/targets.json:1` and `agents/crawler/targets.example.json:1` now document dual dev/prod `api_targets` pointing at the crawler ingestion route so local configs mirror the new capabilities.

Tests
- `pytest agents/crawler/tests`

Next Steps
1. Drop real dev/prod bearer tokens/endpoints into `agents/crawler/config/targets.json` before running the crawler.
2. Run the crawler against a live API instance to confirm end-to-end ingestion beyond the mocked tests.
file update:
diff --git a/agents/crawler/config/targets.json b/agents/crawler/config/targets.json
index f497b08628ac4651493cbcd2995f0c041b43d435..5553670f589b19a367f05be0225034a900f3301a
--- a/agents/crawler/config/targets.json
+++ b/agents/crawler/config/targets.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3000/api/new-listings",
-  "api_token": "YOUR_API_TOKEN_HERE",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "YOUR_DEV_API_TOKEN"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.example.com/v1/crawler/listings",
+      "token": "YOUR_PROD_API_TOKEN"
+    }
+  ],
   "targets": [
     {
       "category": "Electricians",
@@ -27,4 +37,4 @@
       }
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 5ab36db4e9d6040381bf7ae5fa3c224234e2db36..3d1f3b345415af8939c4c3ba592510a0e75c8479
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
@@ -40,6 +41,8 @@
     "link": "a",
     "description": ".listing-description, p",
 }
+MAX_SLUG_LENGTH = 80
+_SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
 
 
 @dataclass
@@ -70,6 +73,14 @@
 
 
 @dataclass
+class APITarget:
+    name: str
+    endpoint: str
+    token: str
+    timeout: Optional[int] = None
+
+
+@dataclass
 class LLMRequest:
     provider: str
     model: str
@@ -257,6 +268,7 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
+        api_targets = self._resolve_api_targets(config)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -279,6 +291,12 @@
                         metadata=meta,
                     )
                 )
+                self._post_batch_to_api(
+                    batches[-1],
+                    target,
+                    api_targets,
+                    config,
+                )
         return batches
 
     def fetch_listings(
@@ -404,6 +422,179 @@
         base = Crawler._normalize_subdomain(subdomain) or "https://"
         return urljoin(base + "/", href.lstrip("/"))
 
+    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
+        targets_config = config.get("api_targets")
+        resolved: List[APITarget] = []
+
+        if isinstance(targets_config, list) and targets_config:
+            for entry in targets_config:
+                if not isinstance(entry, dict):
+                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
+                endpoint = entry.get("endpoint")
+                token = entry.get("token") or entry.get("api_token") or entry.get("bearer_token")
+                if not endpoint or not token:
+                    raise ValueError("Each api_targets entry must include 'endpoint' and 'token'")
+                timeout = self._coerce_positive_timeout(entry.get("timeout"))
+                resolved.append(
+                    APITarget(
+                        name=str(entry.get("name") or endpoint),
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+        else:
+            endpoint = config.get("api_endpoint")
+            token = config.get("api_token")
+            timeout = self._coerce_positive_timeout(config.get("api_request_timeout"))
+            if endpoint and token:
+                resolved.append(
+                    APITarget(
+                        name="default",
+                        endpoint=endpoint,
+                        token=token,
+                        timeout=timeout,
+                    )
+                )
+
+        if not resolved:
+            raise ValueError(
+                "Crawler config must define api_endpoint/api_token or an api_targets list to POST listings"
+            )
+        return resolved
+
+    def _post_batch_to_api(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+        api_targets: List[APITarget],
+        root_config: Dict[str, Any],
+    ) -> None:
+        if not batch.listings or not api_targets:
+            return
+        payloads = self._build_ingestion_payloads(batch, target_config)
+        if not payloads:
+            return
+        for api_target in api_targets:
+            timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            response = self.session.post(
+                api_target.endpoint,
+                json={"listings": payloads},
+                headers={
+                    "Authorization": f"Bearer {api_target.token}",
+                    "Content-Type": "application/json",
+                },
+                timeout=timeout,
+            )
+            response.raise_for_status()
+
+    def _build_ingestion_payloads(
+        self,
+        batch: CrawlerBatch,
+        target_config: Dict[str, Any],
+    ) -> List[Dict[str, Any]]:
+        category_slug = self._resolve_category_slug(target_config)
+        payloads: List[Dict[str, Any]] = []
+        for listing in batch.listings:
+            payload = self._build_listing_payload(listing, target_config, category_slug)
+            if payload:
+                payloads.append(payload)
+        return payloads
+
+    def _build_listing_payload(
+        self,
+        listing: Listing,
+        target_config: Dict[str, Any],
+        default_category_slug: str,
+    ) -> Optional[Dict[str, Any]]:
+        payload: Dict[str, Any] = dict(listing.fields or {})
+        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
+        if not title:
+            return None
+        payload["title"] = title
+
+        summary = self._normalize_string(payload.get("summary"))
+        snippet = self._normalize_string(listing.snippet)
+        if not summary and snippet:
+            payload["summary"] = snippet
+
+        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
+        if source_url:
+            payload["sourceUrl"] = source_url
+
+        category_slug = payload.get("categorySlug") or default_category_slug
+        normalized_category = self._slugify(category_slug)
+        if not normalized_category:
+            raise ValueError("Unable to determine category slug for API payload")
+        payload["categorySlug"] = normalized_category
+
+        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
+        if source_name:
+            payload["sourceName"] = source_name
+
+        return {key: value for key, value in payload.items() if value is not None}
+
+    def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
+        slug_source = (
+            target_config.get("category_slug")
+            or target_config.get("categorySlug")
+            or target_config.get("category")
+            or ""
+        )
+        slug = self._slugify(slug_source)
+        if not slug:
+            raise ValueError("Crawler target must define a category to derive categorySlug")
+        return slug
+
+    def _resolve_source_name(self, target_config: Dict[str, Any]) -> Optional[str]:
+        for key in ("source_name", "sourceName", "subdomain", "category"):
+            value = self._normalize_string(target_config.get(key))
+            if value:
+                return value
+        return None
+
+    def _resolve_api_timeout(
+        self,
+        api_target: APITarget,
+        target_config: Dict[str, Any],
+        root_config: Dict[str, Any],
+    ) -> float:
+        candidates = (
+            api_target.timeout,
+            target_config.get("api_request_timeout"),
+            root_config.get("api_request_timeout"),
+            target_config.get("request_timeout"),
+            root_config.get("request_timeout"),
+        )
+        for candidate in candidates:
+            coerced = self._coerce_positive_timeout(candidate)
+            if coerced is not None:
+                return coerced
+        return self.request_timeout
+
+    @staticmethod
+    def _coerce_positive_timeout(value: Any) -> Optional[float]:
+        if isinstance(value, (int, float)) and value > 0:
+            return float(value)
+        return None
+
+    @staticmethod
+    def _normalize_string(value: Any) -> Optional[str]:
+        if isinstance(value, str):
+            stripped = value.strip()
+            return stripped or None
+        return None
+
+    @staticmethod
+    def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
+        if not isinstance(value, str):
+            return ""
+        normalized = _SLUG_INVALID_CHARS.sub("-", value.lower())
+        normalized = normalized.strip("-")
+        if max_length and len(normalized) > max_length:
+            normalized = normalized[:max_length].rstrip("-")
+        return normalized
+
 
 def run_crawler(
     config: Dict[str, Any],
diff --git a/agents/crawler/targets.example.json b/agents/crawler/targets.example.json
index dcdf3729b94412cdf1036467ddad23ae07534dab..b98ae37a56591370df6217ab8d504fdab790d5a8
--- a/agents/crawler/targets.example.json
+++ b/agents/crawler/targets.example.json
@@ -1,6 +1,16 @@
 {
-  "api_endpoint": "http://localhost:3001/api/new-listings",
-  "api_token": "changeme",
+  "api_targets": [
+    {
+      "name": "dev",
+      "endpoint": "http://localhost:3001/v1/crawler/listings",
+      "token": "dev-token"
+    },
+    {
+      "name": "prod",
+      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
+      "token": "prod-token"
+    }
+  ],
   "targets": [
     {
       "category": "Real Estate",
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 296cd1da4c2f3c1562aede3236021ad5b2a355b1..2c90608e41ea1976bae6e77c4cc8feecf85d05e1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -42,11 +42,23 @@
     def __init__(self, text: str) -> None:
         self.text = text
         self.calls = []
+        self.post_calls = []
 
     def get(self, url: str, timeout: float = 10) -> DummyResponse:
         self.calls.append((url, timeout))
         return DummyResponse(self.text)
 
+    def post(self, url: str, json=None, headers=None, timeout: float = 10) -> DummyResponse:  # type: ignore[no-untyped-def]
+        self.post_calls.append(
+            {
+                "url": url,
+                "json": json,
+                "headers": dict(headers or {}),
+                "timeout": timeout,
+            }
+        )
+        return DummyResponse("")
+
 
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
@@ -119,6 +131,9 @@
         def get(self, *_args, **_kwargs):
             return DummyResponse(SAMPLE_HTML)
 
+        def post(self, *_args, **_kwargs):  # type: ignore[no-untyped-def]
+            return DummyResponse("")
+
     monkeypatch.setattr(
         crawler,
         "requests_cache",
@@ -205,3 +220,67 @@
 
     with pytest.raises(RuntimeError):
         run_crawler(config, session=DummySession(SAMPLE_HTML))
+
+
+def test_run_crawler_posts_batches_to_api_endpoint() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    batches = run_crawler(config, session=session)
+
+    assert len(batches) == 1
+    assert len(session.post_calls) == 1
+    payload = session.post_calls[0]["json"]
+    assert payload["listings"][0]["title"] == "ACME Electric"
+    assert payload["listings"][0]["categorySlug"] == "electricians"
+    assert payload["listings"][0]["summary"] == "Licensed and insured electricians."
+    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
+    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
+
+
+def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            },
+            {
+                "name": "prod",
+                "endpoint": "https://prod.example.com/v1/crawler/listings",
+                "token": "prod-token",
+            },
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+
+    run_crawler(config, session=session)
+
+    assert len(session.post_calls) == 2
+    assert {
+        call["headers"]["Authorization"] for call in session.post_calls
+    } == {"Bearer dev-token", "Bearer prod-token"}
+    assert {
+        call["url"] for call in session.post_calls
+    } == {
+        "https://dev.example.com/v1/crawler/listings",
+        "https://prod.example.com/v1/crawler/listings",
+    }

tokens used
234,789
**Crawler API Posting**
- `agents/crawler/crawler.py:270` now resolves required API targets before crawling and posts each generated batch immediately, so misconfigured runs fail fast instead of silently skipping ingestion. 
- `agents/crawler/crawler.py:430` and `agents/crawler/crawler.py:491` add the ingestion client: multi-target Bearer auth, timeout resolution, and payload builders that default title/summary/sourceUrl, normalize category slugs, and attach source metadata for each listing before hitting `/v1/crawler/listings`.
- `agents/crawler/tests/test_crawler.py:123`, `agents/crawler/tests/test_crawler.py:225`, and `agents/crawler/tests/test_crawler.py:252` add coverage for posting behavior (including dev+prod fan-out) and ensure the cached session shim exposes `post`, enforcing the new contract.
- `agents/crawler/config/targets.json:1` and `agents/crawler/targets.example.json:1` now document dual dev/prod `api_targets` pointing at the crawler ingestion route so local configs mirror the new capabilities.

Tests
- `pytest agents/crawler/tests`

Next Steps
1. Drop real dev/prod bearer tokens/endpoints into `agents/crawler/config/targets.json` before running the crawler.
2. Run the crawler against a live API instance to confirm end-to-end ingestion beyond the mocked tests.
