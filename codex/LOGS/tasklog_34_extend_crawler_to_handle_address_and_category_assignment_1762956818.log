OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a786a-3a39-7832-9570-8eba8aad755c
--------
user
Implement the functionality described in codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml, then verify with any available tests.
ERROR: MCP client for `mcp` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:
[mcp_servers.mcp]
startup_timeout_sec = XX
ERROR: MCP client for `vibe-check-mcp-server` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:
[mcp_servers.vibe-check-mcp-server]
startup_timeout_sec = XX
ERROR: MCP client for `exa` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:
[mcp_servers.exa]
startup_timeout_sec = XX

thinking
**Exploring task instructions and repo**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 180ms:
AGENTS.md
apps
codex
codex.toml
codex.yaml
db
docker-compose.yml
docs
docs.tar
env.json
Makefile
package.json
packages
README.md
scripts
tests
tsconfig.json

thinking
**Listing task files**
exec
bash -lc 'ls codex/TASKS' in /var/home/rick/Development/mega-directory succeeded in 42ms:
00_prepare_repository.yaml
01_initialize_astro_frontend.yaml
02_setup_api_server_with_express_and_jwt_auth.yaml
03_design_postgresql_schema.yaml
04_implement_listing_ingestion_api_endpoint.yaml
05_admin_auth_route_and_middleware.yaml
06_frontend_directory_page_rendering.yaml
07_frontend_subcategory_ui_and_layout.yaml
08_setup_featured_listings_logic.yaml
09_add_category_meta_and_seo_controls.yaml
10_scaffold_python_crawler_agent.yaml
11_llm_field_generation_via_jinja2.yaml
12_post_listings_from_crawler_to_api_server.yaml
13_create_json_crawler_configuration_schema.yaml
14_scaffold_admin_ui_application.yaml
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml

thinking
**Opening start of file 34**
exec
bash -lc "sed -n '1,200p' codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml" in /var/home/rick/Development/mega-directory succeeded in 43ms:
id: 34
filename: 34_extend_crawler_to_handle_address_and_category_assignment.yaml
title: Extend Crawler to Handle Address and Category Assignment
description: >
- Extend Python crawler logic to support tagging listings with categories and addresses.
- Parse and infer location and category from HTML/CSS selectors or text patterns.
- Attach LLM-assisted classification for edge cases.
- additional information regarding entire development project this task is part of can be referenced in `codex/TASKS/codex_phase_1b_2.yaml`
dependencies: []

thinking
**Searching for crawler components**
exec
bash -lc 'rg -n "crawler" -g"*.py" -n' in /var/home/rick/Development/mega-directory succeeded in 96ms:
apps/crawler/dev_runner.py:3:"""Developer-friendly crawler loop that replays sample HTML into the local API."""
apps/crawler/dev_runner.py:17:from crawler import run_crawler
apps/crawler/dev_runner.py:47:    """Simple response shim so crawler parsing works without HTTP."""
apps/crawler/dev_runner.py:106:    logger = logging.getLogger("mega_directory.dev_crawler")
apps/crawler/dev_runner.py:124:        or "http://localhost:3001/v1/crawler/listings",
apps/crawler/dev_runner.py:125:        help="Crawler ingestion endpoint (defaults to http://localhost:3001/v1/crawler/listings).",
apps/crawler/dev_runner.py:131:        or "crawler-dev-token",
apps/crawler/dev_runner.py:132:        help="Bearer token used for crawler POST requests.",
apps/crawler/dev_runner.py:147:        help="Execute a single crawler run instead of looping forever.",
apps/crawler/dev_runner.py:168:            logger.info("Starting dev crawler run #%s", iteration)
apps/crawler/dev_runner.py:171:                run_crawler(config, session=session, logger=logger)
apps/crawler/dev_runner.py:174:            logger.info("Finished dev crawler run #%s", iteration)
apps/crawler/dev_runner.py:176:            logger.info("Interrupted, exiting crawler loop.")
apps/crawler/tests/test_config_schema.py:17:    """Load the crawler configuration schema once per test module."""
apps/crawler/tests/test_config_schema.py:41:                "endpoint": "https://dev.example.com/v1/crawler/listings",
apps/crawler/tests/test_crawler.py:11:import crawler
apps/crawler/tests/test_crawler.py:12:from crawler import run_crawler
apps/crawler/tests/test_crawler.py:108:def test_run_crawler_batches_locations_and_limits_results() -> None:
apps/crawler/tests/test_crawler.py:124:    batches = run_crawler(config, session=dummy_session)
apps/crawler/tests/test_crawler.py:158:    crawler_instance = crawler.Crawler(session=DummySession(html))

thinking
**Inspecting crawler code**
exec
bash -lc 'rg -n "address" -n apps/crawler -g"*.py"' in /var/home/rick/Development/mega-directory exited 1 in 45ms:


thinking
**Checking ripgrep search results**
exec
bash -lc 'rg -n "address" apps/crawler' in /var/home/rick/Development/mega-directory exited 1 in 45ms:


thinking
**Searching entire repo for "address"**
exec
bash -lc 'rg -n "address"' in /var/home/rick/Development/mega-directory succeeded in 462ms:
tests/listings_table_enhancements.test.js:77:test('migration adds listing_addresses table with geo references', () => {
tests/listings_table_enhancements.test.js:84:  expectColumn(block, 'addressLine1', 'TEXT');
tests/listings_table_enhancements.test.js:85:  expectColumn(block, 'addressLine2', 'TEXT');
tests/listings_table_enhancements.test.js:103:    'Expected FK from listing_addresses.listingId to Listing.id'
tests/listings_table_enhancements.test.js:108:    'Expected FK from listing_addresses.cityId to City.id'
tests/listings_table_enhancements.test.js:113:    'Expected FK from listing_addresses.postalCodeId to PostalCode.id'
tests/listings_table_enhancements.test.js:122:    /addresses\s+ListingAddress\[\]/,
tests/listings_table_enhancements.test.js:123:    'Listing model should expose addresses relation'
tests/listings_table_enhancements.test.js:131:  const addressBlock = extractPrismaModel(schema, 'ListingAddress');
tests/listings_table_enhancements.test.js:132:  assert.match(addressBlock, /listingId\s+Int/, 'ListingAddress needs listingId field');
tests/listings_table_enhancements.test.js:133:  assert.match(addressBlock, /listing\s+Listing\s+@relation/, 'ListingAddress should relate to Listing');
docs/development_phase_1b.md:15:### Listing and physical_address tables
docs/development_phase_1b.md:17:While we will be collecting a bunch of listings which are associated with various categories and locations. For some listings it may actually make sense to associate them with more than one category. It may even make sense to associate some listings with multiple addresses, but addresses should be associated only with a single listing (1234 some st. suite 1 and 1234 some st. suite 2 are separate addresses) So the *physical_address* table should have a foreign key on the *listing* table while listings and categories should be related via a join table.
docs/development_phase_1b.md:36:Not all listings will be tied to a specific location... buisinessess and organizations that operate purely over the internet may not list an address, and it may just not make any sense for us to use one in our data presentation to begin with. If an address is available, we should probably store it, but for certain categories, we can probably treat all of the listings as location agnostic. But let's leave that to be determined by the directory configuration. Each listing should have a latitude and longitude value, this should be obtainable from the google maps api or some other such service. A quick google search turns up: MapQuest API, Geocodio, geocode.maps.co, locationIQ. 
docs/development_phase_1b.md:40:We shouldn't need api endpoints to add/edit/delete values for the postal code, city, state/province, or country tables since these will be populated and stay fixed in the long term. But we will need add edit and delete functions for all of the following entities: listings, physical_addresses, categories, directory pages
docs/development_phase_1b.md:47:It may not work well to automatically crawl the web for listings, and it may make a lot more sense if it were possible to provide listing data manually gathered from around the web. What would make this very easy to manage is if there were a very robust means of parsing through such data from a wide variety of sources. Beautiful soup scripts should be able to handle the most common sources of html data. And some data may come in the form of a csv, or a json array for which we arrive at a standardized formatting. But there should also be an option for parsing particulary messy data that it be sent through an inexpensive LLM endpoint. Maybe a gpt-4 or one of the free models available from openrouter. Those models may not be particulary effective at reasoning or coding, but they can certainly identify urls, titles, descriptions, addresses and other such data among large blocks of text in varied formats. In addition to the crawler scripts, we need to generate another one that accepts a path to a text file as an argument or reads from standard in, html crawlable by beautiful soup, serialized data like json, or raw unformatted text to be parsed by an llm (defaulting to one of the free openrouter models) there should be an argument to the script where the format of the input is specified as such. Any code that currently exists to generate short and long descriptions or other post parsing of the individual fields should be moved into a separate module that can be shared between the crawler and this new data collection method script.
docs/development_phase_1b.md:54:I was thinking things over and I think it might be a good idea, maybe as a phase 2, and not until we have everything else working, but I would like to allow for the creation of user accounts. This would allow some interesting functionality that could encourage users to share links to our directories generating significant organic traffic. I envision a user creation system similar to reddit where upon using a function of the site requiring a login, the site should automatically generate an account stub, offering the user a unique token and automatically generated username they can store and use to restore their session the next time they visit the site. Along with the presentation of the token, the user should also be given the option to optionally supply their own username, email address, and password. Users who supply only an email will be assigned the generated username, if they supply a username that username will be assigned and they will be able to login at a later date by entering either their email address or their username, this will bring them to a form where they can enter a code which will be sent to their email address at each login. Once logged in, these users will be able to create a password from the admin panel at any time in the future. Users who supply a password will be able to login using their email or username / password pair on their next visit. I think this workflow overcomes many of the psychological barriers to joining websites since there are so many different ways to end up with an account. None requiring all of the steps be completed at once like traditional website registration.
docs/development_phase_1b.md:60:Listings could have an *add to list* function. I envision a small icon that when clicked expands a box with a list of the lists created by the user and a textfield/button at the end to create a new list. Using async ajax type interaction the user could click a toggle button before one or more of the lists in the box and or enter the name of a new list and hit the button to add the listing to a new list. The toggles prior to the list names should display + or - depending on whether the item is already in that list or not and clicking on a - should remove the listing. Using these lists, users could create sharable collections of links at https://example.com/[username]/[list_name], for listings associated with addresses these pages could have buttons to download csv, kml, or other gps files containing their collections. 
docs/development_phase_1b.md:98:### Should geolocation be triggered automatically after a listing with address is saved, or should it be manual/admin-triggered?
docs/development_phase_1b.md:136:## Multiple Addresses: If a listing has multiple addresses, should all be displayed on the listing detail page, and should the listing appear under each relevant location-based directory page? (Likely yes.) Plan how to handle multiple map pins for one listing if needed.

thinking
**Narrowing search in crawler.py**
exec
bash -lc "sed -n '1,200p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
from __future__ import annotations

from dataclasses import dataclass, field
import logging
import os
import re
from typing import Any, Callable, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup
from jinja2 import Environment, StrictUndefined, TemplateError

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

thinking
**Extracting code sections for parsing**
exec
bash -lc "sed -n '200,400p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:
        provider = config.get("provider")
        model = config.get("model")
        prompt_template = config.get("prompt_template")
        if not provider or not model or not prompt_template:
            raise ValueError(
                f"AI field '{field_name}' requires provider, model, and prompt_template"
            )

        render_context = self._prepare_render_context(context, config)
        prompt = self._render_template(field_name, prompt_template, render_context)
        request = LLMRequest(
            provider=provider,
            model=model,
            prompt=prompt,
            field_name=field_name,
            options=config.get("options", {}),
            target=target,
            listing=context.get("listing"),
        )
        return self.llm_client(request)

thinking
**Continuing file view**
exec
bash -lc "sed -n '400,800p' apps/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 41ms:
            "subdomain": self._normalize_subdomain(target.get("subdomain")),
            "category": quote_plus(target.get("category", "")),
            "location": quote_plus(location),
            "keyword": quote_plus(keyword or ""),
        }
        url = template.format(**substitutions)
        if url.startswith("//"):
            url = f"https:{url}"
        if not url.startswith("http"):
            url = f"https://{url.lstrip('/')}"
        return url

    @staticmethod
    def _normalize_subdomain(value: Optional[str]) -> str:
        if not value:
            return ""
        if value.startswith("http://") or value.startswith("https://"):
            return value.rstrip("/")
        return f"https://{value.strip('/')}"


thinking
**Preparing to extend crawler tests**
exec
bash -lc "sed -n '1,200p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 44ms:
from pathlib import Path
from types import SimpleNamespace
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

import crawler
from crawler import run_crawler


SAMPLE_HTML = """
<section>
  <article data-listing>
    <a class="listing-link" href="https://example.com/acme">
      <span class="listing-title">ACME Electric</span>
    </a>

thinking
**Continuing to review tests**
exec
bash -lc "sed -n '200,400p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 39ms:
    batches = run_crawler(config)

    assert captured["kwargs"]["expire_after"] > 0
    assert len(batches) == 1


def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
    llm_requests = []

    def fake_llm_client(request):  # type: ignore[no-untyped-def]
        llm_requests.append(request)
        return f"LLM::{request.field_name}::{request.prompt}"

    config = {
        "api_endpoint": "https://api.example.com",
        "api_token": "token",
        "targets": [
            {
                "category": "Electricians",
                "subdomain": "electric.example.com",

thinking
**Continuing to search for tests**
exec
bash -lc "sed -n '400,800p' apps/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 40ms:


thinking
**Preparing to extend crawler functionality**
exec
bash -lc "sed -n '1,200p' codex/TASKS/codex_phase_1b_2.yaml" in /var/home/rick/Development/mega-directory succeeded in 39ms:
project: mega-directory
version: 1.0
phases:
  - phase: 1
    description: Core system buildout — listings pipeline, directory structure, admin tools, crawler, API, and frontend delivery

    components:
      api_server:
        language: TypeScript
        framework: Node/Express
        deployment: Railway
        endpoints:
          - listings: [GET, POST, PUT, DELETE]
          - addresses: [GET, POST, PUT, DELETE]
          - categories: [GET, POST, PUT, DELETE]
          - directory_pages: [GET, POST, PUT, DELETE]
          - directory_metadata: [GET, PUT]
        notes:
          - Postal, city, state/province, and country tables are loaded from external datasets.
          - Listings reference postal_code_id or city_id depending on geography.

thinking
**Preparing to extend crawler functionality**
exec
bash -lc 'ls apps/crawler' in /var/home/rick/Development/mega-directory succeeded in 46ms:
config
crawler.py
dev_runner.py
main.py
__pycache__
requirements-dev.txt
targets.example.json
tests

thinking
**Inspecting config directory**
exec
bash -lc 'ls apps/crawler/config' in /var/home/rick/Development/mega-directory succeeded in 39ms:
targets.json
targets.schema.json

thinking
**Inspecting schema for new fields**
exec
bash -lc 'cat apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 92ms:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://mega-directory.dev/schemas/crawler-targets.json",
  "title": "Mega Directory Crawler Configuration",
  "type": "object",
  "description": "Schema describing crawler API targets, scraping targets, and optional LLM field generation rules.",
  "additionalProperties": true,
  "properties": {
    "$schema": {
      "type": "string",
      "description": "Optional pointer for tooling to locate this schema definition."
    },
    "api_endpoint": {
      "type": "string",
      "format": "uri",
      "description": "Single API endpoint to POST crawler batches to when api_targets is omitted."
    },
    "api_token": {
      "$ref": "#/$defs/nonEmptyString",
      "description": "Bearer token paired with api_endpoint."

thinking
**Planning schema and extraction updates**
exec
bash -lc 'cat apps/crawler/targets.example.json' in /var/home/rick/Development/mega-directory succeeded in 41ms:
{
  "$schema": "./config/targets.schema.json",
  "api_targets": [
    {
      "name": "dev",
      "endpoint": "http://localhost:3001/v1/crawler/listings",
      "token": "dev-token"
    },
    {
      "name": "prod",
      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
      "token": "prod-token"
    }
  ],
  "targets": [
    {
      "category": "Real Estate",
      "locations": ["New York", "San Francisco"],
      "subdomain": "realestate.local",
      "keywords": ["ocean view", "downtown"],

thinking
**Exploring address handling and docs**
exec
bash -lc 'rg -n "crawler" -n docs' in /var/home/rick/Development/mega-directory succeeded in 43ms:
docs/development_phase_1b.md:47:It may not work well to automatically crawl the web for listings, and it may make a lot more sense if it were possible to provide listing data manually gathered from around the web. What would make this very easy to manage is if there were a very robust means of parsing through such data from a wide variety of sources. Beautiful soup scripts should be able to handle the most common sources of html data. And some data may come in the form of a csv, or a json array for which we arrive at a standardized formatting. But there should also be an option for parsing particulary messy data that it be sent through an inexpensive LLM endpoint. Maybe a gpt-4 or one of the free models available from openrouter. Those models may not be particulary effective at reasoning or coding, but they can certainly identify urls, titles, descriptions, addresses and other such data among large blocks of text in varied formats. In addition to the crawler scripts, we need to generate another one that accepts a path to a text file as an argument or reads from standard in, html crawlable by beautiful soup, serialized data like json, or raw unformatted text to be parsed by an llm (defaulting to one of the free openrouter models) there should be an argument to the script where the format of the input is specified as such. Any code that currently exists to generate short and long descriptions or other post parsing of the individual fields should be moved into a separate module that can be shared between the crawler and this new data collection method script.
docs/development_phase_1b.md:82:## Should directory pages be manually created only, or should the crawler ever propose new directory pages for review?
docs/development_phase_1b.md:152:Great — I’ll generate a full Codex task set (around 20 tasks) based on the complete updated functionality we’ve outlined, including all aspects from directory pages and geographic structuring to crawler design, admin review flow, and phased-in user accounts.
docs/monitoring.md:3:This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
docs/monitoring.md:22:- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
docs/monitoring.md:29:- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
docs/deployment/railway.md:16:- Environment secrets (JWT secret, admin login, crawler token, etc.).
docs/deployment/railway.md:53:| `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
docs/deployment/railway.md:100:3. **Lock down secrets:** Add the same admin and crawler secrets you use locally to the API service variables. None of these should be committed to git.
docs/deployment/railway.md:111:- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
docs/AGENTS.md:77:Serve listings to the frontend, receive data from crawler, and power the admin interface.
docs/AGENTS.md:88:* Accept and validate new data from the crawler

thinking
**Searching tasks for address details**
exec
bash -lc 'rg -n "address" codex/TASKS -n' in /var/home/rick/Development/mega-directory succeeded in 43ms:
codex/TASKS/work_list:10:32_create_admin_api_endpoints_for_listings_addresses_categories_directories.yaml
codex/TASKS/work_list:12:34_extend_crawler_to_handle_address_and_category_assignment.yaml
codex/TASKS/CODEX_TODO_PHASE1B.md:5:- [ ] 028_enhance_listing_model_with_multiple_categories_addresses
codex/TASKS/codex_phase_1b_2.yaml:14:          - addresses: [GET, POST, PUT, DELETE]
codex/TASKS/codex_phase_1b_2.yaml:21:          - Listings with multiple addresses appear once per address on location-aware directories, once total on agnostic ones.
codex/TASKS/codex_phase_1b_2.yaml:27:          - physical_addresses (FK: listing)
codex/TASKS/codex_phase_1b_2.yaml:89:        trigger: automatic on listing with address
codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml:2:filename: 34_extend_crawler_to_handle_address_and_category_assignment.yaml
codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml:5:- Extend Python crawler logic to support tagging listings with categories and addresses.
codex/TASKS/32_create_admin_api_endpoints_for_listings_addresses_categories_directories.yaml:2:filename: 32_create_admin_api_endpoints_for_listings_addresses_categories_directories.yaml
codex/TASKS/32_create_admin_api_endpoints_for_listings_addresses_categories_directories.yaml:5:- Implement secure CRUD endpoints for listings, addresses, categories, and directory pages.
codex/TASKS/30_set_up_geocoding_integration_with_fallback_support.yaml:6:- Automatically trigger geolocation on listing/address creation or update.
codex/TASKS/29_implement_listings_table_enhancements_for_multiple_categories_and_locations.yaml:7:- Support multiple physical addresses per listing (one-to-many)

thinking
**Checking task details**
exec
bash -lc "sed -n '1,200p' codex/TASKS/29_implement_listings_table_enhancements_for_multiple_categories_and_locations.yaml" in /var/home/rick/Development/mega-directory succeeded in 39ms:
id: 29
filename: 29_implement_listings_table_enhancements_for_multiple_categories_and_locations.yaml
title: Implement Listings Table Enhancements for Multiple Categories and Locations
description: >
Refactor listings schema to:
- Allow multiple categories per listing via join table
- Support multiple physical addresses per listing (one-to-many)
- Include optional default location via postal_code_id or city_id for location-agnostic behavior
- additional information regarding entire development project this task is part of can be referenced in `codex/TASKS/codex_phase_1b_2.yaml`
dependencies: []

thinking
**Reviewing crawler test config**
exec
bash -lc "sed -n '1,200p' apps/crawler/tests/test_config_schema.py" in /var/home/rick/Development/mega-directory succeeded in 46ms:
from __future__ import annotations

import json
from pathlib import Path

import jsonschema
import pytest


AGENT_DIR = Path(__file__).resolve().parents[1]
SCHEMA_PATH = AGENT_DIR / "config" / "targets.schema.json"
EXAMPLE_PATH = AGENT_DIR / "targets.example.json"


@pytest.fixture(scope="module")
def schema() -> dict:
    """Load the crawler configuration schema once per test module."""
    with SCHEMA_PATH.open("r", encoding="utf-8") as fp:
        return json.load(fp)


thinking
**Investigating category and address handling**
exec
bash -lc 'rg -n "category" apps/crawler -g"*.py"' in /var/home/rick/Development/mega-directory succeeded in 42ms:
apps/crawler/dev_runner.py:22:    <section data-category="electricians">
apps/crawler/dev_runner.py:86:                "category": "Electricians",
apps/crawler/tests/test_config_schema.py:47:                # Missing category and locations entries to trigger schema errors
apps/crawler/tests/test_config_schema.py:63:    assert "'category'" in messages
apps/crawler/tests/test_crawler.py:114:                "category": "Electricians",
apps/crawler/tests/test_crawler.py:128:    assert all(batch.category == "Electricians" for batch in batches)
apps/crawler/tests/test_crawler.py:149:        "category": "Consultants",
apps/crawler/tests/test_crawler.py:193:                "category": "Electricians",
apps/crawler/tests/test_crawler.py:218:                "category": "Electricians",
apps/crawler/tests/test_crawler.py:228:                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
apps/crawler/tests/test_crawler.py:252:                "category": "Electricians",
apps/crawler/tests/test_crawler.py:276:                "category": "Electricians",
apps/crawler/tests/test_crawler.py:290:    assert payload["listings"][0]["categorySlug"] == "electricians"
apps/crawler/tests/test_crawler.py:312:                "category": "Photographers",
apps/crawler/tests/test_crawler.py:345:                "category": "Photographers",
apps/crawler/crawler.py:38:    "{subdomain}/search?category={category}&location={location}&keyword={keyword}"
apps/crawler/crawler.py:61:    category: str
apps/crawler/crawler.py:69:            "category": self.category,
apps/crawler/crawler.py:147:            "category": target.get("category"),
apps/crawler/crawler.py:167:            "category": base_context.get("category") or "",

thinking
**Searching for API payload indicators**
exec
bash -lc 'rg -n "sourceUrl" -n' in /var/home/rick/Development/mega-directory succeeded in 47ms:
apps/crawler/tests/test_crawler.py:292:    assert payload["listings"][0]["sourceUrl"] == "https://example.com/acme"
apps/crawler/crawler.py:538:        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
apps/crawler/crawler.py:540:            payload["sourceUrl"] = source_url
apps/api/src/server.ts:34:  sourceUrl: NullableString;
apps/api/src/server.ts:62:  sourceUrl: NullableString;
apps/api/src/server.ts:540:  const sourceUrl = optionalUrl(payload.sourceUrl, 'sourceUrl', errors);
apps/api/src/server.ts:559:      sourceUrl,
apps/api/src/server.ts:582:        sourceUrl: listing.sourceUrl ?? null,
apps/api/src/server.ts:657:      sourceUrl: null,
apps/api/src/server.ts:870:      sourceUrl: validation.value.sourceUrl ?? null,
apps/api/src/server.ts:934:    if (validation.value.sourceUrl !== undefined) {
apps/api/src/server.ts:935:      listing.sourceUrl = validation.value.sourceUrl;
apps/api/src/server.ts:1334:    sourceUrl?: NullableString;
apps/api/src/server.ts:1384:  const sourceUrl = optionalUrl(payload.sourceUrl, 'sourceUrl', errors);
apps/api/src/server.ts:1410:      sourceUrl,
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:476:bash -lc 'rg -n "sourceUrl" -n' in /var/home/rick/Development/mega-directory
apps/api/test/server.test.js:80:    sourceUrl: 'https://source.test/acme',
db/migrations/001_core_schema.sql:124:  "sourceUrl" TEXT,
db/schema.prisma:287:  sourceUrl        String?
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:481:+    sourceUrl: 'https://source.test/acme',

thinking
**Examining API server address requirements**
exec
bash -lc 'rg -n "address" apps/api/src/server.ts' in /var/home/rick/Development/mega-directory succeeded in 41ms:
67:  addressIds: number[];
76:  addressLine1: NullableString;
77:  addressLine2: NullableString;
126:  addresses: ListingAddressRecord[];
662:      addressIds: [],
667:  const addresses: ListingAddressRecord[] = [
672:      addressLine1: '123 5th Ave',
673:      addressLine2: null,
685:  listings[0].addressIds.push(addresses[0].id);
690:    addresses,
694:    nextAddressId: addresses.length + 1
875:      addressIds: [],
880:    if (validation.value.addresses && validation.value.addresses.length > 0) {
881:      replaceListingAddresses(store, listing, validation.value.addresses);
946:    if (validation.value.addresses !== undefined) {
947:      replaceListingAddresses(store, listing, validation.value.addresses);
964:    store.addresses = store.addresses.filter((address) => address.listingId !== listingId);
970:  app.get('/v1/admin/addresses', adminAuth, (_req: Request, res: Response) => {
972:    res.json({ data: store.addresses.slice() });
975:  app.get('/v1/admin/addresses/:addressId', adminAuth, (req: Request, res: Response) => {

thinking
**Investigating address ingestion inputs**
exec
bash -lc 'rg -n "addresses" apps/api/test/server.test.js' in /var/home/rick/Development/mega-directory succeeded in 42ms:
346:test('crawler listing ingestion geocodes addresses via geocode maps', async () => {
621:test('admin listings support nested addresses and propagate to addresses endpoints', async () => {
648:        addresses: [
675:  assert.strictEqual(createRes.body.data.addresses.length, 2);
676:  assert.ok(createRes.body.data.addresses.every((addr) => addr.listingId === createRes.body.data.id));
678:  const addressesRoute = findRoute(app, 'get', '/v1/admin/addresses');
679:  const addressesRes = await runRoute(
680:    addressesRoute,
684:  const linkedAddresses = addressesRes.body.data.filter(
710:  const addressesAfterDelete = await runRoute(
711:    addressesRoute,
715:  assert.ok(!addressesAfterDelete.body.data.some((addr) => addr.listingId === createRes.body.data.id));

thinking
**Checking crawler ingestion test**
exec
bash -lc "sed -n '300,420p' apps/api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 40ms:

  assert.strictEqual(res.statusCode, 200);
  assert.deepStrictEqual(res.body, { status: 'crawler-ok' });
});

test('crawler listing ingestion rejects missing bearer token', async () => {
  const app = createServer(baseConfig);
  const route = findRoute(app, 'post', '/v1/crawler/listings');
  const req = createRequest({
    method: 'POST',
    body: buildListingPayload()
  });
  const res = await runRoute(route, req, createResponse());

  assert.strictEqual(res.statusCode, 401);
  assert.strictEqual(res.body.error, 'Crawler token missing or invalid');
});

test('crawler listing ingestion stores sanitized listings as inactive', async () => {
  const app = createServer(baseConfig);

thinking
**Searching for buildListingPayload usage**
exec
bash -lc 'rg -n "buildListingPayload" -n' in /var/home/rick/Development/mega-directory succeeded in 46ms:
apps/api/test/server.test.js:75:function buildListingPayload(overrides = {}) {
apps/api/test/server.test.js:310:    body: buildListingPayload()
apps/api/test/server.test.js:324:    body: buildListingPayload({ slug: 'ACME Electric!!', summary: 'Trusted' })
apps/api/test/server.test.js:366:    body: buildListingPayload({ location: buildLocation() })
apps/api/test/server.test.js:408:    body: buildListingPayload({ location: buildLocation({ city: 'Seattle', region: 'WA' }) })
apps/api/test/server.test.js:444:    body: buildListingPayload({ location: buildLocation({ city: 'Austin', region: 'TX' }) })
apps/api/test/server.test.js:465:      buildListingPayload({ title: 'Bright Builders', categorySlug: 'builders' }),
apps/api/test/server.test.js:466:      buildListingPayload({ title: 'Northern Lights', categorySlug: 'photographers', slug: null })
apps/api/test/server.test.js:496:    body: buildListingPayload({ slug: null })
apps/api/test/server.test.js:514:      buildListingPayload({ title: '  ', categorySlug: 'electricians' }),
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:168:      buildListingPayload({ title: 'Bright Builders', categorySlug: 'builders' }),
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:169:      buildListingPayload({ title: 'Northern Lights', categorySlug: 'photographers', slug: null })
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:554:    body: buildListingPayload()
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:566:**Searching for buildListingPayload usage**
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:568:bash -lc 'rg -n "buildListingPayload" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:476:+function buildListingPayload(overrides = {}) {
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:517:+function buildListingPayload(overrides = {}) {
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:558:+function buildListingPayload(overrides = {}) {
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:585:+    body: buildListingPayload()
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:599:+    body: buildListingPayload({ slug: 'ACME Electric!!', summary: 'Trusted' })

thinking
**Opening function definition in test file**
exec
bash -lc "sed -n '50,120p' apps/api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 41ms:
  const stack = (app._router && app._router.stack) || (app.router && app.router.stack) || [];
  const methodName = method.toLowerCase();
  return stack.find(
    (layer) => layer.route && layer.route.path === path && layer.route.methods[methodName]
  );
}

function createRequest({ method = 'GET', headers = {}, body } = {}) {
  const normalized = Object.fromEntries(
    Object.entries(headers).map(([key, value]) => [key.toLowerCase(), value])
  );

  return {
    method,
    headers: normalized,
    body,
    get(name) {
      return this.headers[name.toLowerCase()];
    },
    header(name) {

thinking
**Investigating address ingestion structure**
exec
bash -lc "sed -n '760,940p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 40ms:
    }
    const validation = validateCategoryPayload(req.body, 'update');
    if (!validation.valid) {
      return res.status(400).json({ error: 'Validation failed', details: validation.errors });
    }
    if (
      validation.value.slug &&
      store.categories.some((entry) => entry.slug === validation.value.slug && entry.id !== record.id)
    ) {
      return res
        .status(400)
        .json({ error: 'Validation failed', details: ['slug already exists'] });
    }
    if (validation.value.name !== undefined) {
      record.name = validation.value.name;
    }
    if (validation.value.slug) {
      record.slug = validation.value.slug;
    }
    if (validation.value.description !== undefined) {

thinking
**Searching for crawler references**
exec
bash -lc 'rg -n "crawler" apps/api/src/server.ts' in /var/home/rick/Development/mega-directory succeeded in 42ms:
150:  crawlerBearerToken: string;
198:    crawlerBearerToken: overrides.crawlerBearerToken ?? process.env.CRAWLER_BEARER_TOKEN ?? '',
227:  if (!baseConfig.crawlerBearerToken) {
260:  const crawlerAuth = requireCrawlerToken(config);
280:  app.post('/v1/crawler/ping', crawlerAuth, (_req, res) => {
281:    res.json({ status: 'crawler-ok' });
284:  app.post('/v1/crawler/listings', crawlerAuth, createListingIngestionHandler(app));
348:    if (!token || token !== config.crawlerBearerToken) {
448:        'Rejected crawler listings batch due to validation errors'
487:      'Accepted crawler listings batch'

thinking
**Examining createListingIngestionHandler function**
exec
bash -lc "sed -n '284,520p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 63ms:
  app.post('/v1/crawler/listings', crawlerAuth, createListingIngestionHandler(app));

  registerAdminRoutes(app, adminAuth);

  return app;
}

export function startServer() {
  const app = createServer();
  const { port } = getAppLocals(app).config;
  app.listen(port, () => {
    getAppLocals(app).logger.info(
      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
      `API server running at http://localhost:${port}`
    );
  });
}

if (require.main === module) {
  startServer();

thinking
**Inspecting validateListingPayload function**
exec
bash -lc "sed -n '520,720p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 46ms:
    return { valid: false, errors: ['Each listing must be an object'] };
  }

  const title = sanitizeNullableString(payload.title);
  if (!title) {
    errors.push('title is required');
  }

  const categorySlug = sanitizeNullableString(payload.categorySlug);
  if (!categorySlug) {
    errors.push('categorySlug is required');
  }

  const slugSource = sanitizeNullableString(payload.slug) ?? title;
  const slug = slugify(slugSource || '');
  if (!slug) {
    errors.push('slug is required');
  }

  const websiteUrl = optionalUrl(payload.websiteUrl, 'websiteUrl', errors);

thinking
**Investigating location field handling**
exec
bash -lc 'rg -n "sanitizeLocation" -n apps/api/src/server.ts' in /var/home/rick/Development/mega-directory succeeded in 41ms:
546:  const location = sanitizeLocation(payload.location);
1897:function sanitizeLocation(value: unknown): ListingLocation {

thinking
**Inspecting function near file end**
exec
bash -lc "sed -n '1880,1935p' apps/api/src/server.ts" in /var/home/rick/Development/mega-directory succeeded in 41ms:
function optionalEmail(value: unknown, fieldName: string, errors: string[]): NullableString {
  if (value === undefined || value === null) {
    return null;
  }
  if (typeof value !== 'string' || !value.trim()) {
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  const email = value.trim();
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  if (!emailRegex.test(email)) {
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  return email.toLowerCase();
}

function sanitizeLocation(value: unknown): ListingLocation {
  if (!isPlainObject(value)) {
    return null;

thinking
**Planning extended crawler for addresses and categories**
exec
bash -lc "sed -n '1,200p' codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log" in /var/home/rick/Development/mega-directory succeeded in 41ms:
OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a786a-3a39-7832-9570-8eba8aad755c
--------
user
Implement the functionality described in codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml, then verify with any available tests.
ERROR: MCP client for `mcp` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:
[mcp_servers.mcp]
startup_timeout_sec = XX
ERROR: MCP client for `vibe-check-mcp-server` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:
[mcp_servers.vibe-check-mcp-server]
startup_timeout_sec = XX
ERROR: MCP client for `exa` timed out after 10 seconds. Add or adjust `startup_timeout_sec` in your config.toml:

thinking
**Searching log for clues**
exec
bash -lc 'rg -n "address" codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log' in /var/home/rick/Development/mega-directory succeeded in 45ms:
13:Implement the functionality described in codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml, then verify with any available tests.
74:bash -lc "sed -n '1,200p' codex/TASKS/34_extend_crawler_to_handle_address_and_category_assignment.yaml" in /var/home/rick/Development/mega-directory succeeded in 43ms:
76:filename: 34_extend_crawler_to_handle_address_and_category_assignment.yaml
79:- Extend Python crawler logic to support tagging listings with categories and addresses.
113:bash -lc 'rg -n "address" -n apps/crawler -g"*.py"' in /var/home/rick/Development/mega-directory exited 1 in 45ms:
119:bash -lc 'rg -n "address" apps/crawler' in /var/home/rick/Development/mega-directory exited 1 in 45ms:
123:**Searching entire repo for "address"**
125:bash -lc 'rg -n "address"' in /var/home/rick/Development/mega-directory succeeded in 462ms:
126:tests/listings_table_enhancements.test.js:77:test('migration adds listing_addresses table with geo references', () => {
127:tests/listings_table_enhancements.test.js:84:  expectColumn(block, 'addressLine1', 'TEXT');
128:tests/listings_table_enhancements.test.js:85:  expectColumn(block, 'addressLine2', 'TEXT');
129:tests/listings_table_enhancements.test.js:103:    'Expected FK from listing_addresses.listingId to Listing.id'
130:tests/listings_table_enhancements.test.js:108:    'Expected FK from listing_addresses.cityId to City.id'
131:tests/listings_table_enhancements.test.js:113:    'Expected FK from listing_addresses.postalCodeId to PostalCode.id'
132:tests/listings_table_enhancements.test.js:122:    /addresses\s+ListingAddress\[\]/,
133:tests/listings_table_enhancements.test.js:123:    'Listing model should expose addresses relation'
134:tests/listings_table_enhancements.test.js:131:  const addressBlock = extractPrismaModel(schema, 'ListingAddress');
135:tests/listings_table_enhancements.test.js:132:  assert.match(addressBlock, /listingId\s+Int/, 'ListingAddress needs listingId field');
136:tests/listings_table_enhancements.test.js:133:  assert.match(addressBlock, /listing\s+Listing\s+@relation/, 'ListingAddress should relate to Listing');
137:docs/development_phase_1b.md:15:### Listing and physical_address tables

thinking
**Designing crawler address and category tagging**
exec
bash -lc 'rg -n "category assignment" -n' in /var/home/rick/Development/mega-directory succeeded in 48ms:
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:790:bash -lc 'rg -n "category assignment" -n' in /var/home/rick/Development/mega-directory

thinking
**Designing address and category extraction features**

I’m laying out a detailed plan to enhance the crawler with address parsing via CSS selectors and regex patterns, category rules with optional LLM classification fallback, and how to extend the Listing data structure to hold this new info properly while ensuring tests and config updates cover everything. It’s a thoughtful approach to integrate location and category extraction cleanly into the existing pipeline.
exec
bash -lc "rg -n \"Listing\\(\" -n" in /var/home/rick/Development/mega-directory succeeded in 54ms:
apps/crawler/crawler.py:354:                Listing(
apps/api/src/server.ts:826:    const data = store.listings.map((listing) => serializeListing(store, listing));
apps/api/src/server.ts:840:    return res.json({ data: serializeListing(store, listing) });
apps/api/src/server.ts:883:    return res.status(201).json({ data: serializeListing(store, listing) });
apps/api/src/server.ts:950:    return res.json({ data: serializeListing(store, listing) });
apps/api/src/server.ts:1252:function serializeListing(store: AdminStore, record: AdminListingRecord) {
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1642:+    const fallbackHero = pickNextListing(sortedListings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1712:+function pickNextListing(listings, usedSlugs) {
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1728:+    const fallback = pickNextListing(listings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1841:+    const fallbackHero = pickNextListing(sortedListings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1911:+function pickNextListing(listings, usedSlugs) {
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:1927:+    const fallback = pickNextListing(listings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2126:+    const fallbackHero = pickNextListing(sortedListings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2196:+function pickNextListing(listings, usedSlugs) {
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2212:+    const fallback = pickNextListing(listings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2436:+    const fallbackHero = pickNextListing(sortedListings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2506:+function pickNextListing(listings, usedSlugs) {
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2522:+    const fallback = pickNextListing(listings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2743:+    const fallbackHero = pickNextListing(sortedListings, usedSlugs);
codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log:2813:+function pickNextListing(listings, usedSlugs) {


thinking
**Designing Listing enhancements and classification workflow**

I’m outlining a detailed plan to extend the Listing dataclass with categories and location fields, update parsing to extract these from HTML elements using selectors and regex patterns, and integrate a category classification step leveraging LLMs with Jinja templating. This includes schema updates, new methods for extraction and normalization, and adapting FieldGenerator for template rendering and context building to support classification prompts, all while ensuring tests and payload construction align with these changes.
thinking
**Planning context and testing updates for listing enhancements**

This is a thorough exploration of how to integrate location data into listing context without conflicts, update parsing and classification flows, and design focused tests to verify address and category extraction using selectors, patterns, and LLM classification—plus schema and sample config updates. I’m preparing to create a multi-step plan to implement these changes carefully and validate them with pytest.
Plan update
  → Add failing tests covering address parsing, category rules, and classifier behavior in crawler and schema config.
  • Implement crawler logic and configuration schema updates for addresses, category assignment, and LLM classification support.
  • Update sample configs/docs if needed and ensure payload builder uses new data.
  • Run crawler test suite (pytest) and verify updates.

thinking
**Planning test-driven development**

I’m laying out a comprehensive TDD plan, starting with new tests that cover extended parsing, category classification (including LLM fallback), and schema validations before implementing features. This includes carefully designing test data, configs, and expected assertions, while coordinating sample config and schema updates to keep tests meaningful and eventually passing.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_crawler.py
@@ -31,2 +31,36 @@
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
@@ -167,2 +201,35 @@
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M apps/crawler/tests/test_crawler.py
file update:
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..ded1d48a943065ff7e0617d794b1876c2a5dcfe7
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 

file update:
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..ded1d48a943065ff7e0617d794b1876c2a5dcfe7
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 


thinking
**Adding tests for crawler classification and payloads**

I’m preparing three new tests to verify run_crawler’s handling of location payload from address selectors, category classification with a fallback to an LLM, and enforcing that classification requires an LLM client. I’ll place them after existing tests like `test_run_crawler_posts_batches_to_api_endpoint` and ensure configurations support the expected behavior. Next, I’ll use apply_patch to insert these tests properly.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_crawler.py
@@ -362,2 +362,90 @@
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M apps/crawler/tests/test_crawler.py
file update:
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Verifying payload integrity and adding schema test**

I’m checking that key elements like `payloads[2]`, `payload["location"]`, and `categorySlug` exist and conform as expected, along with ensuring regex patterns are correct for parsing. I’ll also add a new schema validation test for missing category slugs to strengthen config checks.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_config_schema.py
@@ -65 +65,28 @@
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
apply_patch(auto_approved=true) exited 0 in 45ms:
Success. Updated the following files:
M apps/crawler/tests/test_config_schema.py
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating plan progress**
Plan update
  ✓ Add failing tests covering address parsing, category rules, and classifier behavior in crawler and schema config.
  → Implement crawler logic and configuration schema updates for addresses, category assignment, and LLM classification support.
  • Update sample configs/docs if needed and ensure payload builder uses new data.
  • Run crawler test suite (pytest) and verify updates.
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing schema and crawler updates**
exec
bash -lc 'rg -n "as_payload" -n' in /var/home/rick/Development/mega-directory succeeded in 53ms:
apps/crawler/crawler.py:67:    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_34_extend_crawler_to_handle_address_and_category_assignment_1762956818.log:2258:bash -lc 'rg -n "as_payload" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:1674:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:1904:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:2281:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:2673:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:3058:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:3456:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:3858:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:4258:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:4660:+    def as_payload(self) -> Dict[str, Any]:
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:5053:+    def as_payload(self) -> Dict[str, Any]:
file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Investigating CrawlerBatch.as_payload usage**
exec
bash -lc 'rg -n "as_payload" -n apps/crawler/dev_runner.py' in /var/home/rick/Development/mega-directory exited 1 in 41ms:

file update:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [



thinking
**Implementing advanced listing parsing**

I'm adding robust support for location extraction, category rule matching, and classification within listing parsing. This involves extending the Listing dataclass, compiling regex patterns efficiently, enhancing context building in FieldGenerator, and carefully integrating category classification using LLMs. I'll also build utility helpers for regex compilation, text gathering, and location normalization to keep things clean and maintainable. It’s a big refactor aimed at making listings richer and more accurately categorized!
thinking
**Incorporating category classification into listing parsing**

I'm integrating classification calls after listings are fetched to assign categories using LLMs, ensuring listings without slugs get processed. This involves adding robust helper methods for regex compilation, category rule matching, location extraction, and classification slug resolution. I'll carefully handle token merging for prompts and ensure error cases like missing configs are raised. Logging will track regex compile failures. These additions prepare for smoothly enriching listings with informed categories before field population.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -10,3 +10,3 @@
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
@@ -47,2 +47,8 @@
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..d95ec9c287501f91eee15b5f5b2cd4a2644ffcd1
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..d95ec9c287501f91eee15b5f5b2cd4a2644ffcd1
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing to update dataclass**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -62,2 +62,5 @@
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..1fa20a67c73fd169643d1e063343def27f1480f9
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..1fa20a67c73fd169643d1e063343def27f1480f9
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating FieldGenerator methods**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -122,3 +122,3 @@
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
@@ -136,2 +136,13 @@
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
@@ -147,2 +158,5 @@
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..a1f6bc8eb338615e56b8d51ab228f2933eaa9032
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..a1f6bc8eb338615e56b8d51ab228f2933eaa9032
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Reviewing method dependencies and patch plan**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -356,2 +356,5 @@
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
@@ -374,15 +377,26 @@
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..3a40ab1c0826870e26bba6adaa95056aea1cc3be
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -331,6 +354,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,19 +375,30 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..3a40ab1c0826870e26bba6adaa95056aea1cc3be
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -331,6 +354,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,19 +375,30 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Planning helper method implementations**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -404,2 +404,133 @@
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..566a7265afd0a80d5e74083979d5b8bd46c257e4
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -331,6 +354,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +375,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..566a7265afd0a80d5e74083979d5b8bd46c257e4
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -331,6 +354,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +375,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Adding category classification methods and integration**

I’m planning to insert a new classification method `_assign_classifier_categories` right after `_populate_fields` and update the `run` method to call it on the limited listings. I’ll implement helper methods `_classify_listing_category` and `_resolve_classifier_slug` for classifying listings using LLM prompts, then handle matching slugs from responses. This involves careful token and config handling, and ensuring graceful fallbacks if configs or clients are missing. I’ll place these new methods near similar helpers for clarity.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -307,2 +307,3 @@
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..0c5a89a4a89ab9ce197a350ab455f1a6eb28bd8f
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..0c5a89a4a89ab9ce197a350ab455f1a6eb28bd8f
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Planning method insertion**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -547,2 +547,88 @@
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..156a67a9e2b5fba7a1b990863f51db1800c53ad3
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..156a67a9e2b5fba7a1b990863f51db1800c53ad3
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating payload construction logic**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -796,3 +796,3 @@
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
@@ -801,2 +801,5 @@
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
@@ -806,2 +809,9 @@
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..ec3123592ec515e877d62b4ddbd3895d44220d8d
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..ec3123592ec515e877d62b4ddbd3895d44220d8d
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
             )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
+            )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Adding _normalize_location method**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -870,2 +870,15 @@
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating schema with new target properties**

I'm preparing to add new definitions and properties related to addresses and categories into the targets schema JSON file. This involves carefully inserting references for `address_selectors`, `address_patterns`, `category_rules`, and `category_classifier` into the existing target properties section, and defining these new schema components with appropriate constraints and patterns. I'll use patching cautiously given the size and impact of these changes.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/config/targets.schema.json
@@ -138,2 +138,8 @@
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
@@ -141,2 +147,8 @@
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M apps/crawler/config/targets.schema.json
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..446701333858124dae34a733c005f5927d85283e
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..446701333858124dae34a733c005f5927d85283e
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Adding definitions before selectors**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/config/targets.schema.json
@@ -171,2 +171,81 @@
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M apps/crawler/config/targets.schema.json
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing schema and config updates**

I’m making sure new schema definitions fit before existing selectors and reviewing defaults like allowing empty lists or optional properties. Next, I’ll update the sample config file with new address selectors, patterns, category rules, and classifiers, keeping JSON valid and reflecting the new schema. Tests will be run afterward to confirm everything holds together.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/targets.example.json
@@ -20,2 +20,29 @@
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M apps/crawler/targets.example.json
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..9ee199383e6b55a87e9b66bc506057f25453c45b
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,164 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
+                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +545,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +794,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +868,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Ensuring robust pattern rule handling**

I'm making sure the JSON strings and regex patterns are correctly escaped and formatted. I'm also fixing the `_prepare_category_rules` method so it only adds a rule if it has at least one valid detection condition like selector, text, or a successfully compiled pattern, to prevent always-true false positives.
file update
M /var/home/rick/Development/mega-directory/apps/crawler/crawler.py
@@ -460,2 +460,6 @@
                 compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
             resolved.append(
@@ -463,4 +467,4 @@
                     "slug": slug,
-                    "selector": selector.strip() if isinstance(selector, str) and selector.strip() else None,
-                    "text": text.lower().strip() if isinstance(text, str) and text.strip() else None,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
                     "pattern": compiled_pattern,
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M apps/crawler/crawler.py
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Reviewing variable usage and plan status updates**
Plan update
  ✓ Add failing tests covering address parsing, category rules, and classifier behavior in crawler and schema config.
  ✓ Implement crawler logic and configuration schema updates for addresses, category assignment, and LLM classification support.
  ✓ Update sample configs/docs if needed and ensure payload builder uses new data.
  → Run crawler test suite (pytest) and verify updates.
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Running tests in crawler app**
exec
bash -lc 'pytest apps/crawler/tests' in /var/home/rick/Development/mega-directory exited 1 in 15.24s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 15 items

apps/crawler/tests/test_config_schema.py ..F                             [ 20%]
apps/crawler/tests/test_crawler.py ............                          [100%]

=================================== FAILURES ===================================
_______________ test_schema_rejects_category_rules_missing_slug ________________

schema = {'$defs': {'addressPattern': {'additionalProperties': False, 'properties': {'flags': {'pattern': '^[imsIMS]*$', 'type'...as/crawler-targets.json', '$schema': 'https://json-schema.org/draft/2020-12/schema', 'additionalProperties': True, ...}

    def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
        invalid_config = {
            "api_endpoint": "https://api.example.com/v1/crawler/listings",
            "api_token": "token",
            "targets": [
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..8d3acb3f0eb06b8d91e334223780d06d519ec297
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,30 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "category_rules" in messages
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating test assertions for error messages**
file update
M /var/home/rick/Development/mega-directory/apps/crawler/tests/test_config_schema.py
@@ -90,3 +90,2 @@
     messages = "\n".join(error.message for error in errors)
-    assert "category_rules" in messages
     assert "slug" in messages
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M apps/crawler/tests/test_config_schema.py
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Rerunning tests**
exec
bash -lc 'pytest apps/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 2.35s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 15 items

apps/crawler/tests/test_config_schema.py ...                             [ 20%]
apps/crawler/tests/test_crawler.py ............                          [100%]

============================== 15 passed in 1.38s ==============================
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Updating plan status**
Plan update
  ✓ Add failing tests covering address parsing, category rules, and classifier behavior in crawler and schema config.
  ✓ Implement crawler logic and configuration schema updates for addresses, category assignment, and LLM classification support.
  ✓ Update sample configs/docs if needed and ensure payload builder uses new data.
  ✓ Run crawler test suite (pytest) and verify updates.
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing summary with git status**
exec
bash -lc 'git status -sb' in /var/home/rick/Development/mega-directory succeeded in 121ms:
## main...origin/main [ahead 6]
 M .env.example
?? .editorconfig
?? .vscode/
?? apps/
?? codex/LOGS/codex_run_2025-11-12_06-13-21.log
?? codex/LOGS/tasklog_24_setup_project_scaffolding_and_configuration_1762946001.log
?? codex/LOGS/tasklog_24_setup_project_scaffolding_and_configuration_1762946001.log.done
?? codex/LOGS/tasklog_25_define_directory_page_model_and_metadata_fields_1762949245.log
?? codex/LOGS/tasklog_25_define_directory_page_model_and_metadata_fields_1762949245.log.done
?? codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log
?? codex/LOGS/tasklog_25a_initialize_astro_frontend_with_base_layouts_1762948216.log.done
?? codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log
?? codex/LOGS/tasklog_26_initialize_geographic_location_tables_and_seed_data_1762949750.log.done
?? codex/LOGS/tasklog_27_implement_directory_pages_table_and_metadata_fields_1762950694.log
?? codex/LOGS/tasklog_27_implement_directory_pages_table_and_metadata_fields_1762950694.log.done
?? codex/LOGS/tasklog_28_build_admin_ui_for_directory_page_creation_and_editing_1762951273.log
?? codex/LOGS/tasklog_28_build_admin_ui_for_directory_page_creation_and_editing_1762951273.log.done
?? codex/LOGS/tasklog_29_implement_listings_table_enhancements_for_multiple_categories_and_locations_1762952190.log
?? codex/LOGS/tasklog_29_implement_listings_table_enhancements_for_multiple_categories_and_locations_1762952190.log.done
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing detailed file and line references**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '1,200p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
     1	from __future__ import annotations
     2	
     3	from dataclasses import dataclass, field
     4	import logging
     5	import os
     6	import re
     7	from typing import Any, Callable, Dict, Iterable, List, Optional
     8	from urllib.parse import quote_plus, urljoin
     9	
    10	import requests
    11	from bs4 import BeautifulSoup, Tag
    12	from jinja2 import Environment, StrictUndefined, TemplateError
    13	
    14	try:
    15	    import requests_cache
    16	except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    17	    from types import SimpleNamespace
    18	
    19	    class _ShimCachedSession(requests.Session):
    20	        """Minimal drop-in replacement when requests-cache is unavailable."""
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Adding line numbers for parsing methods**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '240,520p'" in /var/home/rick/Development/mega-directory succeeded in 42ms:
   240	            listing=context.get("listing"),
   241	        )
   242	        return self.llm_client(request)
   243	
   244	    def _prepare_render_context(
   245	        self,
   246	        context: Dict[str, Any],
   247	        config: Dict[str, Any],
   248	    ) -> Dict[str, Any]:
   249	        render_context = dict(context)
   250	        render_context["listing"] = context["listing"]
   251	        render_context["batch"] = context["batch"]
   252	        tokens = dict(context.get("tokens", {}))
   253	        extra_tokens = config.get("tokens") or {}
   254	        tokens.update(extra_tokens)
   255	        render_context["tokens"] = tokens
   256	        return render_context
   257	
   258	    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
   259	        try:
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Extracting parse_listings lines with context**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '360,640p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   360	        category_rules = self._prepare_category_rules(target.get("category_rules"))
   361	        soup = BeautifulSoup(html, "html.parser")
   362	        elements = soup.select(selectors["listing"])
   363	        listings: List[Listing] = []
   364	
   365	        for element in elements:
   366	            title_node = element.select_one(selectors["title"])
   367	            link_node = element.select_one(selectors["link"])
   368	            desc_node = element.select_one(selectors["description"])
   369	
   370	            title = title_node.get_text(strip=True) if title_node else ""
   371	            url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
   372	            snippet = desc_node.get_text(strip=True) if desc_node else ""
   373	            link_text = link_node.get_text(strip=True) if link_node else ""
   374	            link_text = link_text or title
   375	
   376	            if not title and not url:
   377	                continue
   378	
   379	            listing_text = self._gather_listing_text(element, title, snippet, link_text)
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Requesting additional helper definitions**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '440,760p'" in /var/home/rick/Development/mega-directory succeeded in 44ms:
   440	        resolved: List[Dict[str, Any]] = []
   441	        if not isinstance(config, list):
   442	            return resolved
   443	        for entry in config:
   444	            if not isinstance(entry, dict):
   445	                continue
   446	            slug = self._slugify(entry.get("slug"))
   447	            selector = entry.get("selector")
   448	            text = entry.get("text")
   449	            pattern_value = entry.get("pattern")
   450	            if not slug:
   451	                continue
   452	            has_condition = any(
   453	                isinstance(value, str) and value.strip()
   454	                for value in (selector, text, pattern_value)
   455	            )
   456	            if not has_condition:
   457	                continue
   458	            compiled_pattern = None
   459	            if isinstance(pattern_value, str) and pattern_value.strip():
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Considering inclusion of assignment lines**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '520,800p'" in /var/home/rick/Development/mega-directory succeeded in 42ms:
   520	                if not match:
   521	                    continue
   522	                captures = match.groupdict()
   523	                for field in LOCATION_FIELDS:
   524	                    candidate = captures.get(field)
   525	                    if candidate and field not in extracted:
   526	                        extracted[field] = candidate.strip()
   527	        return self._normalize_location(extracted)
   528	
   529	    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
   530	        flags = 0
   531	        if isinstance(flags_value, str):
   532	            for char in flags_value:
   533	                mapped = _REGEX_FLAG_MAP.get(char.lower())
   534	                if mapped:
   535	                    flags |= mapped
   536	        try:
   537	            return re.compile(pattern, flags)
   538	        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
   539	            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Continuing to capture method lines**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '800,1040p'" in /var/home/rick/Development/mega-directory succeeded in 47ms:
   800	
   801	        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
   802	        normalized_category = self._slugify(category_slug)
   803	        if not normalized_category:
   804	            raise ValueError("Unable to determine category slug for API payload")
   805	        payload["categorySlug"] = normalized_category
   806	        listing.category_slug = normalized_category
   807	        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
   808	            listing.categories.append(normalized_category)
   809	
   810	        source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
   811	        if source_name:
   812	            payload["sourceName"] = source_name
   813	
   814	        location_payload = payload.get("location")
   815	        if not isinstance(location_payload, dict):
   816	            location_payload = listing.location
   817	        normalized_location = self._normalize_location(location_payload)
   818	        if normalized_location:
   819	            payload["location"] = normalized_location
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Planning to retrieve full method snippet**
exec
bash -lc "nl -ba apps/crawler/crawler.py | sed -n '612,680p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
   612	    def _resolve_classifier_slug(
   613	        self,
   614	        response: Any,
   615	        choices: List[str],
   616	    ) -> Optional[str]:
   617	        normalized = self._normalize_string(response)
   618	        if not normalized:
   619	            return None
   620	        first_line = normalized.splitlines()[0]
   621	        cleaned = first_line.split("|")[0].split(",")[0].strip()
   622	        lowered_full = normalized.lower()
   623	        lowered_cleaned = cleaned.lower()
   624	        normalized_choices: List[tuple[str, str]] = []
   625	        for choice in choices:
   626	            slug = self._slugify(choice)
   627	            if slug:
   628	                normalized_choices.append((choice.lower(), slug))
   629	        for original_lower, slug in normalized_choices:
   630	            if lowered_cleaned == slug or lowered_cleaned == original_lower:
   631	                return slug
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing to cite new tests and schema changes**
exec
bash -lc "nl -ba apps/crawler/tests/test_crawler.py | sed -n '1,260p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
     1	from pathlib import Path
     2	from types import SimpleNamespace
     3	import sys
     4	
     5	import pytest
     6	
     7	AGENT_DIR = Path(__file__).resolve().parents[1]
     8	if str(AGENT_DIR) not in sys.path:
     9	    sys.path.insert(0, str(AGENT_DIR))
    10	
    11	import crawler
    12	from crawler import run_crawler
    13	
    14	
    15	SAMPLE_HTML = """
    16	<section>
    17	  <article data-listing>
    18	    <a class="listing-link" href="https://example.com/acme">
    19	      <span class="listing-title">ACME Electric</span>
    20	    </a>
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Extracting lines for new tests**
exec
bash -lc "nl -ba apps/crawler/tests/test_crawler.py | sed -n '140,360p'" in /var/home/rick/Development/mega-directory succeeded in 43ms:
   140	
   141	
   142	def test_run_crawler_batches_locations_and_limits_results() -> None:
   143	    config = {
   144	        "api_endpoint": "https://api.example.com",
   145	        "api_token": "token",
   146	        "targets": [
   147	            {
   148	                "category": "Electricians",
   149	                "subdomain": "electric.example.com",
   150	                "locations": ["Holland MI", "Grand Rapids MI"],
   151	                "keywords": ["licensed"],
   152	                "listings_per_location": 1,
   153	            }
   154	        ],
   155	    }
   156	    dummy_session = DummySession(SAMPLE_HTML)
   157	
   158	    batches = run_crawler(config, session=dummy_session)
   159	
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Identifying line numbers for new tests**
exec
bash -lc "nl -ba apps/crawler/tests/test_crawler.py | sed -n '360,520p'" in /var/home/rick/Development/mega-directory succeeded in 49ms:
   360	    assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
   361	
   362	
   363	def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
   364	    config = {
   365	        "api_endpoint": "https://api.example.com/v1/crawler/listings",
   366	        "api_token": "crawler-token",
   367	        "targets": [
   368	            {
   369	                "category": "Services",
   370	                "locations": ["Cleveland"],
   371	                "address_selectors": {
   372	                    "addressLine1": ".address .street",
   373	                    "city": ".address .city",
   374	                    "region": ".address .region",
   375	                },
   376	                "category_rules": [
   377	                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
   378	                ],
   379	            }
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Checking line numbers for test_config_schema**
exec
bash -lc 'nl -ba apps/crawler/tests/test_config_schema.py' in /var/home/rick/Development/mega-directory succeeded in 46ms:
     1	from __future__ import annotations
     2	
     3	import json
     4	from pathlib import Path
     5	
     6	import jsonschema
     7	import pytest
     8	
     9	
    10	AGENT_DIR = Path(__file__).resolve().parents[1]
    11	SCHEMA_PATH = AGENT_DIR / "config" / "targets.schema.json"
    12	EXAMPLE_PATH = AGENT_DIR / "targets.example.json"
    13	
    14	
    15	@pytest.fixture(scope="module")
    16	def schema() -> dict:
    17	    """Load the crawler configuration schema once per test module."""
    18	    with SCHEMA_PATH.open("r", encoding="utf-8") as fp:
    19	        return json.load(fp)
    20	
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Gathering schema line numbers**
exec
bash -lc 'rg -n "address_selectors" -n apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 41ms:
139:        "address_selectors": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Searching for 'category_rules' definitions with line numbers**
exec
bash -lc 'rg -n "category_rules" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 45ms:
148:        "category_rules": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Searching for line numbers of addressSelectors**
exec
bash -lc 'rg -n "addressSelectors" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 41ms:
140:          "$ref": "#/$defs/addressSelectors"
172:    "addressSelectors": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Searching for categoryRuleList and categoryClassifier lines**
exec
bash -lc 'rg -n "categoryRule" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 43ms:
149:          "$ref": "#/$defs/categoryRuleList"
202:    "categoryRuleList": {
205:        "$ref": "#/$defs/categoryRule"
208:    "categoryRule": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Searching for categoryClassifier line numbers**
exec
bash -lc 'rg -n "categoryClassifier" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 43ms:
152:          "$ref": "#/$defs/categoryClassifier"
232:    "categoryClassifier": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Searching for address_patterns location**
exec
bash -lc 'rg -n "address_patterns" apps/crawler/config/targets.schema.json' in /var/home/rick/Development/mega-directory succeeded in 43ms:
142:        "address_patterns": {
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Preparing config line numbering**
exec
bash -lc 'nl -ba apps/crawler/targets.example.json' in /var/home/rick/Development/mega-directory succeeded in 40ms:
     1	{
     2	  "$schema": "./config/targets.schema.json",
     3	  "api_targets": [
     4	    {
     5	      "name": "dev",
     6	      "endpoint": "http://localhost:3001/v1/crawler/listings",
     7	      "token": "dev-token"
     8	    },
     9	    {
    10	      "name": "prod",
    11	      "endpoint": "https://api.yourdomain.com/v1/crawler/listings",
    12	      "token": "prod-token"
    13	    }
    14	  ],
    15	  "targets": [
    16	    {
    17	      "category": "Real Estate",
    18	      "locations": ["New York", "San Francisco"],
    19	      "subdomain": "realestate.local",
    20	      "keywords": ["ocean view", "downtown"],
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [


thinking
**Summarizing crawler updates and tests**

I’m organizing a final message with clear sections: a summary highlighting new location and category extraction features in the crawler, detailed line references for code and schema changes, and a tests section citing specific test files and ranges. I’ll also suggest natural next steps like running the crawler in a real environment to verify selectors, keeping the format simple with bullet points and standalone file path references.
codex
- Listing metadata now retains structured `categories`, `category_slug`, and `location`, and that context flows through FieldGenerator tokens so downstream prompts can reference parsed addresses/categories (apps/crawler/crawler.py:48) (apps/crawler/crawler.py:118).
- Parsing now understands `address_selectors`, regex-based `address_patterns`, and `category_rules`, plus helper utilities for gathering text and compiling patterns to populate each listing before batching (apps/crawler/crawler.py:360) (apps/crawler/crawler.py:406) (apps/crawler/crawler.py:414).
- Added category-classifier orchestration that runs only for unresolved listings, enforces LLM availability, and feeds results back into payload construction alongside normalized location objects (apps/crawler/crawler.py:552) (apps/crawler/crawler.py:573) (apps/crawler/crawler.py:801) (apps/crawler/crawler.py:874).
- Extended the crawler schema/example config and tests to cover the new knobs for selectors, patterns, rules, and classifiers, ensuring invalid configs (like missing rule slugs) are caught early (apps/crawler/config/targets.schema.json:139) (apps/crawler/targets.example.json:16) (apps/crawler/tests/test_config_schema.py:68).
- Added regression tests exercising selector-based parsing, location payload emission, and LLM classification fallbacks so the new behavior is protected (apps/crawler/tests/test_crawler.py:202) (apps/crawler/tests/test_crawler.py:363) (apps/crawler/tests/test_crawler.py:392) (apps/crawler/tests/test_crawler.py:429).

**Tests**
- `pytest apps/crawler/tests`

**Next Steps**
1. Dry-run the crawler against real target HTML to tune selector/pattern configs and confirm the classifier prompt produces the desired slugs across your actual category set.
file update:
diff --git a/apps/crawler/config/targets.schema.json b/apps/crawler/config/targets.schema.json
index 2b86ce9670cdf8e9c84ea5e81f08b41704c01b22..f9fb82cc649ac807e622bb09f283ce06e5c52f09
--- a/apps/crawler/config/targets.schema.json
+++ b/apps/crawler/config/targets.schema.json
@@ -136,9 +136,21 @@
         "selectors": {
           "$ref": "#/$defs/selectors"
         },
+        "address_selectors": {
+          "$ref": "#/$defs/addressSelectors"
+        },
+        "address_patterns": {
+          "$ref": "#/$defs/addressPatternList"
+        },
         "fields": {
           "$ref": "#/$defs/fieldMap"
         },
+        "category_rules": {
+          "$ref": "#/$defs/categoryRuleList"
+        },
+        "category_classifier": {
+          "$ref": "#/$defs/categoryClassifier"
+        },
         "listings_per_location": {
           "$ref": "#/$defs/positiveInteger"
         },
@@ -157,6 +169,85 @@
       },
       "required": ["category", "locations"]
     },
+    "addressSelectors": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "addressLine1": { "$ref": "#/$defs/nonEmptyString" },
+        "addressLine2": { "$ref": "#/$defs/nonEmptyString" },
+        "city": { "$ref": "#/$defs/nonEmptyString" },
+        "region": { "$ref": "#/$defs/nonEmptyString" },
+        "postalCode": { "$ref": "#/$defs/nonEmptyString" },
+        "country": { "$ref": "#/$defs/nonEmptyString" }
+      }
+    },
+    "addressPatternList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/addressPattern"
+      }
+    },
+    "addressPattern": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["pattern"]
+    },
+    "categoryRuleList": {
+      "type": "array",
+      "items": {
+        "$ref": "#/$defs/categoryRule"
+      }
+    },
+    "categoryRule": {
+      "type": "object",
+      "additionalProperties": false,
+      "properties": {
+        "slug": { "$ref": "#/$defs/nonEmptyString" },
+        "selector": { "$ref": "#/$defs/nonEmptyString" },
+        "text": { "$ref": "#/$defs/nonEmptyString" },
+        "pattern": { "$ref": "#/$defs/nonEmptyString" },
+        "flags": {
+          "type": "string",
+          "pattern": "^[imsIMS]*$"
+        }
+      },
+      "required": ["slug"],
+      "allOf": [
+        {
+          "anyOf": [
+            { "required": ["selector"] },
+            { "required": ["text"] },
+            { "required": ["pattern"] }
+          ]
+        }
+      ]
+    },
+    "categoryClassifier": {
+      "type": "object",
+      "properties": {
+        "provider": { "$ref": "#/$defs/nonEmptyString" },
+        "model": { "$ref": "#/$defs/nonEmptyString" },
+        "prompt_template": { "$ref": "#/$defs/nonEmptyString" },
+        "choices": { "$ref": "#/$defs/stringList" },
+        "tokens": {
+          "type": "object",
+          "additionalProperties": {
+            "type": "string"
+          }
+        },
+        "options": {
+          "type": "object"
+        }
+      },
+      "required": ["provider", "model", "prompt_template"]
+    },
     "selectors": {
       "type": "object",
       "additionalProperties": true,
diff --git a/apps/crawler/crawler.py b/apps/crawler/crawler.py
index 276346a85055b18b9239018b9bab759065b7fbef..b0de1d685de20f81ed6caa75edc8574ffbc0fb37
--- a/apps/crawler/crawler.py
+++ b/apps/crawler/crawler.py
@@ -8,7 +8,7 @@
 from urllib.parse import quote_plus, urljoin
 
 import requests
-from bs4 import BeautifulSoup
+from bs4 import BeautifulSoup, Tag
 from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
@@ -45,6 +45,12 @@
 }
 MAX_SLUG_LENGTH = 80
 _SLUG_INVALID_CHARS = re.compile(r"[^a-z0-9]+")
+LOCATION_FIELDS = ("addressLine1", "addressLine2", "city", "region", "postalCode", "country")
+_REGEX_FLAG_MAP = {
+    "i": re.IGNORECASE,
+    "m": re.MULTILINE,
+    "s": re.DOTALL,
+}
 
 
 @dataclass
@@ -54,6 +60,9 @@
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
     fields: Dict[str, Any] = field(default_factory=dict)
+    categories: List[str] = field(default_factory=list)
+    category_slug: Optional[str] = None
+    location: Optional[Dict[str, Any]] = None
 
 
 @dataclass
@@ -111,7 +120,7 @@
         if not field_configs:
             return {}
 
-        context = self._build_context(listing, target, batch)
+        context = self.build_context(listing, target, batch)
         generated: Dict[str, Any] = {}
 
         for field_name, config in field_configs.items():
@@ -125,6 +134,17 @@
 
         return generated
 
+    def build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        return self._build_context(listing, target, batch)
+
+    def render_template(self, label: str, template: str, context: Dict[str, Any]) -> str:
+        return self._render_template(label, template, context)
+
     def _build_context(
         self,
         listing: Listing,
@@ -136,6 +156,9 @@
             "url": listing.url,
             "snippet": listing.snippet,
             "extras": dict(listing.extras),
+            "categories": list(getattr(listing, "categories", []) or []),
+            "category_slug": getattr(listing, "category_slug", None),
+            "location": getattr(listing, "location", None),
         }
         for key, value in listing.extras.items():
             if key not in listing_context:
@@ -282,6 +305,7 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                self._assign_classifier_categories(listings, target, location_batch)
                 if target.get("fields"):
                     self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -331,6 +355,9 @@
         keyword: Optional[str] = None,
     ) -> List[Listing]:
         selectors = {**DEFAULT_SELECTORS, **target.get("selectors", {})}
+        address_selectors = self._prepare_address_selectors(target.get("address_selectors"))
+        address_patterns = self._prepare_address_patterns(target.get("address_patterns"))
+        category_rules = self._prepare_category_rules(target.get("category_rules"))
         soup = BeautifulSoup(html, "html.parser")
         elements = soup.select(selectors["listing"])
         listings: List[Listing] = []
@@ -349,22 +376,168 @@
             if not title and not url:
                 continue
 
+            listing_text = self._gather_listing_text(element, title, snippet, link_text)
+
             normalized_url = self._normalize_url(url, target.get("subdomain"))
-            listings.append(
-                Listing(
-                    title=title,
-                    url=normalized_url,
-                    snippet=snippet,
-                    extras={
-                        "location": location,
-                        "keyword": keyword,
-                        "link_text": link_text,
-                    },
-                )
+            listing = Listing(
+                title=title,
+                url=normalized_url,
+                snippet=snippet,
+                extras={
+                    "location": location,
+                    "keyword": keyword,
+                    "link_text": link_text,
+                },
+            )
+            listing.location = self._extract_location_from_element(
+                element,
+                address_selectors,
+                address_patterns,
+                listing_text,
             )
+            categories = self._match_category_rules(element, category_rules, listing_text)
+            if categories:
+                listing.categories = categories
+                listing.category_slug = categories[0]
+            listings.append(listing)
 
         return listings
 
+    @staticmethod
+    def _gather_listing_text(element: Tag, *parts: Optional[str]) -> str:
+        text_parts = [part for part in parts if isinstance(part, str) and part]
+        element_text = element.get_text(" ", strip=True)
+        if element_text:
+            text_parts.append(element_text)
+        return " ".join(text_parts).strip()
+
+    def _prepare_address_selectors(self, config: Any) -> Dict[str, str]:
+        selectors: Dict[str, str] = {}
+        if isinstance(config, dict):
+            for key, selector in config.items():
+                if key not in LOCATION_FIELDS or not isinstance(selector, str):
+                    continue
+                trimmed = selector.strip()
+                if trimmed:
+                    selectors[key] = trimmed
+        return selectors
+
+    def _prepare_address_patterns(self, config: Any) -> List[re.Pattern[str]]:
+        patterns: List[re.Pattern[str]] = []
+        if isinstance(config, list):
+            for entry in config:
+                if not isinstance(entry, dict):
+                    continue
+                pattern_value = entry.get("pattern")
+                if not isinstance(pattern_value, str) or not pattern_value.strip():
+                    continue
+                compiled = self._compile_regex(pattern_value, entry.get("flags"))
+                if compiled:
+                    patterns.append(compiled)
+        return patterns
+
+    def _prepare_category_rules(self, config: Any) -> List[Dict[str, Any]]:
+        resolved: List[Dict[str, Any]] = []
+        if not isinstance(config, list):
+            return resolved
+        for entry in config:
+            if not isinstance(entry, dict):
+                continue
+            slug = self._slugify(entry.get("slug"))
+            selector = entry.get("selector")
+            text = entry.get("text")
+            pattern_value = entry.get("pattern")
+            if not slug:
+                continue
+            has_condition = any(
+                isinstance(value, str) and value.strip()
+                for value in (selector, text, pattern_value)
+            )
+            if not has_condition:
+                continue
+            compiled_pattern = None
+            if isinstance(pattern_value, str) and pattern_value.strip():
+                compiled_pattern = self._compile_regex(pattern_value, entry.get("flags"))
+            trimmed_selector = selector.strip() if isinstance(selector, str) and selector.strip() else None
+            lowered_text = text.lower().strip() if isinstance(text, str) and text.strip() else None
+            if not any((trimmed_selector, lowered_text, compiled_pattern)):
+                continue
+            resolved.append(
+                {
+                    "slug": slug,
+                    "selector": trimmed_selector,
+                    "text": lowered_text,
+                    "pattern": compiled_pattern,
+                }
+            )
+        return resolved
+
+    def _match_category_rules(
+        self,
+        element: Tag,
+        rules: List[Dict[str, Any]],
+        listing_text: str,
+    ) -> List[str]:
+        if not rules:
+            return []
+        matches: List[str] = []
+        lowered_text = listing_text.lower()
+        for rule in rules:
+            selector = rule.get("selector")
+            rule_text = rule.get("text")
+            pattern = rule.get("pattern")
+            if selector and not element.select_one(selector):
+                continue
+            if rule_text and rule_text not in lowered_text:
+                continue
+            if pattern and not pattern.search(listing_text):
+                continue
+            slug = rule["slug"]
+            if slug not in matches:
+                matches.append(slug)
+        return matches
+
+    def _extract_location_from_element(
+        self,
+        element: Tag,
+        selectors: Dict[str, str],
+        patterns: List[re.Pattern[str]],
+        listing_text: str,
+    ) -> Optional[Dict[str, str]]:
+        if not selectors and not patterns:
+            return None
+        extracted: Dict[str, str] = {}
+        for field, selector in selectors.items():
+            node = element.select_one(selector)
+            if not node:
+                continue
+            value = node.get_text(strip=True)
+            if value:
+                extracted[field] = value
+        if listing_text and patterns:
+            for pattern in patterns:
+                match = pattern.search(listing_text)
+                if not match:
+                    continue
+                captures = match.groupdict()
+                for field in LOCATION_FIELDS:
+                    candidate = captures.get(field)
+                    if candidate and field not in extracted:
+                        extracted[field] = candidate.strip()
+        return self._normalize_location(extracted)
+
+    def _compile_regex(self, pattern: str, flags_value: Any) -> Optional[re.Pattern[str]]:
+        flags = 0
+        if isinstance(flags_value, str):
+            for char in flags_value:
+                mapped = _REGEX_FLAG_MAP.get(char.lower())
+                if mapped:
+                    flags |= mapped
+        try:
+            return re.compile(pattern, flags)
+        except re.error as exc:  # pragma: no cover - invalid configs should be fixed upstream
+            self.logger.warning("Failed to compile pattern '%s': %s", pattern, exc)
+            return None
     def _populate_fields(
         self,
         listings: List[Listing],
@@ -376,6 +549,92 @@
         for listing in listings:
             listing.fields = self.field_generator.generate(listing, target, batch)
 
+    def _assign_classifier_categories(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        classifier = target.get("category_classifier")
+        if not classifier:
+            return
+        pending = [listing for listing in listings if not listing.category_slug]
+        if not pending:
+            return
+        if not self.field_generator.llm_client:
+            raise RuntimeError("Category classifier requires an llm_client instance")
+        for listing in pending:
+            slug = self._classify_listing_category(listing, target, batch, classifier)
+            if slug:
+                listing.category_slug = slug
+                if slug not in listing.categories:
+                    listing.categories.append(slug)
+
+    def _classify_listing_category(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+        classifier: Dict[str, Any],
+    ) -> Optional[str]:
+        provider = classifier.get("provider")
+        model = classifier.get("model")
+        prompt_template = classifier.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError("category_classifier requires provider, model, and prompt_template")
+        context = self.field_generator.build_context(listing, target, batch)
+        tokens = dict(context.get("tokens", {}))
+        raw_choices = classifier.get("choices") or []
+        choices = [choice for choice in raw_choices if isinstance(choice, str) and choice.strip()]
+        if choices:
+            tokens["category_choices"] = ", ".join(choices)
+        extra_tokens = classifier.get("tokens")
+        if isinstance(extra_tokens, dict):
+            tokens.update(extra_tokens)
+        context["tokens"] = tokens
+        prompt = self.field_generator.render_template(
+            "category_classifier",
+            prompt_template,
+            context,
+        )
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name="category_classifier",
+            options=classifier.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        response = self.field_generator.llm_client(request)
+        return self._resolve_classifier_slug(response, choices)
+
+    def _resolve_classifier_slug(
+        self,
+        response: Any,
+        choices: List[str],
+    ) -> Optional[str]:
+        normalized = self._normalize_string(response)
+        if not normalized:
+            return None
+        first_line = normalized.splitlines()[0]
+        cleaned = first_line.split("|")[0].split(",")[0].strip()
+        lowered_full = normalized.lower()
+        lowered_cleaned = cleaned.lower()
+        normalized_choices: List[tuple[str, str]] = []
+        for choice in choices:
+            slug = self._slugify(choice)
+            if slug:
+                normalized_choices.append((choice.lower(), slug))
+        for original_lower, slug in normalized_choices:
+            if lowered_cleaned == slug or lowered_cleaned == original_lower:
+                return slug
+        for original_lower, slug in normalized_choices:
+            if original_lower in lowered_cleaned or slug in lowered_cleaned or slug in lowered_full:
+                return slug
+        slug = self._slugify(cleaned)
+        return slug or None
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -539,16 +798,26 @@
         if source_url:
             payload["sourceUrl"] = source_url
 
-        category_slug = payload.get("categorySlug") or default_category_slug
+        category_slug = payload.get("categorySlug") or listing.category_slug or default_category_slug
         normalized_category = self._slugify(category_slug)
         if not normalized_category:
             raise ValueError("Unable to determine category slug for API payload")
         payload["categorySlug"] = normalized_category
+        listing.category_slug = normalized_category
+        if isinstance(listing.categories, list) and normalized_category not in listing.categories:
+            listing.categories.append(normalized_category)
 
         source_name = self._normalize_string(payload.get("sourceName")) or self._resolve_source_name(target_config)
         if source_name:
             payload["sourceName"] = source_name
 
+        location_payload = payload.get("location")
+        if not isinstance(location_payload, dict):
+            location_payload = listing.location
+        normalized_location = self._normalize_location(location_payload)
+        if normalized_location:
+            payload["location"] = normalized_location
+
         return {key: value for key, value in payload.items() if value is not None}
 
     def _resolve_category_slug(self, target_config: Dict[str, Any]) -> str:
@@ -603,6 +872,19 @@
         return None
 
     @staticmethod
+    def _normalize_location(value: Any) -> Optional[Dict[str, str]]:
+        if not isinstance(value, dict):
+            return None
+        normalized: Dict[str, str] = {}
+        for field in LOCATION_FIELDS:
+            if field not in value:
+                continue
+            normalized_value = Crawler._normalize_string(value[field])
+            if normalized_value:
+                normalized[field] = normalized_value
+        return normalized or None
+
+    @staticmethod
     def _slugify(value: Any, max_length: int = MAX_SLUG_LENGTH) -> str:
         if not isinstance(value, str):
             return ""
diff --git a/apps/crawler/targets.example.json b/apps/crawler/targets.example.json
index 5a86e14dad84b94efd1ed95335161deb8cef0eb7..6e542240cf2a13f0fffdc32dbb01b7d64d32e4c3
--- a/apps/crawler/targets.example.json
+++ b/apps/crawler/targets.example.json
@@ -18,6 +18,33 @@
       "locations": ["New York", "San Francisco"],
       "subdomain": "realestate.local",
       "keywords": ["ocean view", "downtown"],
+      "address_selectors": {
+        "addressLine1": ".listing-address .line1",
+        "city": ".listing-address .city",
+        "region": ".listing-address .region"
+      },
+      "address_patterns": [
+        {
+          "pattern": "(?P<city>[A-Za-z\\s]+),\\s*(?P<region>[A-Z]{2})"
+        }
+      ],
+      "category_rules": [
+        {
+          "slug": "real-estate-agents",
+          "selector": ".badge-agent"
+        },
+        {
+          "slug": "property-managers",
+          "pattern": "Property Management",
+          "flags": "i"
+        }
+      ],
+      "category_classifier": {
+        "provider": "openai",
+        "model": "gpt-4o-mini",
+        "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }} near {{ location }}.",
+        "choices": ["real-estate", "property-management"]
+      },
       "listings_per_location": 50,
       "fields": {
         "title": { "source": "scrape" },
diff --git a/apps/crawler/tests/test_config_schema.py b/apps/crawler/tests/test_config_schema.py
index 81825ce6bd0ac4c04fe4f7c55f6bfc5c2984b4b4..9394b54c2f91df2d58ee8e8fa1f0f88928784023
--- a/apps/crawler/tests/test_config_schema.py
+++ b/apps/crawler/tests/test_config_schema.py
@@ -63,3 +63,29 @@
     assert "'category'" in messages
     assert "'locations'" in messages or "minItems" in messages
     assert "'provider'" in messages
+
+
+def test_schema_rejects_category_rules_missing_slug(schema: dict) -> None:
+    invalid_config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["NYC"],
+                "category_rules": [
+                    {
+                        # Missing slug on purpose to trigger schema validation failure.
+                        "pattern": r"\bhvac",
+                        "flags": "i",
+                    }
+                ],
+            }
+        ],
+    }
+
+    errors = _collect_errors(schema, invalid_config)
+
+    assert errors, "expected schema to enforce slug requirement"
+    messages = "\n".join(error.message for error in errors)
+    assert "slug" in messages
diff --git a/apps/crawler/tests/test_crawler.py b/apps/crawler/tests/test_crawler.py
index a62dfb05c7ce6a712aaa16192189d677926c9cd6..8dd24bc16f6c9681368f7355a8fe8a8896885c11
--- a/apps/crawler/tests/test_crawler.py
+++ b/apps/crawler/tests/test_crawler.py
@@ -29,6 +29,40 @@
 </section>
 """
 
+ADDRESS_HTML = """
+<section>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/hvac">
+      <span class="listing-title">HVAC Masters</span>
+    </a>
+    <div class="listing-description">
+      Full service heating and cooling crews.
+    </div>
+    <div class="address">
+      <span class="street">444 Main St</span>
+      <span class="city">Cleveland</span>
+      <span class="region">OH</span>
+    </div>
+    <div class="badges">
+      <span class="tag tag-hvac">HVAC</span>
+    </div>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/river">
+      <span class="listing-title">River Plumbing</span>
+    </a>
+    <p class="listing-description">Trusted local plumbers covering Austin metro.</p>
+    <p class="full-address">500 Elm St Austin, TX 78701</p>
+  </article>
+  <article data-listing>
+    <a class="listing-link" href="https://example.com/sunsets">
+      <span class="listing-title">Sunset Studios</span>
+    </a>
+    <p class="listing-description">Creative storytellers on every coast.</p>
+  </article>
+</section>
+"""
+
 
 class DummyResponse:
     def __init__(self, text: str) -> None:
@@ -165,6 +199,39 @@
     assert parsed[0].snippet == "Great service."
 
 
+def test_parse_listings_extracts_addresses_and_category_rules() -> None:
+    target = {
+        "category": "Services",
+        "locations": ["Midwest"],
+        "address_selectors": {
+            "addressLine1": ".address .street",
+            "city": ".address .city",
+            "region": ".address .region",
+        },
+        "address_patterns": [
+            {
+                "pattern": r"(?P<addressLine1>\d+\s+[A-Za-z]+\s+\w+)\s+(?P<city>[A-Za-z ]+),\s*(?P<region>[A-Z]{2})\s*(?P<postalCode>\d{5})",
+            }
+        ],
+        "category_rules": [
+            {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+            {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+        ],
+    }
+    crawler_instance = crawler.Crawler(session=DummySession(ADDRESS_HTML))
+
+    parsed = crawler_instance.parse_listings(ADDRESS_HTML, target, "Cleveland")
+
+    assert parsed[0].location == {
+        "addressLine1": "444 Main St",
+        "city": "Cleveland",
+        "region": "OH",
+    }
+    assert parsed[0].category_slug == "hvac-contractors"
+    assert parsed[1].location["postalCode"] == "78701"
+    assert parsed[1].category_slug == "plumbers"
+
+
 def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
     captured = {}
 
@@ -293,6 +360,94 @@
     assert session.post_calls[0]["headers"]["Authorization"] == "Bearer crawler-token"
 
 
+def test_run_crawler_includes_location_payload_from_address_selectors() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Cleveland"],
+                "address_selectors": {
+                    "addressLine1": ".address .street",
+                    "city": ".address .city",
+                    "region": ".address .region",
+                },
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                ],
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session)
+
+    payload = session.post_calls[0]["json"]["listings"][0]
+    assert payload["location"]["city"] == "Cleveland"
+    assert payload["location"]["region"] == "OH"
+    assert payload["categorySlug"] == "hvac-contractors"
+
+
+def test_run_crawler_category_classifier_uses_llm_fallback() -> None:
+    llm_requests = []
+
+    def fake_llm(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return "photographers"
+
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_rules": [
+                    {"slug": "hvac-contractors", "selector": ".tag-hvac"},
+                    {"slug": "plumbers", "pattern": r"\bplumb", "flags": "i"},
+                ],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Choose {{ tokens.category_choices }} for {{ listing.title }}",
+                    "choices": ["photographers", "builders"],
+                },
+            }
+        ],
+    }
+    session = DummySession(ADDRESS_HTML)
+
+    run_crawler(config, session=session, llm_client=fake_llm)
+
+    assert llm_requests, "expected llm classifier to be invoked"
+    payloads = session.post_calls[0]["json"]["listings"]
+    assert payloads[2]["categorySlug"] == "photographers"
+    assert "Sunset Studios" in llm_requests[0].prompt
+
+
+def test_run_crawler_category_classifier_requires_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com/v1/crawler/listings",
+        "api_token": "crawler-token",
+        "targets": [
+            {
+                "category": "Services",
+                "locations": ["Remote"],
+                "category_classifier": {
+                    "provider": "openai",
+                    "model": "gpt-4o-mini",
+                    "prompt_template": "Pick category for {{ listing.title }}",
+                    "choices": ["photographers"],
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(ADDRESS_HTML))
+
+
 def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
     config = {
         "api_targets": [

tokens used
263,263
- Listing metadata now retains structured `categories`, `category_slug`, and `location`, and that context flows through FieldGenerator tokens so downstream prompts can reference parsed addresses/categories (apps/crawler/crawler.py:48) (apps/crawler/crawler.py:118).
- Parsing now understands `address_selectors`, regex-based `address_patterns`, and `category_rules`, plus helper utilities for gathering text and compiling patterns to populate each listing before batching (apps/crawler/crawler.py:360) (apps/crawler/crawler.py:406) (apps/crawler/crawler.py:414).
- Added category-classifier orchestration that runs only for unresolved listings, enforces LLM availability, and feeds results back into payload construction alongside normalized location objects (apps/crawler/crawler.py:552) (apps/crawler/crawler.py:573) (apps/crawler/crawler.py:801) (apps/crawler/crawler.py:874).
- Extended the crawler schema/example config and tests to cover the new knobs for selectors, patterns, rules, and classifiers, ensuring invalid configs (like missing rule slugs) are caught early (apps/crawler/config/targets.schema.json:139) (apps/crawler/targets.example.json:16) (apps/crawler/tests/test_config_schema.py:68).
- Added regression tests exercising selector-based parsing, location payload emission, and LLM classification fallbacks so the new behavior is protected (apps/crawler/tests/test_crawler.py:202) (apps/crawler/tests/test_crawler.py:363) (apps/crawler/tests/test_crawler.py:392) (apps/crawler/tests/test_crawler.py:429).

**Tests**
- `pytest apps/crawler/tests`

**Next Steps**
1. Dry-run the crawler against real target HTML to tune selector/pattern configs and confirm the classifier prompt produces the desired slugs across your actual category set.
