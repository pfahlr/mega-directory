OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a72bd-d101-7a52-812b-61de4414fdfd
--------
user
Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.

thinking
**Preparing to inspect repository structure**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 38ms:
admin
agents
AGENTS.md
api
astro
codex
codex.toml
codex.yaml
db
docker-compose.yml
docs
README.md

thinking
**Opening task file**
exec
bash -lc 'ls codex/TASKS' in /var/home/rick/Development/mega-directory succeeded in 38ms:
00_prepare_repository.yaml
01_initialize_astro_frontend.yaml
02_setup_api_server_with_express_and_jwt_auth.yaml
03_design_postgresql_schema.yaml
04_implement_listing_ingestion_api_endpoint.yaml
05_admin_auth_route_and_middleware.yaml
06_frontend_directory_page_rendering.yaml
07_frontend_subcategory_ui_and_layout.yaml
08_setup_featured_listings_logic.yaml
09_add_category_meta_and_seo_controls.yaml
10_scaffold_python_crawler_agent.yaml
11_llm_field_generation_via_jinja2.yaml
12_post_listings_from_crawler_to_api_server.yaml
13_create_json_crawler_configuration_schema.yaml
14_scaffold_admin_ui_application.yaml
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml

thinking
**Opening task file 19**
exec
bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
id: 19
title: Setup Logging, Healthchecks & Monitoring
description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
  or Railway logging hooks.
dependencies:
- 2
- 10

thinking
**Investigating logging and healthcheck setup**
exec
bash -lc 'ls api' in /var/home/rick/Development/mega-directory succeeded in 38ms:
Dockerfile
index.js
node_modules
package.json
package-lock.json
test

thinking
**Opening index.js file**
exec
bash -lc "sed -n '1,200p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
const express = require('express');
const jwt = require('jsonwebtoken');

const DEFAULT_PORT = 3001;
const DEFAULT_LISTING_STATUS = 'INACTIVE';
const MAX_SLUG_LENGTH = 80;
const DEFAULT_ADMIN_TOKEN_TTL_SECONDS = 60 * 15;

function resolveConfig(overrides = {}) {
  const parsedPort = process.env.PORT ? parseInt(process.env.PORT, 10) : undefined;
  const envPort = Number.isNaN(parsedPort) ? undefined : parsedPort;
  const parsedAdminTokenTtl =
    overrides.adminTokenTtlSeconds ??
    (process.env.ADMIN_TOKEN_TTL_SECONDS
      ? parseInt(process.env.ADMIN_TOKEN_TTL_SECONDS, 10)
      : undefined);

  const baseConfig = {
    port: overrides.port ?? envPort ?? DEFAULT_PORT,
    adminJwtSecret: overrides.adminJwtSecret ?? process.env.ADMIN_JWT_SECRET,
exec
bash -lc "sed -n '200,400p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
    console.log(`API server running at http://localhost:${port}`);
  });
}

if (require.main === module) {
  startServer();
}

module.exports = {
  createServer
};

function createListingIngestionHandler(app) {
  return (req, res) => {
    const payloads = normalizeListingBatch(req.body);
    if (!payloads) {
      return res
        .status(400)
        .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
    }
exec
bash -lc "sed -n '400,800p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  const email = value.trim();
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  if (!emailRegex.test(email)) {
    errors.push(`${fieldName} must be a valid email address`);
    return null;
  }
  return email.toLowerCase();
}

function sanitizeLocation(value) {
  if (!isPlainObject(value)) {
    return null;
  }
  const fields = ['addressLine1', 'addressLine2', 'city', 'region', 'postalCode', 'country'];
  const location = {};
  fields.forEach((field) => {
    const sanitized = sanitizeNullableString(value[field]);

thinking
**Checking existing health endpoint and logging**
exec
bash -lc 'rg -n "logging" -n' in /var/home/rick/Development/mega-directory succeeded in 473ms:
codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:64:  or Railway logging hooks.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:150:**Checking existing health endpoint and logging**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:152:bash -lc 'rg -n "logging" -n' in /var/home/rick/Development/mega-directory
codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:4:  or Railway logging hooks.
codex/TASKS/work_list:5:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/TASKS/CODEX_TODO.md:64:- [ ] 19_env_and_logging.yaml â€” Standardize `.env` handling, logging output format, and secrets for dev/prod environments.
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:70:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:82:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:4:  or Railway logging hooks.
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:355:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:358:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:359:  or Railway logging hooks.
codex/LOGS/tasklog_17_admin_ui:_seo_field_editor_1762860852.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_17_admin_ui:_seo_field_editor_1762860852.log:555:19_setup_logging,_healthchecks_and_monitoring.yaml

thinking
**Exploring agent logs and crawler**
exec
bash -lc 'ls agents' in /var/home/rick/Development/mega-directory succeeded in 38ms:
crawler

thinking
**Requesting more information**
exec
bash -lc 'ls agents/crawler' in /var/home/rick/Development/mega-directory succeeded in 39ms:
config
crawler.py
main.py
__pycache__
targets.example.json
tests

thinking
**Opening crawler.py**
exec
bash -lc "sed -n '1,200p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
from __future__ import annotations

from dataclasses import dataclass, field
import re
from typing import Any, Callable, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup
from jinja2 import Environment, StrictUndefined, TemplateError

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

        def __init__(self, *args, **kwargs):

thinking
**Investigating crawler logging requirements**
exec
bash -lc "sed -n '1,200p' agents/crawler/main.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
import json
from crawler import run_crawler

if __name__ == "__main__":
    with open("config/targets.json", "r") as f:
        config = json.load(f)
    run_crawler(config)

thinking
**Searching for run_crawler definition**
exec
bash -lc 'rg -n "run_crawler" -n agents/crawler' in /var/home/rick/Development/mega-directory succeeded in 41ms:
agents/crawler/tests/test_crawler.py:12:from crawler import run_crawler
agents/crawler/tests/test_crawler.py:63:def test_run_crawler_batches_locations_and_limits_results() -> None:
agents/crawler/tests/test_crawler.py:79:    batches = run_crawler(config, session=dummy_session)
agents/crawler/tests/test_crawler.py:123:def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
agents/crawler/tests/test_crawler.py:155:    batches = run_crawler(config)
agents/crawler/tests/test_crawler.py:161:def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
agents/crawler/tests/test_crawler.py:190:    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
agents/crawler/tests/test_crawler.py:201:def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
agents/crawler/tests/test_crawler.py:222:        run_crawler(config, session=DummySession(SAMPLE_HTML))
agents/crawler/tests/test_crawler.py:225:def test_run_crawler_posts_batches_to_api_endpoint() -> None:
agents/crawler/tests/test_crawler.py:239:    batches = run_crawler(config, session=session)
agents/crawler/tests/test_crawler.py:251:def test_run_crawler_can_post_to_dev_and_prod_targets() -> None:
agents/crawler/tests/test_crawler.py:275:    run_crawler(config, session=session)
agents/crawler/crawler.py:599:def run_crawler(
agents/crawler/main.py:2:from crawler import run_crawler
agents/crawler/main.py:7:    run_crawler(config)

thinking
**Continuing to retrieve more content**
exec
bash -lc "sed -n '200,400p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 37ms:
        prompt_template = config.get("prompt_template")
        if not provider or not model or not prompt_template:
            raise ValueError(
                f"AI field '{field_name}' requires provider, model, and prompt_template"
            )

        render_context = self._prepare_render_context(context, config)
        prompt = self._render_template(field_name, prompt_template, render_context)
        request = LLMRequest(
            provider=provider,
            model=model,
            prompt=prompt,
            field_name=field_name,
            options=config.get("options", {}),
            target=target,
            listing=context.get("listing"),
        )
        return self.llm_client(request)

    def _prepare_render_context(

thinking
**Inspecting run_crawler location**
exec
bash -lc "sed -n '400,800p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
            url = f"https:{url}"
        if not url.startswith("http"):
            url = f"https://{url.lstrip('/')}"
        return url

    @staticmethod
    def _normalize_subdomain(value: Optional[str]) -> str:
        if not value:
            return ""
        if value.startswith("http://") or value.startswith("https://"):
            return value.rstrip("/")
        return f"https://{value.strip('/')}"

    @staticmethod
    def _normalize_url(
        href: str,
        subdomain: Optional[str],
    ) -> str:
        if not href:
            return ""

thinking
**Planning logging enhancement for crawler and API**
exec
bash -lc 'rg -n "health" -n' in /var/home/rick/Development/mega-directory succeeded in 43ms:
codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
docs/deployment/railway.md:40:| Health check | `GET /health` |
docs/deployment/railway.md:108:- The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
api/test/server.test.js:93:test('health endpoint reports ok', () => {
api/test/server.test.js:95:  const route = findRoute(app, 'get', '/health');
api/index.js:70:  app.get('/health', (_req, res) => {
codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:150:**Checking existing health endpoint and logging**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:153:codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:154:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:155:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:156:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:158:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:64:  or Railway logging hooks.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:159:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**

thinking
**Investigating logging and healthcheck docs**
exec
bash -lc "sed -n '1,200p' docs/deployment/railway.md" in /var/home/rick/Development/mega-directory succeeded in 38ms:
# Railway Deployment Guide

Deploy the Mega Directory API, Astro frontend, and PostgreSQL database on [Railway](https://railway.app) using the Dockerfiles that live in this repository. Railway treats each runtime as an isolated service, so you will provision three services inside a single project:

1. **PostgreSQL database** (managed by Railway)
2. **Express API** built from `api/Dockerfile`
3. **Astro web frontend** built from `astro/Dockerfile`

The sections below walk through prerequisites, per-service configuration, and an end-to-end deployment workflow.

## Prerequisites

- Railway account with a project created (free tier works for small tests).
- [Railway CLI](https://docs.railway.app/reference/cli/installation) installed locally **or** access to the Railway dashboard.
- Docker installed locally so the CLI can build the images defined in this repo.
- Environment secrets (JWT secret, admin login, crawler token, etc.).

> **Tip:** Run `docker build` locally before pushing to Railway if you want to catch Dockerfile issues early:
>
> ```bash

thinking
**Reviewing health-related tests**
exec
bash -lc "sed -n '1,200p' api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 38ms:
const test = require('node:test');
const assert = require('node:assert');
const jwt = require('jsonwebtoken');
const { createServer } = require('..');

const baseConfig = Object.freeze({
  adminJwtSecret: 'test-admin-secret',
  adminJwtIssuer: 'mega-directory',
  adminJwtAudience: 'admin',
  adminLoginEmail: 'admin@example.com',
  adminLoginPasscode: 'letmein',
  crawlerBearerToken: 'crawler-token'
});

function findRoute(app, method, path) {
  const stack = (app._router && app._router.stack) || (app.router && app.router.stack) || [];
  const methodName = method.toLowerCase();
  return stack.find(
    (layer) => layer.route && layer.route.path === path && layer.route.methods[methodName]
  );

thinking
**Continuing process**
exec
bash -lc "sed -n '200,400p' api/test/server.test.js" in /var/home/rick/Development/mega-directory succeeded in 93ms:
  const res = runRoute(
    route,
    createRequest({ method: 'POST', headers: { Authorization: 'Bearer wrong-token' } }),
    createResponse()
  );

  assert.strictEqual(res.statusCode, 401);
  assert.strictEqual(res.body.error, 'Crawler token missing or invalid');
});

test('crawler ping accepts matching bearer token', () => {
  const app = createServer(baseConfig);
  const route = findRoute(app, 'post', '/v1/crawler/ping');
  const req = createRequest({
    method: 'POST',
    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` }
  });
  const res = runRoute(route, req, createResponse());

  assert.strictEqual(res.statusCode, 200);

thinking
**Planning logging and health checks**
exec
bash -lc 'rg -n "healthcheck" -n' in /var/home/rick/Development/mega-directory succeeded in 95ms:
codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
docker-compose.yml:20:    healthcheck:
docker-compose.yml:42:    healthcheck:
docker-compose.yml:64:    healthcheck:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:153:codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:154:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:155:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:156:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:158:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:64:  or Railway logging hooks.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:159:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:160:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:150:**Checking existing health endpoint and logging**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:161:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:152:bash -lc 'rg -n "logging" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:162:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:163:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:4:  or Railway logging hooks.

thinking
**Reviewing docker-compose for healthchecks**
exec
bash -lc 'cat docker-compose.yml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
version: '3.8'

services:
  api:
    build:
      context: ./api
    ports:
      - '3001:3001'
    environment:
      PORT: 3001
      DATABASE_URL: postgresql://postgres:password@db:5432/mega_directory
      ADMIN_JWT_SECRET: local-dev-secret
      ADMIN_LOGIN_EMAIL: admin@example.com
      ADMIN_LOGIN_PASSCODE: localpass
      CRAWLER_BEARER_TOKEN: crawler-dev-token
      ADMIN_TOKEN_TTL_SECONDS: 900
    depends_on:
      db:
        condition: service_healthy
    healthcheck:

thinking
**Reviewing healthchecks and documentation**
exec
bash -lc 'cat codex/TASKS/work_list' in /var/home/rick/Development/mega-directory succeeded in 38ms:
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml
20_verify_dev_environment_bootstraps_cleanly.yaml
21_dockerize_api_server.yaml
22_dockerize_astro_frontend.yaml

thinking
**Checking dependencies and logging needs**
exec
bash -lc "sed -n '1,200p' codex/TASKS/CODEX_TODO.md" in /var/home/rick/Development/mega-directory succeeded in 38ms:
# CODEX TODO

This document lists the Codex tasks in their recommended execution order.
Each task corresponds to a YAML file under `/codex/TASKS/`.

---

## 00 - Repository Setup

- [ ] 00_prepare_repository.yaml â€” Prepare the repository structure, move directories into place, update README and docker-compose config.

---

## 01â€“05 - Core Scaffolding

- [ ] 01_initialize_astro_frontend.yaml â€” Scaffold the Astro SSR frontend site.
- [ ] 02_initialize_api_server.yaml â€” Set up the Express/PostgreSQL API server with base routing and JWT auth.
- [ ] 03_initialize_database_schema.yaml â€” Define and migrate core tables: listings, categories, locations, directory.
- [ ] 04_initialize_admin_app.yaml â€” Scaffold a separate admin web interface with basic routing and layout.
- [ ] 05_initialize_python_crawler.yaml â€” Bootstrap the external Python-based crawler project with config loading and CLI entrypoint.

thinking
**Clarifying task alignment and test approach**
exec
bash -lc 'cat api/package.json' in /var/home/rick/Development/mega-directory succeeded in 37ms:
{
  "name": "mega-directory-api",
  "private": true,
  "version": "0.1.0",
  "main": "index.js",
  "scripts": {
    "dev": "node index.js",
    "start": "node index.js",
    "test": "node --test"
  },
  "dependencies": {
    "express": "^4.19.2",
    "jsonwebtoken": "^9.0.2"
  }
}

thinking
**Considering logging and healthcheck additions**
exec
bash -lc 'rg -n "UptimeRobot" -n .. -g*' in /var/home/rick/Development/mega-directory exited 124 in 10.17s:


thinking
**Planning focused search for UptimeRobot**
exec
bash -lc 'rg -n "UptimeRobot"' in /var/home/rick/Development/mega-directory succeeded in 43ms:
docs/deployment/railway.md:108:- The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:162:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:169:codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:358:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:305:docs/deployment/railway.md:108:- The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:309:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:313:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:320:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:410:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:416:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:421:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:162:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:510:bash -lc 'rg -n "UptimeRobot" -n .. -g*' in /var/home/rick/Development/mega-directory exited 124 in 10.17s:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:514:**Planning focused search for UptimeRobot**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:516:bash -lc 'rg -n "UptimeRobot"' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:358:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_18_deploy_web_+_api_to_railway_1762861251.log:1439:- The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot

thinking
**Reviewing documentation and tests for logging**
exec
bash -lc "sed -n '1,200p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
from pathlib import Path
from types import SimpleNamespace
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

import crawler
from crawler import run_crawler


SAMPLE_HTML = """
<section>
  <article data-listing>
    <a class="listing-link" href="https://example.com/acme">
      <span class="listing-title">ACME Electric</span>
    </a>

thinking
**Checking for logging tests**
exec
bash -lc "sed -n '200,400p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:

def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
    config = {
        "api_endpoint": "https://api.example.com",
        "api_token": "token",
        "targets": [
            {
                "category": "Electricians",
                "locations": ["Holland MI"],
                "fields": {
                    "description": {
                        "source": "ai",
                        "provider": "openai",
                        "model": "gpt-4",
                        "prompt_template": "Describe {{ listing.title }}",
                    }
                },
            }
        ],
    }

thinking
**Needing more input**
exec
bash -lc "sed -n '400,800p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:


thinking
**Planning logging and health check implementation**
exec
bash -lc "sed -n '1,200p' README.md" in /var/home/rick/Development/mega-directory succeeded in 38ms:
# Mega Directory

Mega Directory is a server-rendered directory listing platform built with Astro, Express, and Prisma. It supports dynamic subdomains, AI-generated content enrichment, and moderation workflows.

## Features

- ðŸ›°ï¸ Subdomain-based regional directories (e.g. electricians.nyc.example.com)
- âš¡ Express API with JWT-based auth
- ðŸ§  AI agents for crawling, enrichment, and moderation (via OpenRouter)
- ðŸ§© Admin dashboard for approving or rejecting listings
- ðŸ’… Tailwind CSS and minimal JavaScript for accessibility and performance
- ðŸ³ Dockerized and deployable behind Cloudflare with SSR

## Tech Stack

- Astro (SSR)
- Express.js
- Prisma + PostgreSQL
- Tailwind CSS
- OpenRouter (AI Agents)

thinking
**Planning logging and healthcheck features**
exec
bash -lc 'rg -n "logging"' in /var/home/rick/Development/mega-directory succeeded in 43ms:
codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:364:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:64:  or Railway logging hooks.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:150:**Checking existing health endpoint and logging**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:152:bash -lc 'rg -n "logging" -n' in /var/home/rick/Development/mega-directory succeeded in 473ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:153:codex.yaml:42:    path: codex/TASKS/19_setup_logging,_healthchecks_&_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:154:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:13:Implement the functionality described in codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml, then verify with any available tests.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:155:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:55:19_setup_logging,_healthchecks_and_monitoring.yaml
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:156:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:60:bash -lc 'cat codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml' in /var/home/rick/Development/mega-directory succeeded in 37ms:
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:157:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:63:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:158:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:64:  or Railway logging hooks.
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:159:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:70:**Investigating logging and healthcheck setup**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:160:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:150:**Checking existing health endpoint and logging**
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:161:codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:152:bash -lc 'rg -n "logging" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_19_setup_logging,_healthchecks_and_monitoring_1762861633.log:162:codex/TASKS/19_setup_logging,_healthchecks_and_monitoring.yaml:3:description: Add logging for API + crawler, with endpoint healthcheck. Suggest UptimeRobot

thinking
**Formulating multi-step logging plan**

Iâ€™m outlining a detailed plan for adding structured logging and enhanced health checks to both the API and crawler. The plan includes designing features, writing tests first per TDD, implementing code with logging toggles, and updating documentationâ€”all while ensuring each step is verifiable with tests and involves appropriate dependencies.
Plan update
  â†’ Review existing API and crawler code to design logging & healthcheck additions per task scope.
  â€¢ Add or update automated tests to capture the new logging/health behaviors before implementation.
  â€¢ Implement logging utilities, healthcheck output, and documentation updates across API and crawler, ensuring code passes tests.
  â€¢ Run available test suites (API + crawler) and verify everything passes.


thinking
**Summarizing logging and healthcheck design and tests**

I'm outlining detailed plans to add structured logging (using pino for API, Python logging for crawler), enhance healthcheck endpoints with more metadata, and update related tests. I'll integrate logger injection for better testability and document hooking to external monitoring tools. Next, Iâ€™ll implement these changes carefully to ensure smooth testing and useful diagnostics.
thinking
**Detailing logger test setup and API logging module**

Iâ€™m preparing tests to confirm ingestion logging emits structured info capturable via a stub logger and drafting the new API logging module using pino and pino-http. Iâ€™m also sketching plans to enhance docs and test health and logging comprehensively next.
file update
M /var/home/rick/Development/mega-directory/api/test/server.test.js
@@ -92,4 +92,24 @@
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
@@ -98,3 +118,6 @@
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
@@ -292,2 +315,22 @@
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
apply_patch(auto_approved=true) exited 0 in 41ms:
Success. Updated the following files:
M api/test/server.test.js
file update:
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Adding and designing stub logger tests**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/tests/test_crawler.py
@@ -62,2 +62,47 @@
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M agents/crawler/tests/test_crawler.py
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..7204f3a3a7f6786db5fe376872acf86a7aed2657
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..7204f3a3a7f6786db5fe376872acf86a7aed2657
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Adding new logging test**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/tests/test_crawler.py
@@ -331 +331,29 @@
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M agents/crawler/tests/test_crawler.py
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Updating plan status**
Plan update
  âœ“ Review existing API and crawler code to design logging & healthcheck additions per task scope.
  âœ“ Add or update automated tests to capture the new logging/health behaviors before implementation.
  â†’ Implement logging utilities, healthcheck output, and documentation updates across API and crawler, ensuring code passes tests.
  â€¢ Run available test suites (API + crawler) and verify everything passes.
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Implementing comprehensive API and crawler logging**

I'm planning to add detailed logging by creating a new `api/logger.js` with Pino for structured logs, update `api/index.js` to integrate this logger and enhance health checks, and improve crawler logging in Python with configurable log levels. I'll also prepare doc updates covering logging setup and monitoring guidance. This will make the API more observable and ready for production monitoring.
file update
A /var/home/rick/Development/mega-directory/api/logger.js
const pino = require('pino');
const pinoHttp = require('pino-http');

function resolveLogLevel(explicitLevel) {
  if (explicitLevel) {
    return explicitLevel;
  }
  if (process.env.LOG_LEVEL) {
    return process.env.LOG_LEVEL;
  }
  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
}

function createLogger(options = {}) {
  const level = resolveLogLevel(options.level);
  const destination = options.destination;
  const logger = pino(
    {
      name: options.name || 'mega-directory-api',
      level,
      redact: {
        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
        censor: '[REDACTED]'
      }
    },
    destination
  );
  return logger;
}

function createRequestLogger(logger) {
  if (!logger || typeof logger.info !== 'function') {
    return (_req, _res, next) => next();
  }
  return pinoHttp({
    logger,
    quietReqLogger: true,
    customLogLevel(res, err) {
      if (err || res.statusCode >= 500) {
        return 'error';
      }
      if (res.statusCode >= 400) {
        return 'warn';
      }
      return 'info';
    },
    customSuccessMessage(req, res) {
      return `${req.method} ${req.url} ${res.statusCode}`;
    },
    customErrorMessage(req, res, err) {
      if (err) {
        return `${req.method} ${req.url} failed`;
      }
      return `${req.method} ${req.url} ${res.statusCode}`;
    }
  });
}

module.exports = {
  createLogger,
  createRequestLogger
};
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
A api/logger.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Preparing to patch api/index.js**
file update
M /var/home/rick/Development/mega-directory/api/index.js
@@ -2,2 +2,3 @@
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/index.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..e5feed98597e0a20a7aa09a6330e8bf64b9df18b
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..e5feed98597e0a20a7aa09a6330e8bf64b9df18b
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Updating createServer function setup**
file update
M /var/home/rick/Development/mega-directory/api/index.js
@@ -62,3 +62,9 @@
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
@@ -67,3 +73,6 @@
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
@@ -71,3 +80,12 @@
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/index.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..918cb25e660e5ea3aa8afb057f80191f7261f718
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..918cb25e660e5ea3aa8afb057f80191f7261f718
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Updating server start logging**
file update
M /var/home/rick/Development/mega-directory/api/index.js
@@ -218,3 +218,7 @@
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/index.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..960992333503da9743c29b540b8f600840d1b66c
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..960992333503da9743c29b540b8f600840d1b66c
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Inserting logger helper function**
file update
M /var/home/rick/Development/mega-directory/api/index.js
@@ -234,2 +234,6 @@
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/index.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5b144fe2d0a9c238ab6681a77e725884fa6d9435
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,6 +232,10 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
     const payloads = normalizeListingBatch(req.body);
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5b144fe2d0a9c238ab6681a77e725884fa6d9435
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,6 +232,10 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
     const payloads = normalizeListingBatch(req.body);
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Adding logging to ingestion handler**
file update
M /var/home/rick/Development/mega-directory/api/index.js
@@ -240,4 +240,9 @@
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
@@ -259,2 +264,10 @@
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
@@ -266,2 +279,11 @@
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/index.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Verifying logger setup and dependency installation**
exec
bash -lc 'ls node_modules | head' in /var/home/rick/Development/mega-directory/api succeeded in 91ms:
accepts
array-flatten
body-parser
buffer-equal-constant-time
bytes
call-bind-apply-helpers
call-bound
content-disposition
content-type
cookie
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Checking for pino package presence**
exec
bash -lc "rg -n \"\\\"pino\\\"\" package-lock.json" in /var/home/rick/Development/mega-directory/api exited 1 in 40ms:

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..f4b4714434bc9b6fca108f9f878f8c44aa79fa16
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,62 @@
+const pino = require('pino');
+const pinoHttp = require('pino-http');
+
+function resolveLogLevel(explicitLevel) {
+  if (explicitLevel) {
+    return explicitLevel;
+  }
+  if (process.env.LOG_LEVEL) {
+    return process.env.LOG_LEVEL;
+  }
+  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+}
+
+function createLogger(options = {}) {
+  const level = resolveLogLevel(options.level);
+  const destination = options.destination;
+  const logger = pino(
+    {
+      name: options.name || 'mega-directory-api',
+      level,
+      redact: {
+        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
+        censor: '[REDACTED]'
+      }
+    },
+    destination
+  );
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return pinoHttp({
+    logger,
+    quietReqLogger: true,
+    customLogLevel(res, err) {
+      if (err || res.statusCode >= 500) {
+        return 'error';
+      }
+      if (res.statusCode >= 400) {
+        return 'warn';
+      }
+      return 'info';
+    },
+    customSuccessMessage(req, res) {
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    },
+    customErrorMessage(req, res, err) {
+      if (err) {
+        return `${req.method} ${req.url} failed`;
+      }
+      return `${req.method} ${req.url} ${res.statusCode}`;
+    }
+  });
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Designing custom logging without external packages**

I'm rethinking our logging approach since we can't install new dependencies like `pino`. I'll replace the current `api/logger.js` with a simple custom logger using built-in `console` and `process.stdout`, supporting common log levels, structured JSON output, and a request-logging middleware built with native Node.js features. This avoids network issues, keeps functionality similar, and removes the need for extra installs.
file update
M /var/home/rick/Development/mega-directory/api/logger.js
@@ -1,62 +0,0 @@
-const pino = require('pino');
-const pinoHttp = require('pino-http');
-
-function resolveLogLevel(explicitLevel) {
-  if (explicitLevel) {
-    return explicitLevel;
-  }
-  if (process.env.LOG_LEVEL) {
-    return process.env.LOG_LEVEL;
-  }
-  return process.env.NODE_ENV === 'production' ? 'info' : 'debug';
-}
-
-function createLogger(options = {}) {
-  const level = resolveLogLevel(options.level);
-  const destination = options.destination;
-  const logger = pino(
-    {
-      name: options.name || 'mega-directory-api',
-      level,
-      redact: {
-        paths: ['req.headers.authorization', 'headers.authorization', 'req.body.passcode'],
-        censor: '[REDACTED]'
-      }
-    },
-    destination
-  );
-  return logger;
-}
-
-function createRequestLogger(logger) {
-  if (!logger || typeof logger.info !== 'function') {
-    return (_req, _res, next) => next();
-  }
-  return pinoHttp({
-    logger,
-    quietReqLogger: true,
-    customLogLevel(res, err) {
-      if (err || res.statusCode >= 500) {
-        return 'error';
-      }
-      if (res.statusCode >= 400) {
-        return 'warn';
-      }
-      return 'info';
-    },
-    customSuccessMessage(req, res) {
-      return `${req.method} ${req.url} ${res.statusCode}`;
-    },
-    customErrorMessage(req, res, err) {
-      if (err) {
-        return `${req.method} ${req.url} failed`;
-      }
-      return `${req.method} ${req.url} ${res.statusCode}`;
-    }
-  });
-}
-
-module.exports = {
-  createLogger,
-  createRequestLogger
-};
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/logger.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Adding custom logging code**
file update
M /var/home/rick/Development/mega-directory/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M api/logger.js
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Adding logging to crawler module**

Iâ€™m planning to integrate Pythonâ€™s standard logging into the crawler by importing `logging` and `os`, adding helper functions to configure log levels from environment variables, and injecting a logger instance into the `Crawler` class with informative log calls throughout the run cycle. This involves updating method signatures, adding informative logs especially around batch processing, and ensuring compatibility with existing test stubs.
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Addressing patch failure step-by-step**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -3,2 +3,4 @@
 from dataclasses import dataclass, field
+import logging
+import os
 import re
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..3ad0866e6967b46d0b70e8e960cf22e145e98d6d
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..3ad0866e6967b46d0b70e8e960cf22e145e98d6d
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Patching class definition**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -262,2 +262,3 @@
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
@@ -270,2 +271,3 @@
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
@@ -273,2 +275,4 @@
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
@@ -301,2 +305,3 @@
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
apply_patch(auto_approved=true) exited 0 in 45ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..9bf434a7cdc5089466d5f5908513fc3721d3963e
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..9bf434a7cdc5089466d5f5908513fc3721d3963e
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Preparing to patch API batch method**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -486,2 +486,12 @@
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..f1493dae7d5f9ac02731d14fb9ae4b4a52a5db31
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..f1493dae7d5f9ac02731d14fb9ae4b4a52a5db31
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Enhancing logger setup with default and extra handling**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -619,4 +619,27 @@
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Verifying log level and logger behavior**
exec
bash -lc 'rg -n "thisTimeout" -n agents/crawler/crawler.py' in /var/home/rick/Development/mega-directory exited 1 in 39ms:

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Validating code snippet**
exec
bash -lc "sed -n '520,700p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:

    def _build_listing_payload(
        self,
        listing: Listing,
        target_config: Dict[str, Any],
        default_category_slug: str,
    ) -> Optional[Dict[str, Any]]:
        payload: Dict[str, Any] = dict(listing.fields or {})
        title = self._normalize_string(payload.get("title")) or self._normalize_string(listing.title)
        if not title:
            return None
        payload["title"] = title

        summary = self._normalize_string(payload.get("summary"))
        snippet = self._normalize_string(listing.snippet)
        if not summary and snippet:
            payload["summary"] = snippet

        source_url = self._normalize_string(payload.get("sourceUrl")) or self._normalize_string(listing.url)
        if source_url:
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Reviewing _post_batch_to_api function**
exec
bash -lc "sed -n '420,520p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
    @staticmethod
    def _normalize_url(
        href: str,
        subdomain: Optional[str],
    ) -> str:
        if not href:
            return ""
        if href.startswith("http://") or href.startswith("https://"):
            return href
        base = Crawler._normalize_subdomain(subdomain) or "https://"
        return urljoin(base + "/", href.lstrip("/"))

    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
        targets_config = config.get("api_targets")
        resolved: List[APITarget] = []

        if isinstance(targets_config, list) and targets_config:
            for entry in targets_config:
                if not isinstance(entry, dict):
                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Reviewing logging integration and documentation updates**

I'm making sure logging setup handles log levels, avoids duplicate handlers, and integrates with existing code without breaking things. I'll also prepare new documentation sections about logging and monitoring, including environment variables and health checks, to keep the user well informed.
file update
M /var/home/rick/Development/mega-directory/README.md
@@ -45,2 +45,9 @@
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M README.md
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');

file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');


thinking
**Updating deployment documentation for new environment variables**
file update
M /var/home/rick/Development/mega-directory/docs/deployment/railway.md
@@ -54,2 +54,3 @@
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
apply_patch(auto_approved=true) exited 0 in 13ms:
Success. Updated the following files:
M docs/deployment/railway.md
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..7cb30cce751f39a35a4e6530cdc07b4f4805775e
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 

file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..7cb30cce751f39a35a4e6530cdc07b4f4805775e
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 


thinking
**Adding monitoring doc reference**
file update
M /var/home/rick/Development/mega-directory/docs/deployment/railway.md
@@ -110,2 +110,3 @@
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M docs/deployment/railway.md
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.

file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.


thinking
**Creating monitoring documentation**

Iâ€™m planning to draft a new monitoring.md file to cover logging setup, API and crawler log configurations, and instructions for integrating with UptimeRobot and Railway logs. Iâ€™ll assemble content including the health-check monitor setup and environment-specific logging tips, then apply the patch to add this documentation file.
file update
A /var/home/rick/Development/mega-directory/docs/monitoring.md
# Monitoring & Logging Guide

This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.

## API Service

- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.

### UptimeRobot example

1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.

Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.

## Crawler Agent

- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.

## Quick checklist

- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.
apply_patch(auto_approved=true) exited 0 in 14ms:
Success. Updated the following files:
A docs/monitoring.md
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.

file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Verifying code integrity and test readiness**

I'm double-checking that all key modules and configurations are correctly set up without mutation or undefined values, ensuring logging and health objects behave as expected. Now, I'm preparing to run the API tests with npm and crawler tests with pytest to confirm everything works smoothly.
exec
bash -lc 'npm test' in /var/home/rick/Development/mega-directory/api succeeded in 1.79s:

> mega-directory-api@0.1.0 test
> node --test

TAP version 13
# Subtest: test/server.test.js
ok 1 - test/server.test.js
  ---
  duration_ms: 1147.871232
  type: 'test'
  ...
1..1
# tests 1
# suites 0
# pass 1
# fail 0
# cancelled 0
# skipped 0
# todo 0
# duration_ms 1168.860016
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Running crawler tests**
exec
bash -lc pytest in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 10.78s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory/agents/crawler
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 10 items

tests/test_config_schema.py ..                                           [ 20%]
tests/test_crawler.py ........                                           [100%]

============================== 10 passed in 3.87s ==============================
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Updating plan after tests**
Plan update
  âœ“ Review existing API and crawler code to design logging & healthcheck additions per task scope.
  âœ“ Add or update automated tests to capture the new logging/health behaviors before implementation.
  âœ“ Implement logging utilities, healthcheck output, and documentation updates across API and crawler, ensuring code passes tests.
  âœ“ Run available test suites (API + crawler) and verify everything passes.
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Checking git status**
exec
bash -lc 'git status -sb' in /var/home/rick/Development/mega-directory succeeded in 41ms:
## main...origin/main [ahead 4]
 M README.md
 M agents/crawler/crawler.py
 M agents/crawler/tests/test_crawler.py
 M api/index.js
 M api/test/server.test.js
?? admin/data/categoryLocationDiscoveries.js
?? admin/data/categoryLocations.js
?? admin/routes/categoryLocations.js
?? admin/services/apiClient.js
?? admin/services/categoryLocationService.js
?? admin/test/
?? admin/views/category-locations/
?? api/.dockerignore
?? api/logger.js
?? astro/.dockerignore
?? codex/LOGS/codex_run_2025-11-11_06-14-32.log
?? codex/LOGS/tasklog_15_admin_ui:_listing_review_table_1762859672.log
?? codex/LOGS/tasklog_15_admin_ui:_listing_review_table_1762859672.log.done
?? codex/LOGS/tasklog_16_admin_ui:_category_location_approval_1762860436.log
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Organizing final change summary details**
exec
bash -lc "nl -ba index.js | sed -n '1,200p'" in /var/home/rick/Development/mega-directory/api succeeded in 39ms:
     1	const express = require('express');
     2	const jwt = require('jsonwebtoken');
     3	const { createLogger, createRequestLogger } = require('./logger');
     4	
     5	const DEFAULT_PORT = 3001;
     6	const DEFAULT_LISTING_STATUS = 'INACTIVE';
     7	const MAX_SLUG_LENGTH = 80;
     8	const DEFAULT_ADMIN_TOKEN_TTL_SECONDS = 60 * 15;
     9	
    10	function resolveConfig(overrides = {}) {
    11	  const parsedPort = process.env.PORT ? parseInt(process.env.PORT, 10) : undefined;
    12	  const envPort = Number.isNaN(parsedPort) ? undefined : parsedPort;
    13	  const parsedAdminTokenTtl =
    14	    overrides.adminTokenTtlSeconds ??
    15	    (process.env.ADMIN_TOKEN_TTL_SECONDS
    16	      ? parseInt(process.env.ADMIN_TOKEN_TTL_SECONDS, 10)
    17	      : undefined);
    18	
    19	  const baseConfig = {
    20	    port: overrides.port ?? envPort ?? DEFAULT_PORT,
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Adding line numbers near ingestion handler**
exec
bash -lc "nl -ba index.js | sed -n '200,420p'" in /var/home/rick/Development/mega-directory/api succeeded in 39ms:
   200	        issuer: config.adminJwtIssuer,
   201	        audience: config.adminJwtAudience,
   202	        expiresIn
   203	      }
   204	    );
   205	
   206	    return res.json({
   207	      token,
   208	      tokenType: 'Bearer',
   209	      expiresIn
   210	    });
   211	  };
   212	}
   213	
   214	function startServer() {
   215	  const app = createServer();
   216	  const { port } = app.locals.config ?? { port: DEFAULT_PORT };
   217	
   218	  app.listen(port, () => {
   219	    const logger = getLogger(app);
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Adding line numbers to file**
exec
bash -lc 'nl -ba logger.js' in /var/home/rick/Development/mega-directory/api succeeded in 38ms:
     1	const os = require('os');
     2	
     3	const LEVEL_INDEX = {
     4	  fatal: 0,
     5	  error: 1,
     6	  warn: 2,
     7	  info: 3,
     8	  debug: 4,
     9	  trace: 5
    10	};
    11	
    12	function resolveLevelName(level) {
    13	  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
    14	  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
    15	  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
    16	}
    17	
    18	function createLogger(options = {}) {
    19	  const levelName = resolveLevelName(options.level);
    20	  const threshold = LEVEL_INDEX[levelName];
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Preparing line number references for tests**
exec
bash -lc "nl -ba test/server.test.js | sed -n '1,200p'" in /var/home/rick/Development/mega-directory/api succeeded in 38ms:
     1	const test = require('node:test');
     2	const assert = require('node:assert');
     3	const jwt = require('jsonwebtoken');
     4	const { createServer } = require('..');
     5	
     6	const baseConfig = Object.freeze({
     7	  adminJwtSecret: 'test-admin-secret',
     8	  adminJwtIssuer: 'mega-directory',
     9	  adminJwtAudience: 'admin',
    10	  adminLoginEmail: 'admin@example.com',
    11	  adminLoginPasscode: 'letmein',
    12	  crawlerBearerToken: 'crawler-token'
    13	});
    14	
    15	function findRoute(app, method, path) {
    16	  const stack = (app._router && app._router.stack) || (app.router && app.router.stack) || [];
    17	  const methodName = method.toLowerCase();
    18	  return stack.find(
    19	    (layer) => layer.route && layer.route.path === path && layer.route.methods[methodName]
    20	  );
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Continuing to review ingestion logging test**
exec
bash -lc "nl -ba test/server.test.js | sed -n '200,400p'" in /var/home/rick/Development/mega-directory/api succeeded in 39ms:
   200	  assert.strictEqual(loginRes.body.expiresIn, 900);
   201	
   202	  const decoded = jwt.verify(loginRes.body.token, baseConfig.adminJwtSecret, {
   203	    issuer: baseConfig.adminJwtIssuer,
   204	    audience: baseConfig.adminJwtAudience
   205	  });
   206	  assert.strictEqual(decoded.role, 'admin');
   207	  assert.strictEqual(decoded.sub, baseConfig.adminLoginEmail.toLowerCase());
   208	
   209	  const pingRoute = findRoute(app, 'get', '/v1/admin/ping');
   210	  const pingRes = runRoute(
   211	    pingRoute,
   212	    createRequest({ headers: { Authorization: `Bearer ${loginRes.body.token}` } }),
   213	    createResponse()
   214	  );
   215	
   216	  assert.strictEqual(pingRes.statusCode, 200);
   217	  assert.deepStrictEqual(pingRes.body, { status: 'admin-ok' });
   218	});
   219	
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Adding line numbers to test file**
exec
bash -lc "nl -ba tests/test_crawler.py | sed -n '1,200p'" in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 39ms:
     1	from pathlib import Path
     2	from types import SimpleNamespace
     3	import sys
     4	
     5	import pytest
     6	
     7	AGENT_DIR = Path(__file__).resolve().parents[1]
     8	if str(AGENT_DIR) not in sys.path:
     9	    sys.path.insert(0, str(AGENT_DIR))
    10	
    11	import crawler
    12	from crawler import run_crawler
    13	
    14	
    15	SAMPLE_HTML = """
    16	<section>
    17	  <article data-listing>
    18	    <a class="listing-link" href="https://example.com/acme">
    19	      <span class="listing-title">ACME Electric</span>
    20	    </a>
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Locating lines near new logging test**
exec
bash -lc "nl -ba tests/test_crawler.py | sed -n '200,400p'" in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 39ms:
   200	    batches = run_crawler(config)
   201	
   202	    assert captured["kwargs"]["expire_after"] > 0
   203	    assert len(batches) == 1
   204	
   205	
   206	def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
   207	    llm_requests = []
   208	
   209	    def fake_llm_client(request):  # type: ignore[no-untyped-def]
   210	        llm_requests.append(request)
   211	        return f"LLM::{request.field_name}::{request.prompt}"
   212	
   213	    config = {
   214	        "api_endpoint": "https://api.example.com",
   215	        "api_token": "token",
   216	        "targets": [
   217	            {
   218	                "category": "Electricians",
   219	                "subdomain": "electric.example.com",
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Identifying logging references in crawler.py**
exec
bash -lc "nl -ba crawler.py | sed -n '200,360p'" in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 39ms:
   200	        provider = config.get("provider")
   201	        model = config.get("model")
   202	        prompt_template = config.get("prompt_template")
   203	        if not provider or not model or not prompt_template:
   204	            raise ValueError(
   205	                f"AI field '{field_name}' requires provider, model, and prompt_template"
   206	            )
   207	
   208	        render_context = self._prepare_render_context(context, config)
   209	        prompt = self._render_template(field_name, prompt_template, render_context)
   210	        request = LLMRequest(
   211	            provider=provider,
   212	            model=model,
   213	            prompt=prompt,
   214	            field_name=field_name,
   215	            options=config.get("options", {}),
   216	            target=target,
   217	            listing=context.get("listing"),
   218	        )
   219	        return self.llm_client(request)
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Adding line numbers for reference**
exec
bash -lc "nl -ba crawler.py | sed -n '420,520p'" in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 39ms:
   420	    @staticmethod
   421	    def _normalize_url(
   422	        href: str,
   423	        subdomain: Optional[str],
   424	    ) -> str:
   425	        if not href:
   426	            return ""
   427	        if href.startswith("http://") or href.startswith("https://"):
   428	            return href
   429	        base = Crawler._normalize_subdomain(subdomain) or "https://"
   430	        return urljoin(base + "/", href.lstrip("/"))
   431	
   432	    def _resolve_api_targets(self, config: Dict[str, Any]) -> List[APITarget]:
   433	        targets_config = config.get("api_targets")
   434	        resolved: List[APITarget] = []
   435	
   436	        if isinstance(targets_config, list) and targets_config:
   437	            for entry in targets_config:
   438	                if not isinstance(entry, dict):
   439	                    raise ValueError("Each api_targets entry must be an object with endpoint and token")
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Extracting lines for logger function**
exec
bash -lc "nl -ba crawler.py | sed -n '640,780p'" in /var/home/rick/Development/mega-directory/agents/crawler succeeded in 39ms:
   640	            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
   641	        )
   642	        logger.addHandler(handler)
   643	    logger.setLevel(_resolve_log_level())
   644	    logger.propagate = False
   645	    return logger
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Preparing file references with line numbers**
exec
bash -lc "nl -ba docs/deployment/railway.md | sed -n '1,200p'" in /var/home/rick/Development/mega-directory succeeded in 39ms:
     1	# Railway Deployment Guide
     2	
     3	Deploy the Mega Directory API, Astro frontend, and PostgreSQL database on [Railway](https://railway.app) using the Dockerfiles that live in this repository. Railway treats each runtime as an isolated service, so you will provision three services inside a single project:
     4	
     5	1. **PostgreSQL database** (managed by Railway)
     6	2. **Express API** built from `api/Dockerfile`
     7	3. **Astro web frontend** built from `astro/Dockerfile`
     8	
     9	The sections below walk through prerequisites, per-service configuration, and an end-to-end deployment workflow.
    10	
    11	## Prerequisites
    12	
    13	- Railway account with a project created (free tier works for small tests).
    14	- [Railway CLI](https://docs.railway.app/reference/cli/installation) installed locally **or** access to the Railway dashboard.
    15	- Docker installed locally so the CLI can build the images defined in this repo.
    16	- Environment secrets (JWT secret, admin login, crawler token, etc.).
    17	
    18	> **Tip:** Run `docker build` locally before pushing to Railway if you want to catch Dockerfile issues early:
    19	>
    20	> ```bash
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Checking README line numbers**
exec
bash -lc "nl -ba README.md | sed -n '1,200p'" in /var/home/rick/Development/mega-directory succeeded in 38ms:
     1	# Mega Directory
     2	
     3	Mega Directory is a server-rendered directory listing platform built with Astro, Express, and Prisma. It supports dynamic subdomains, AI-generated content enrichment, and moderation workflows.
     4	
     5	## Features
     6	
     7	- ðŸ›°ï¸ Subdomain-based regional directories (e.g. electricians.nyc.example.com)
     8	- âš¡ Express API with JWT-based auth
     9	- ðŸ§  AI agents for crawling, enrichment, and moderation (via OpenRouter)
    10	- ðŸ§© Admin dashboard for approving or rejecting listings
    11	- ðŸ’… Tailwind CSS and minimal JavaScript for accessibility and performance
    12	- ðŸ³ Dockerized and deployable behind Cloudflare with SSR
    13	
    14	## Tech Stack
    15	
    16	- Astro (SSR)
    17	- Express.js
    18	- Prisma + PostgreSQL
    19	- Tailwind CSS
    20	- OpenRouter (AI Agents)
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Considering line numbering for docs/monitoring.md**
exec
bash -lc 'nl -ba docs/monitoring.md' in /var/home/rick/Development/mega-directory succeeded in 37ms:
     1	# Monitoring & Logging Guide
     2	
     3	This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
     4	
     5	## API Service
     6	
     7	- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
     8	- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
     9	- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
    10	- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
    11	
    12	### UptimeRobot example
    13	
    14	1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
    15	2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
    16	3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
    17	
    18	Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
    19	
    20	## Crawler Agent
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.


thinking
**Summarizing structured logging updates**

I've put together a concise summary explaining the new structured logging and enhanced health metadata integrated into the API and crawler, along with documentation about monitoring setup and environment variables. The details specifically mention key files and line ranges touched, testing verification, and suggestions for next steps involving database health integration.
codex
Implemented structured logging plus richer health metadata for the API and crawler, along with documentation on wiring monitors and new tests to lock it all down.

**Updates**
- API now boots with a shared logger, request middleware, and `/health` telemetry (`api/index.js:62-300`). Batch ingestion warns on invalid payloads and emits summary logs so external monitors/UptimeRobot have context, while server startup logs go through the same channel.
- Added a lightweight JSON logger + request-duration middleware that default to `LOG_LEVEL` but can be overridden or swapped for stubs in tests (`api/logger.js:1-112`).
- Extended the API test suite with a stub logger to assert health metadata and ingestion logging behavior (`api/test/server.test.js:93-360`).
- Crawler instances accept an optional logger, default to a configured Python `logging` instance, and log run/batch progress before POSTing (`agents/crawler/crawler.py:255-507,640-645`). Tests cover the new hooks via a `RecordingLogger` (`agents/crawler/tests/test_crawler.py:63-359`).
- Documented how to use the new knobs: README logging section (`README.md:46-51`), Railway env table (`docs/deployment/railway.md:35-111`), and a fresh monitoring runbook that calls out UptimeRobot + Railway log tips (`docs/monitoring.md:1-31`).

**Tests**
- `npm test` (from `api/`)
- `pytest` (from `agents/crawler/`)

Next up you may want to extend `/health` with database connectivity once the persistence layer lands so monitors can catch DB outages too.
file update:
diff --git a/README.md b/README.md
index e061769d899d7a49b6e9a88f4cc06bb945094fe2..5eb512f6463e6465540423d5d6371ad757533b3c
--- a/README.md
+++ b/README.md
@@ -43,6 +43,13 @@
 
 Provide an `.env` file for the API service (e.g., JWT secrets, database URLs) before running Compose. Postgres data is stored in the `pgdata` volume declared in the compose file.
 
+## Logging & Monitoring
+
+- The Express API emits JSON logs (request + application events) to stdout. Override the level with `LOG_LEVEL=info` or `debug` as neededâ€”Railway and Docker automatically capture the output.
+- `GET /health` now returns uptime metadata so monitors like UptimeRobot or Better Stack can alert when the service fails its liveness probe.
+- The Python crawler uses the standard `logging` module; set `CRAWLER_LOG_LEVEL` (defaults to `DEBUG` locally, `INFO` in production) to tune verbosity when running long jobs.
+- See `docs/monitoring.md` for tips on wiring the `/health` endpoint into UptimeRobot and for tailing logs via Railway CLI.
+
 ## Railway Deployment
 
 The repository is ready to deploy on [Railway](https://railway.app) using the Dockerfiles under `api/` and `astro/`. Follow the step-by-step guide in `docs/deployment/railway.md` to provision the Postgres plugin, deploy the two services, and wire their environment variables together.
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 3d1f3b345415af8939c4c3ba592510a0e75c8479..276346a85055b18b9239018b9bab759065b7fbef
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,6 +1,8 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
+import logging
+import os
 import re
 from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -258,6 +260,7 @@
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
         llm_client: Optional[LLMClient] = None,
+        logger: Optional[logging.Logger] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -266,9 +269,12 @@
         )
         self.request_timeout = request_timeout
         self.field_generator = FieldGenerator(llm_client=llm_client)
+        self.logger = logger or _build_default_logger()
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         api_targets = self._resolve_api_targets(config)
+        target_count = len(config.get("targets") or [])
+        self.logger.info("Starting crawler run for %s targets", target_count)
         batches: List[CrawlerBatch] = []
         for target in config.get("targets", []):
             for location_batch in self._iter_location_batches(target):
@@ -297,6 +303,7 @@
                     api_targets,
                     config,
                 )
+        self.logger.info("Crawler finished with %s batches", len(batches))
         return batches
 
     def fetch_listings(
@@ -477,6 +484,16 @@
             return
         for api_target in api_targets:
             timeout = self._resolve_api_timeout(api_target, target_config, root_config)
+            self.logger.info(
+                "Posting %s listings to %s",
+                len(payloads),
+                api_target.name,
+                extra={
+                    "batch_category": batch.category,
+                    "batch_location": batch.location,
+                    "api_endpoint": api_target.endpoint,
+                },
+            )
             response = self.session.post(
                 api_target.endpoint,
                 json={"listings": payloads},
@@ -600,6 +617,29 @@
     config: Dict[str, Any],
     session: Optional[requests.Session] = None,
     llm_client: Optional[LLMClient] = None,
+    logger: Optional[logging.Logger] = None,
 ) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session, llm_client=llm_client)
+    crawler = Crawler(session=session, llm_client=llm_client, logger=logger)
     return crawler.run(config)
+
+
+def _resolve_log_level() -> int:
+    configured = (
+        os.getenv("CRAWLER_LOG_LEVEL")
+        or os.getenv("LOG_LEVEL")
+        or ("INFO" if os.getenv("NODE_ENV") == "production" else "DEBUG")
+    ).upper()
+    return getattr(logging, configured, logging.INFO)
+
+
+def _build_default_logger() -> logging.Logger:
+    logger = logging.getLogger("mega_directory.crawler")
+    if not logger.handlers:
+        handler = logging.StreamHandler()
+        handler.setFormatter(
+            logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
+        )
+        logger.addHandler(handler)
+    logger.setLevel(_resolve_log_level())
+    logger.propagate = False
+    return logger
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 2c90608e41ea1976bae6e77c4cc8feecf85d05e1..a62dfb05c7ce6a712aaa16192189d677926c9cd6
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -60,6 +60,51 @@
         return DummyResponse("")
 
 
+class RecordingLogger:
+    def __init__(self) -> None:
+        self.records = []
+
+    def info(self, message: str, *args, **kwargs) -> None:
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "info",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def debug(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "debug",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def warning(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "warning",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+    def error(self, message: str, *args, **kwargs) -> None:  # pragma: no cover - helper
+        formatted = message % args if args else message
+        self.records.append(
+            {
+                "level": "error",
+                "message": formatted,
+                "extra": kwargs,
+            }
+        )
+
+
 def test_run_crawler_batches_locations_and_limits_results() -> None:
     config = {
         "api_endpoint": "https://api.example.com",
@@ -284,3 +329,31 @@
         "https://dev.example.com/v1/crawler/listings",
         "https://prod.example.com/v1/crawler/listings",
     }
+
+
+def test_run_crawler_emits_logging_metadata() -> None:
+    config = {
+        "api_targets": [
+            {
+                "name": "dev",
+                "endpoint": "https://dev.example.com/v1/crawler/listings",
+                "token": "dev-token",
+            }
+        ],
+        "targets": [
+            {
+                "category": "Photographers",
+                "locations": ["Grand Rapids MI"],
+                "listings_per_location": 1,
+            }
+        ],
+    }
+    session = DummySession(SAMPLE_HTML)
+    logger = RecordingLogger()
+
+    run_crawler(config, session=session, logger=logger)
+
+    assert logger.records, "expected crawler to log activity"
+    assert any(
+        "Posting 1 listings to dev" in record["message"] for record in logger.records
+    ), "expected post log entry"
diff --git a/api/index.js b/api/index.js
index b8b17f11d459d0dc2f538955e1ec16a983fd2f52..5422487e3bf347b7e1df212d1d0c213b7b9057cc
--- a/api/index.js
+++ b/api/index.js
@@ -1,5 +1,6 @@
 const express = require('express');
 const jwt = require('jsonwebtoken');
+const { createLogger, createRequestLogger } = require('./logger');
 
 const DEFAULT_PORT = 3001;
 const DEFAULT_LISTING_STATUS = 'INACTIVE';
@@ -59,16 +60,34 @@
 }
 
 function createServer(overrides = {}) {
-  const config = resolveConfig(overrides);
+  const { logger: loggerOverride, logLevel, ...configOverrides } = overrides;
+  const config = resolveConfig(configOverrides);
+  const logger =
+    loggerOverride ||
+    createLogger({
+      level: typeof logLevel === 'string' ? logLevel : undefined
+    });
   const app = express();
 
   app.locals.config = config;
   app.locals.ingestionStore = createListingStore();
+  app.locals.logger = logger;
+  app.locals.health = { startedAt: new Date() };
 
+  app.use(createRequestLogger(logger));
   app.use(express.json());
 
   app.get('/health', (_req, res) => {
-    res.json({ status: 'ok' });
+    const now = new Date();
+    const startedAt = app.locals.health.startedAt;
+    app.locals.health.lastCheck = now;
+    res.json({
+      status: 'ok',
+      uptime: process.uptime(),
+      startedAt: startedAt.toISOString(),
+      timestamp: now.toISOString(),
+      environment: process.env.NODE_ENV || 'development'
+    });
   });
 
   app.post('/v1/admin/auth', createAdminAuthHandler(config));
@@ -197,7 +216,11 @@
   const { port } = app.locals.config ?? { port: DEFAULT_PORT };
 
   app.listen(port, () => {
-    console.log(`API server running at http://localhost:${port}`);
+    const logger = getLogger(app);
+    logger.info(
+      { event: 'api.start', port, environment: process.env.NODE_ENV || 'development' },
+      `API server running at http://localhost:${port}`
+    );
   });
 }
 
@@ -209,10 +232,19 @@
   createServer
 };
 
+function getLogger(app) {
+  return (app && app.locals && app.locals.logger) || console;
+}
+
 function createListingIngestionHandler(app) {
   return (req, res) => {
+    const logger = getLogger(app);
     const payloads = normalizeListingBatch(req.body);
     if (!payloads) {
+      logger.warn(
+        { event: 'listings.ingest.invalid', reason: 'bad-shape' },
+        'Invalid listing payload received'
+      );
       return res
         .status(400)
         .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
@@ -230,6 +262,14 @@
       .map((item) => ({ index: item.index, messages: item.result.errors }));
 
     if (invalidEntries.length > 0) {
+      logger.warn(
+        {
+          event: 'listings.ingest.invalid',
+          reason: 'validation-failed',
+          failures: invalidEntries.length
+        },
+        'Rejected crawler listings batch due to validation errors'
+      );
       return res.status(400).json({ error: 'Invalid listing payload', details: invalidEntries });
     }
 
@@ -237,6 +277,15 @@
     const savedRecords = validations.map((validation, index) =>
       store.insert(validation.value, payloads[index])
     );
+    const categories = Array.from(new Set(savedRecords.map((record) => record.categorySlug)));
+    logger.info(
+      {
+        event: 'listings.ingested',
+        ingestedCount: savedRecords.length,
+        categories
+      },
+      'Accepted crawler listings batch'
+    );
 
     return res.status(202).json({
       ingestedCount: savedRecords.length,
diff --git a/api/logger.js b/api/logger.js
new file mode 100644
index 0000000000000000000000000000000000000000..cdbd299e1b220a7a471e1dcdf1de3dabab5e4a33
--- /dev/null
+++ b/api/logger.js
@@ -0,0 +1,112 @@
+const os = require('os');
+
+const LEVEL_INDEX = {
+  fatal: 0,
+  error: 1,
+  warn: 2,
+  info: 3,
+  debug: 4,
+  trace: 5
+};
+
+function resolveLevelName(level) {
+  const fallback = process.env.NODE_ENV === 'production' ? 'info' : 'debug';
+  const candidate = (level || process.env.LOG_LEVEL || fallback).toLowerCase();
+  return LEVEL_INDEX[candidate] !== undefined ? candidate : fallback;
+}
+
+function createLogger(options = {}) {
+  const levelName = resolveLevelName(options.level);
+  const threshold = LEVEL_INDEX[levelName];
+  const baseFields = {
+    service: options.name || 'mega-directory-api',
+    hostname: os.hostname(),
+    ...(options.baseFields || {})
+  };
+
+  function shouldLog(targetLevel) {
+    return LEVEL_INDEX[targetLevel] <= threshold;
+  }
+
+  function write(targetLevel, data, message) {
+    const entry = {
+      level: targetLevel,
+      time: new Date().toISOString(),
+      msg: message || '',
+      pid: process.pid,
+      ...baseFields,
+      ...data
+    };
+    process.stdout.write(`${JSON.stringify(entry)}\n`);
+  }
+
+  function buildMethod(targetLevel) {
+    return (arg1, arg2) => {
+      if (!shouldLog(targetLevel)) {
+        return;
+      }
+      let data = {};
+      let message = '';
+      if (typeof arg1 === 'string') {
+        message = arg1;
+        if (arg2 && typeof arg2 === 'object') {
+          data = arg2;
+        }
+      } else if (arg1 && typeof arg1 === 'object') {
+        data = arg1;
+        if (typeof arg2 === 'string') {
+          message = arg2;
+        }
+      } else if (arg1 !== undefined) {
+        message = String(arg1);
+      }
+      write(targetLevel, data, message);
+    };
+  }
+
+  const logger = {
+    level: levelName,
+    info: buildMethod('info'),
+    warn: buildMethod('warn'),
+    error: buildMethod('error'),
+    debug: buildMethod('debug'),
+    child(bindings = {}) {
+      return createLogger({
+        level: levelName,
+        name: baseFields.service,
+        baseFields: { ...baseFields, ...bindings }
+      });
+    }
+  };
+
+  return logger;
+}
+
+function createRequestLogger(logger) {
+  if (!logger || typeof logger.info !== 'function') {
+    return (_req, _res, next) => next();
+  }
+  return (req, res, next) => {
+    const start = process.hrtime();
+    res.on('finish', () => {
+      const diff = process.hrtime(start);
+      const durationMs = diff[0] * 1e3 + diff[1] / 1e6;
+      logger.info(
+        {
+          event: 'http.request',
+          method: req.method,
+          path: req.originalUrl || req.url,
+          statusCode: res.statusCode,
+          durationMs: Number(durationMs.toFixed(3))
+        },
+        'Handled request'
+      );
+    });
+    next();
+  };
+}
+
+module.exports = {
+  createLogger,
+  createRequestLogger
+};
diff --git a/api/test/server.test.js b/api/test/server.test.js
index bf8b58af8f5d8cd3897441feee6d9a1a9138bec7..56b643c361125a9d8094f6b1357f48ea92ddda01
--- a/api/test/server.test.js
+++ b/api/test/server.test.js
@@ -90,13 +90,36 @@
   return res;
 }
 
-test('health endpoint reports ok', () => {
-  const app = createServer(baseConfig);
+function createStubLogger() {
+  const calls = [];
+  const logger = {
+    calls,
+    info(...args) {
+      calls.push({ level: 'info', args });
+    },
+    warn(...args) {
+      calls.push({ level: 'warn', args });
+    },
+    error(...args) {
+      calls.push({ level: 'error', args });
+    },
+    child() {
+      return logger;
+    }
+  };
+  return logger;
+}
+
+test('health endpoint reports monitoring metadata', () => {
+  const app = createServer({ ...baseConfig, logger: createStubLogger() });
   const route = findRoute(app, 'get', '/health');
   const res = runRoute(route, createRequest(), createResponse());
 
   assert.strictEqual(res.statusCode, 200);
-  assert.deepStrictEqual(res.body, { status: 'ok' });
+  assert.strictEqual(res.body.status, 'ok');
+  assert.ok(typeof res.body.uptime === 'number');
+  assert.ok(res.body.timestamp);
+  assert.ok(res.body.startedAt);
 });
 
 test('admin ping rejects missing token', () => {
@@ -290,6 +313,26 @@
   assert.strictEqual(records[1].categorySlug, 'photographers');
 });
 
+test('crawler listing ingestion logs batch summary', () => {
+  const logger = createStubLogger();
+  const app = createServer({ ...baseConfig, logger });
+  const route = findRoute(app, 'post', '/v1/crawler/listings');
+  const req = createRequest({
+    method: 'POST',
+    headers: { Authorization: `Bearer ${baseConfig.crawlerBearerToken}` },
+    body: buildListingPayload({ slug: null })
+  });
+
+  runRoute(route, req, createResponse());
+
+  const infoLog = logger.calls.find((entry) => entry.level === 'info');
+  assert.ok(infoLog, 'expected ingestion handler to log summary');
+  const [metadata] = infoLog.args;
+  assert.strictEqual(metadata.event, 'listings.ingested');
+  assert.strictEqual(metadata.ingestedCount, 1);
+  assert.deepStrictEqual(metadata.categories, ['electricians']);
+});
+
 test('crawler listing ingestion validates payload shape and reports errors', () => {
   const app = createServer(baseConfig);
   const route = findRoute(app, 'post', '/v1/crawler/listings');
diff --git a/docs/deployment/railway.md b/docs/deployment/railway.md
index 9f3c9259f65c4fff62acda26d9e86d395c716f9d..0836fb33c48a2d0e2374e65e620132690a5525ac
--- a/docs/deployment/railway.md
+++ b/docs/deployment/railway.md
@@ -52,6 +52,7 @@
 | `ADMIN_LOGIN_PASSCODE` | Passcode paired with the admin email. |
 | `CRAWLER_BEARER_TOKEN` | Token shared with the crawler agent for ingestion endpoints. |
 | `ADMIN_TOKEN_TTL_SECONDS` | Optional TTL override for admin tokens (defaults to 900). |
+| `LOG_LEVEL` | Optional log verbosity for the API (`info` recommended in production). |
 
 ### Deploying via CLI
 
@@ -107,5 +108,6 @@
 - Use the **Metrics** tab on each service to watch container CPU/memory after deployment.
 - The API service exposes `GET /health`; add a Railway Healthcheck or external monitor (UptimeRobot, BetterStack) against that endpoint.
 - Tail logs with `railway logs --service api` or via the dashboard if something fails during boot.
+- Refer to `docs/monitoring.md` for a quick checklist that pairs the `/health` endpoint with UptimeRobot and shows how to trace crawler logs.
 
 With these steps complete, the Mega Directory API, frontend, and database all run on Railway using the Dockerfiles checked into this repository.
diff --git a/docs/monitoring.md b/docs/monitoring.md
new file mode 100644
index 0000000000000000000000000000000000000000..66b3b952047d2916389c07ba0db458dee177f24d
--- /dev/null
+++ b/docs/monitoring.md
@@ -0,0 +1,31 @@
+# Monitoring & Logging Guide
+
+This document summarizes how to observe the Mega Directory platform across the API service and the external crawler agent.
+
+## API Service
+
+- **Structured logs:** The Express API now emits JSON logs to stdout. When running on Railway or Docker, those logs automatically stream to the platform log viewer. Adjust verbosity with the `LOG_LEVEL` environment variable (`info` is recommended in production, `debug` is useful locally).
+- **Request tracing:** Every HTTP request is logged with its method, path, status code, and latency so you can correlate spikes or failures with specific routes.
+- **Health endpoint:** `GET /health` returns uptime metadata (`status`, `startedAt`, `timestamp`, `uptime`, and `environment`). Attach any external monitor to this endpoint.
+- **Railway hook:** Add a Railway Healthcheck that pings `/health` and configure alerts through Railway's dashboard. You can also tail logs with `railway logs --service api`.
+
+### UptimeRobot example
+
+1. Create a new *HTTP(s)* monitor and point it at `https://<your-api-domain>/health`.
+2. Set the interval (e.g., 5 minutes) and choose contacts for alerts.
+3. Optionally set keyword checking for `"status":"ok"` to ensure JSON parsing succeeds.
+
+Because the endpoint responds quickly and does not hit the database yet, the monitor can be aggressive without load concerns.
+
+## Crawler Agent
+
+- The crawler uses Python's standard `logging` module. It defaults to `DEBUG` locally and `INFO` in production unless you override `CRAWLER_LOG_LEVEL`.
+- Each ingestion batch logs the category, location, and API endpoint before it is POSTed so you can see progress in long-running jobs.
+- When running on a long-lived box (e.g., Railway worker, EC2 instance, or a cron host), redirect stdout/stderr to your preferred log collector or use `tmux`/`systemd`'s journaling to retain the output.
+- Run `CRAWLER_LOG_LEVEL=debug python main.py` while testing new selectors so you see each target/keyword combination as it executes.
+
+## Quick checklist
+
+- [ ] Configure `LOG_LEVEL=info` (API) and `CRAWLER_LOG_LEVEL=info` (crawler) in production.
+- [ ] Add a Railway Healthcheck and an external UptimeRobot monitor targeting `/health`.
+- [ ] Wire your alerting platform (Railway, UptimeRobot, PagerDuty) to the monitors so failures reach the on-call developer.

tokens used
204,563
Implemented structured logging plus richer health metadata for the API and crawler, along with documentation on wiring monitors and new tests to lock it all down.

**Updates**
- API now boots with a shared logger, request middleware, and `/health` telemetry (`api/index.js:62-300`). Batch ingestion warns on invalid payloads and emits summary logs so external monitors/UptimeRobot have context, while server startup logs go through the same channel.
- Added a lightweight JSON logger + request-duration middleware that default to `LOG_LEVEL` but can be overridden or swapped for stubs in tests (`api/logger.js:1-112`).
- Extended the API test suite with a stub logger to assert health metadata and ingestion logging behavior (`api/test/server.test.js:93-360`).
- Crawler instances accept an optional logger, default to a configured Python `logging` instance, and log run/batch progress before POSTing (`agents/crawler/crawler.py:255-507,640-645`). Tests cover the new hooks via a `RecordingLogger` (`agents/crawler/tests/test_crawler.py:63-359`).
- Documented how to use the new knobs: README logging section (`README.md:46-51`), Railway env table (`docs/deployment/railway.md:35-111`), and a fresh monitoring runbook that calls out UptimeRobot + Railway log tips (`docs/monitoring.md:1-31`).

**Tests**
- `npm test` (from `api/`)
- `pytest` (from `agents/crawler/`)

Next up you may want to extend `/health` with database connectivity once the persistence layer lands so monitors can catch DB outages too.
