OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/mega-directory
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a727c-22d3-7560-b0ad-f47ecba84720
--------
user
Implement the functionality described in codex/TASKS/11_llm_field_generation_via_jinja2.yaml, then verify with any available tests.

thinking
**Preparing to inspect task instructions**
exec
bash -lc ls in /var/home/rick/Development/mega-directory succeeded in 39ms:
admin
agents
AGENTS.md
api
astro
codex
codex.toml
codex.yaml
db
docker-compose.yml
docs
README.md

thinking
**Inspecting task file content**
exec
bash -lc "sed -n '1,200p' codex/TASKS/11_llm_field_generation_via_jinja2.yaml" in /var/home/rick/Development/mega-directory succeeded in 38ms:
id: 11
title: LLM Field Generation via Jinja2
description: Add dynamic field generation with provider/model config, jinja2 templates,
  and token substitution.
dependencies:
- 10

thinking
**Seeking additional context**
exec
bash -lc 'ls codex/TASKS' in /var/home/rick/Development/mega-directory succeeded in 44ms:
00_prepare_repository.yaml
01_initialize_astro_frontend.yaml
02_setup_api_server_with_express_and_jwt_auth.yaml
03_design_postgresql_schema.yaml
04_implement_listing_ingestion_api_endpoint.yaml
05_admin_auth_route_and_middleware.yaml
06_frontend_directory_page_rendering.yaml
07_frontend_subcategory_ui_and_layout.yaml
08_setup_featured_listings_logic.yaml
09_add_category_meta_and_seo_controls.yaml
10_scaffold_python_crawler_agent.yaml
11_llm_field_generation_via_jinja2.yaml
12_post_listings_from_crawler_to_api_server.yaml
13_create_json_crawler_configuration_schema.yaml
14_scaffold_admin_ui_application.yaml
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml

thinking
**Investigating task details and requirements**
exec
bash -lc 'ls agents' in /var/home/rick/Development/mega-directory succeeded in 41ms:
crawler

thinking
**Inspecting crawler code list**
exec
bash -lc 'ls agents/crawler' in /var/home/rick/Development/mega-directory succeeded in 38ms:
config
crawler.py
main.py
__pycache__
targets.example.json
tests

thinking
**Preparing to explore LLM field generation**
exec
bash -lc "sed -n '1,200p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 38ms:
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

        def __init__(self, *args, **kwargs):
            super().__init__()
            self._cache: Dict[Any, requests.Response] = {}

thinking
**Investigating crawler configuration**
exec
bash -lc 'ls agents/crawler/config' in /var/home/rick/Development/mega-directory succeeded in 38ms:
targets.json

thinking
**Considering next steps**
exec
bash -lc 'cat agents/crawler/config/targets.json' in /var/home/rick/Development/mega-directory succeeded in 38ms:
{
  "api_endpoint": "http://localhost:3000/api/new-listings",
  "api_token": "YOUR_API_TOKEN_HERE",
  "targets": [
    {
      "category": "Electricians",
      "locations": [
        "Holland MI"
      ],
      "subdomain": "electricians.example.com",
      "keywords": [
        "licensed",
        "residential",
        "commercial"
      ],
      "listings_per_location": 50,
      "fields": {
        "title": {
          "source": "scrape"
        },

thinking
**Planning field generation with Jinja**
exec
bash -lc 'ls agents/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 38ms:
__pycache__
test_crawler.py

thinking
**Checking tests for expectations**
exec
bash -lc "sed -n '1,200p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 48ms:
from pathlib import Path
from types import SimpleNamespace
import sys

import pytest

AGENT_DIR = Path(__file__).resolve().parents[1]
if str(AGENT_DIR) not in sys.path:
    sys.path.insert(0, str(AGENT_DIR))

import crawler
from crawler import run_crawler


SAMPLE_HTML = """
<section>
  <article data-listing>
    <a class="listing-link" href="https://example.com/acme">
      <span class="listing-title">ACME Electric</span>
    </a>

thinking
**Continuing file review**
exec
bash -lc "sed -n '200,400p' agents/crawler/tests/test_crawler.py" in /var/home/rick/Development/mega-directory succeeded in 53ms:


thinking
**Checking for related functions**
exec
bash -lc 'rg -n "run_crawler" -n -g"*.py"' in /var/home/rick/Development/mega-directory succeeded in 127ms:
agents/crawler/main.py:2:from crawler import run_crawler
agents/crawler/main.py:7:    run_crawler(config)
agents/crawler/tests/test_crawler.py:12:from crawler import run_crawler
agents/crawler/tests/test_crawler.py:51:def test_run_crawler_batches_locations_and_limits_results() -> None:
agents/crawler/tests/test_crawler.py:67:    batches = run_crawler(config, session=dummy_session)
agents/crawler/tests/test_crawler.py:111:def test_run_crawler_uses_cached_session(monkeypatch: pytest.MonkeyPatch) -> None:
agents/crawler/tests/test_crawler.py:140:    batches = run_crawler(config)
agents/crawler/crawler.py:215:def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
exec
bash -lc "sed -n '200,320p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 55ms:
        return f"https://{value.strip('/')}"

    @staticmethod
    def _normalize_url(
        href: str,
        subdomain: Optional[str],
    ) -> str:
        if not href:
            return ""
        if href.startswith("http://") or href.startswith("https://"):
            return href
        base = Crawler._normalize_subdomain(subdomain) or "https://"
        return urljoin(base + "/", href.lstrip("/"))


def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
    crawler = Crawler(session=session)
    return crawler.run(config)

thinking
**Searching for jinja references**
exec
bash -lc 'rg -n "jinja" -n' in /var/home/rick/Development/mega-directory succeeded in 176ms:
AGENTS.md:15:* `requests`, `beautifulsoup4`, `requests-cache`, `jinja2`
codex.yaml:26:    path: codex/TASKS/11_llm_field_generation_via_jinja2.yaml
docs/AGENTS.md:15:* `requests`, `beautifulsoup4`, `requests-cache`, `jinja2`
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:13:Implement the functionality described in codex/TASKS/11_llm_field_generation_via_jinja2.yaml, then verify with any available tests.
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:35:bash -lc "sed -n '1,200p' codex/TASKS/11_llm_field_generation_via_jinja2.yaml" in /var/home/rick/Development/mega-directory succeeded in 38ms:
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:38:description: Add dynamic field generation with provider/model config, jinja2 templates,
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:58:11_llm_field_generation_via_jinja2.yaml
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:213:**Searching for jinja references**
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:215:bash -lc 'rg -n "jinja" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:124:codex/LOGS/codex_run_2025-11-11_04-26-54.log:7:Tasks: 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:127:codex/LOGS/codex_run_2025-11-11_04-14-04.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:128:codex/LOGS/codex_run_2025-11-11_04-18-20.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:285:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:124:codex/LOGS/codex_run_2025-11-11_04-26-54.log:7:Tasks: 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:288:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:127:codex/LOGS/codex_run_2025-11-11_04-14-04.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:289:codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:128:codex/LOGS/codex_run_2025-11-11_04-18-20.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:316:11_llm_field_generation_via_jinja2.yaml
codex/LOGS/codex_run_2025-11-11_04-26-54.log:7:Tasks: 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/codex_run_2025-11-11_04-26-54.log:65:ðŸŸ¡ Executing task: 11_llm_field_generation_via_jinja2
codex/LOGS/codex_run_2025-11-11_04-18-20.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application
codex/LOGS/codex_run_2025-11-11_04-14-04.log:7:Tasks: 03_design_postgresql_schema 04_implement_listing_ingestion_api_endpoint 05_admin_auth_route_and_middleware 23_docker_compose_local_dev 06_frontend_directory_page_rendering 07_frontend_subcategory_ui_and_layout 08_setup_featured_listings_logic 09_add_category_meta_and_seo_controls 10_scaffold_python_crawler_agent 11_llm_field_generation_via_jinja2 12_post_listings_from_crawler_to_api_server 13_create_json_crawler_configuration_schema 14_scaffold_admin_ui_application

thinking
**Searching for prompt_template references**
exec
bash -lc 'rg -n "prompt_template" -n' in /var/home/rick/Development/mega-directory succeeded in 57ms:
agents/crawler/targets.example.json:18:          "prompt_template": "Write a friendly listing description for a {{ category }} in {{ location }}. Details: {{ details }}."
agents/crawler/config/targets.json:25:          "prompt_template": "Describe {{ listing.link_text }} located in {{ location }}."
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:238:**Searching for prompt_template references**
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:240:bash -lc 'rg -n "prompt_template" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:99:          "prompt_template": "Write a friendly listing description for a {{ category }} in {{ location }}. Details: {{ details }}."
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:408:          "prompt_template": "Write a friendly listing description for a {{ category }} in {{ location }}. Details: {{ details }}."

thinking
**Checking example config file**
exec
bash -lc 'cat agents/crawler/targets.example.json' in /var/home/rick/Development/mega-directory succeeded in 42ms:
{
  "api_endpoint": "http://localhost:3001/api/new-listings",
  "api_token": "changeme",
  "targets": [
    {
      "category": "Real Estate",
      "locations": ["New York", "San Francisco"],
      "subdomain": "realestate.local",
      "keywords": ["ocean view", "downtown"],
      "listings_per_location": 50,
      "fields": {
        "title": { "source": "scrape" },
        "price": { "source": "scrape" },
        "description": {
          "source": "ai",
          "provider": "openai",
          "model": "gpt-4",
          "prompt_template": "Write a friendly listing description for a {{ category }} in {{ location }}. Details: {{ details }}."
        }
      }

thinking
**Investigating crawler entry points**
exec
bash -lc "sed -n '1,200p' agents/crawler/main.py" in /var/home/rick/Development/mega-directory succeeded in 51ms:
import json
from crawler import run_crawler

if __name__ == "__main__":
    with open("config/targets.json", "r") as f:
        config = json.load(f)
    run_crawler(config)

thinking
**Checking agents crawler initialization**
exec
bash -lc 'rg -n "class LLM" -n' in /var/home/rick/Development/mega-directory succeeded in 60ms:
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:288:bash -lc 'rg -n "class LLM" -n' in /var/home/rick/Development/mega-directory

thinking
**Planning dynamic field generation implementation**
exec
bash -lc "rg -n \"\\\"fields\\\"\" -n agents/crawler -n" in /var/home/rick/Development/mega-directory succeeded in 63ms:
agents/crawler/targets.example.json:11:      "fields": {
agents/crawler/config/targets.json:17:      "fields": {

thinking
**Investigating unimplemented config features**
exec
bash -lc 'rg -n "source" agents/crawler -n' in /var/home/rick/Development/mega-directory succeeded in 59ms:
agents/crawler/targets.example.json:12:        "title": { "source": "scrape" },
agents/crawler/targets.example.json:13:        "price": { "source": "scrape" },
agents/crawler/targets.example.json:15:          "source": "ai",
agents/crawler/config/targets.json:19:          "source": "scrape"
agents/crawler/config/targets.json:22:          "source": "ai",

thinking
**Investigating task dependencies**
exec
bash -lc "sed -n '1,200p' codex/TASKS/12_post_listings_from_crawler_to_api_server.yaml" in /var/home/rick/Development/mega-directory succeeded in 38ms:
id: 12
title: POST Listings from Crawler to API Server
description: After field population, push data to API using HTTP and token. Validate
  integration against dev and prod targets.
dependencies:
- 4
- 10

thinking
**Opening task specification file**
exec
bash -lc "sed -n '1,200p' codex/TASKS/10_scaffold_python_crawler_agent.yaml" in /var/home/rick/Development/mega-directory succeeded in 38ms:
id: 10
title: Scaffold Python Crawler Agent
description: Build Python module using requests, BeautifulSoup4, and requests-cache.
  Read from config and support category-location batches.
dependencies: []

thinking
**Planning dynamic field generation**
exec
bash -lc 'rg -n "openai" -n' in /var/home/rick/Development/mega-directory succeeded in 43ms:
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:5:provider: openai
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:267:          "provider": "openai",
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:333:bash -lc 'rg -n "openai" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:5:provider: openai
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:97:          "provider": "openai",
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:233:provider: openai
codex/LOGS/tasklog_02_setup_api_server_with_express_and_jwt_auth_1762851022.log:5:provider: openai
codex/LOGS/tasklog_02_setup_api_server_with_express_and_jwt_auth_1762851022.log:137:provider: openai
agents/crawler/targets.example.json:16:          "provider": "openai",
codex/LOGS/tasklog_23_docker_compose_local_dev_1762853831.log:5:provider: openai
agents/crawler/config/targets.json:23:          "provider": "openai",
codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log:5:provider: openai
codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log:772:provider: openai
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:5:provider: openai
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:207:provider: openai
codex/LOGS/tasklog_05_admin_auth_route_and_middleware_1762853548.log:235:provider: openai
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:5:provider: openai
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:140:provider: openai
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log:406:          "provider": "openai",
codex/LOGS/tasklog_03_design_postgresql_schema_1762852701.log:5:provider: openai

thinking
**Designing dynamic field generation**
exec
bash -lc 'rg -n "Field Generation" -n' in /var/home/rick/Development/mega-directory succeeded in 47ms:
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:37:title: LLM Field Generation via Jinja2
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log:358:bash -lc 'rg -n "Field Generation" -n' in /var/home/rick/Development/mega-directory
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log:282:title: LLM Field Generation via Jinja2
codex/TASKS/11_llm_field_generation_via_jinja2.yaml:2:title: LLM Field Generation via Jinja2

thinking
**Checking documentation folder**
exec
bash -lc 'ls docs' in /var/home/rick/Development/mega-directory succeeded in 41ms:
AGENTS.md

thinking
**Designing dynamic LLM field generation**

I'm working out how to add per-field dynamic generation in the Crawler using config to specify sourceâ€”scrape or AIâ€”with integrating Jinja2 templates for prompt token substitution. Since network calls are restricted, I plan to build a stubbed LLM provider interface for flexible hooking and prepare tests upfront following TDD. This will let listingsâ€™ fields be generated dynamically by LLM providers as configured, while dealing with current environment limits.
exec
bash -lc "sed -n '1,200p' codex.yaml" in /var/home/rick/Development/mega-directory succeeded in 39ms:
version: 1
tasks:
  - id: '00'
    path: codex/TASKS/00_prepare_repository.yaml
  - id: '01'
    path: codex/TASKS/01_initialize_astro_frontend.yaml
  - id: '02'
    path: codex/TASKS/02_setup_api_server_with_express_and_jwt_auth.yaml
  - id: '03'
    path: codex/TASKS/03_design_postgresql_schema.yaml
  - id: '04'
    path: codex/TASKS/04_implement_listing_ingestion_api_endpoint.yaml
  - id: '05'
    path: codex/TASKS/05_admin_auth_route_&_middleware.yaml
  - id: '06'
    path: codex/TASKS/06_frontend_directory_page_rendering.yaml
  - id: '07'
    path: codex/TASKS/07_frontend_subcategory_ui_&_layout.yaml
  - id: '08'
    path: codex/TASKS/08_setup_featured_listings_logic.yaml

thinking
**Drafting multi-step plan for field generation**
exec
bash -lc 'rg -n "Listing" -n' in /var/home/rick/Development/mega-directory succeeded in 45ms:
api/test/server.test.js:41:function buildListingPayload(overrides = {}) {
api/test/server.test.js:228:    body: buildListingPayload()
api/test/server.test.js:242:    body: buildListingPayload({ slug: 'ACME Electric!!', summary: 'Trusted' })
api/test/server.test.js:269:      buildListingPayload({ title: 'Bright Builders', categorySlug: 'builders' }),
api/test/server.test.js:270:      buildListingPayload({ title: 'Northern Lights', categorySlug: 'photographers', slug: null })
api/test/server.test.js:298:      buildListingPayload({ title: '  ', categorySlug: 'electricians' }),
api/index.js:66:  app.locals.ingestionStore = createListingStore();
api/index.js:87:    createListingIngestionHandler(app)
api/index.js:212:function createListingIngestionHandler(app) {
api/index.js:214:    const payloads = normalizeListingBatch(req.body);
api/index.js:226:    const validations = payloads.map((payload) => validateListingPayload(payload));
api/index.js:255:function normalizeListingBatch(payload) {
api/index.js:268:function validateListingPayload(payload) {
api/index.js:322:function createListingStore() {
admin/views/review.ejs:4:  <title>Review Listings</title>
admin/views/review.ejs:7:  <h1>Pending Listings</h1>
astro/README.md:40:- `ListingItem.astro`: semantic `<article>` cards with WCAG-friendly focus states.
astro/test/directory-helpers.test.mjs:4:  sortListingsByScore,
astro/test/directory-helpers.test.mjs:10:  filterListingsBySubcategory,
astro/test/directory-helpers.test.mjs:12:  segmentFeaturedListings,

thinking
**Filtering search results for crawler**
exec
bash -lc 'rg -n "Listing" agents/crawler -n' in /var/home/rick/Development/mega-directory succeeded in 41ms:
agents/crawler/crawler.py:45:class Listing:
agents/crawler/crawler.py:57:    listings: List[Listing]
agents/crawler/crawler.py:112:    ) -> List[Listing]:
agents/crawler/crawler.py:131:    ) -> List[Listing]:
agents/crawler/crawler.py:135:        listings: List[Listing] = []
agents/crawler/crawler.py:151:                Listing(

thinking
**Investigating API ingestion integration**
exec
bash -lc "sed -n '200,340p' api/index.js" in /var/home/rick/Development/mega-directory succeeded in 39ms:
    console.log(`API server running at http://localhost:${port}`);
  });
}

if (require.main === module) {
  startServer();
}

module.exports = {
  createServer
};

function createListingIngestionHandler(app) {
  return (req, res) => {
    const payloads = normalizeListingBatch(req.body);
    if (!payloads) {
      return res
        .status(400)
        .json({ error: 'Invalid listing payload', details: ['Request body must be an object or array'] });
    }

thinking
**Designing dynamic field generation system**

I'm planning a minimal but flexible architecture to implement dynamic field generation using provider/model configs, Jinja2 templating, and token substitution. I'll extend the Listing dataclass with a fields attribute for generated data, design a FieldGenerator to produce those fields per listing, and build an LLMService interface to eventually integrate real or mock LLM calls while keeping testing and graceful degradation in mind. Let's explore all necessary pieces and dependencies carefully!
exec
bash -lc "sed -n '1,200p' codex/TASKS/work_list" in /var/home/rick/Development/mega-directory succeeded in 90ms:
04_implement_listing_ingestion_api_endpoint.yaml
05_admin_auth_route_and_middleware.yaml
23_docker_compose_local_dev.yaml
06_frontend_directory_page_rendering.yaml
07_frontend_subcategory_ui_and_layout.yaml
08_setup_featured_listings_logic.yaml
09_add_category_meta_and_seo_controls.yaml
10_scaffold_python_crawler_agent.yaml
11_llm_field_generation_via_jinja2.yaml
12_post_listings_from_crawler_to_api_server.yaml
13_create_json_crawler_configuration_schema.yaml
14_scaffold_admin_ui_application.yaml
15_admin_ui:_listing_review_table.yaml
16_admin_ui:_category_location_approval.yaml
17_admin_ui:_seo_field_editor.yaml
18_deploy_web_+_api_to_railway.yaml
19_setup_logging,_healthchecks_and_monitoring.yaml
20_verify_dev_environment_bootstraps_cleanly.yaml
21_dockerize_api_server.yaml
22_dockerize_astro_frontend.yaml

thinking
**Designing dynamic field generation with LLM integration**

I'm brainstorming how to add flexible field generation to the crawler, mixing scraped listing data with AI-generated fields via LLM calls. Iâ€™m planning an interface for injecting an LLM client that can render fields based on Jinja2 templates with token substitution from listing and context info. Iâ€™m balancing fallback behavior for scraped fields and enabling AI prompts for others, plus figuring out dependency injection so testing works without network calls. Also considering how to manage dependencies like Jinja2 and where to place this logic in the codebase.
exec
bash -lc 'rg -l "requests" --files' in /var/home/rick/Development/mega-directory exited 2 in 39ms:
rg: requests: IO error for operation on requests: No such file or directory (os error 2)

thinking
**Correcting ripgrep usage**
exec
bash -lc 'rg -l "requests"' in /var/home/rick/Development/mega-directory succeeded in 46ms:
AGENTS.md
docs/AGENTS.md
astro/test/directory-helpers.test.mjs
codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log
codex/TASKS/CODEX_TODO.md
codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log
agents/crawler/tests/test_crawler.py
codex/TASKS/10_scaffold_python_crawler_agent.yaml
codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log
agents/crawler/crawler.py
codex/LOGS/tasklog_04_implement_listing_ingestion_api_endpoint_1762853214.log
codex/LOGS/tasklog_02_setup_api_server_with_express_and_jwt_auth_1762851022.log

thinking
**Searching for packaging files**
exec
bash -lc "rg --files -g 'pyproject.toml'" in /var/home/rick/Development/mega-directory exited 1 in 43ms:


thinking
**Checking for requirements files**
exec
bash -lc "rg --files -g 'requirements*.txt'" in /var/home/rick/Development/mega-directory exited 1 in 43ms:


thinking
**Planning FieldGenerator integration**
exec
bash -lc 'rg -n "token"' in /var/home/rick/Development/mega-directory succeeded in 49ms:
AGENTS.md:39:* Authentication (token-based or admin login)
AGENTS.md:83:* JWT and API token auth
docs/AGENTS.md:39:* Authentication (token-based or admin login)
docs/AGENTS.md:83:* JWT and API token auth
api/package-lock.json:12:        "jsonwebtoken": "^9.0.2"
api/package-lock.json:456:    "node_modules/jsonwebtoken": {
api/package-lock.json:458:      "resolved": "https://registry.npmjs.org/jsonwebtoken/-/jsonwebtoken-9.0.2.tgz",
api/package-lock.json:478:    "node_modules/jsonwebtoken/node_modules/ms": {
api/test/server.test.js:3:const jwt = require('jsonwebtoken');
api/test/server.test.js:12:  crawlerBearerToken: 'crawler-token'
api/test/server.test.js:102:test('admin ping rejects missing token', () => {
api/test/server.test.js:108:  assert.strictEqual(res.body.error, 'Admin token missing or invalid');
api/test/server.test.js:114:  const token = jwt.sign(
api/test/server.test.js:121:    headers: { Authorization: `Bearer ${token}` }
api/test/server.test.js:175:  assert.strictEqual(typeof loginRes.body.token, 'string');
api/test/server.test.js:176:  assert.strictEqual(loginRes.body.tokenType, 'Bearer');
api/test/server.test.js:179:  const decoded = jwt.verify(loginRes.body.token, baseConfig.adminJwtSecret, {
api/test/server.test.js:189:    createRequest({ headers: { Authorization: `Bearer ${loginRes.body.token}` } }),
api/test/server.test.js:197:test('crawler ping rejects invalid bearer token', () => {
api/test/server.test.js:202:    createRequest({ method: 'POST', headers: { Authorization: 'Bearer wrong-token' } }),


thinking
**Designing token substitution and field generation**
thinking
**Specifying context construction and field generation details**

I'll clarify how the field generator builds listing and token contexts, implements AI field generation with jinja templating, and integrates with the crawler setup. This includes handling missing LLM clients by raising errors and test strategies to verify template rendering, token substitution, and error cases. I'm also considering seamless hooking of the LLM client into the crawler's lifecycle for dynamic field computation.
Plan update
Will add jinja/LLM coverage via tests before implementing code, then wire crawler and validate with pytest.
  â†’ Study crawler config/tests to infer LLM field requirements
  â€¢ Add failing tests covering jinja-based AI field generation and token context
  â€¢ Implement FieldGenerator + crawler wiring to satisfy tests
  â€¢ Run crawler test suite and verify

thinking
**Updating plan status**
Plan update
  âœ“ Study crawler config/tests to infer LLM field requirements
  â†’ Add failing tests covering jinja-based AI field generation and token context
  â€¢ Implement FieldGenerator + crawler wiring to satisfy tests
  â€¢ Run crawler test suite and verify

thinking
**Designing and planning failing tests**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/tests/test_crawler.py
@@ -143 +143,65 @@
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M agents/crawler/tests/test_crawler.py
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))

file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Verifying list mutation and updating plan**
Plan update
  âœ“ Study crawler config/tests to infer LLM field requirements
  âœ“ Add failing tests covering jinja-based AI field generation and token context
  â†’ Implement FieldGenerator + crawler wiring to satisfy tests
  â€¢ Run crawler test suite and verify
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Implementing advanced field generation**

I'm preparing to enhance the crawler by adding a FieldGenerator class that integrates AI and scraping-based field extraction, updating the Listing dataclass to hold generated fields, and enabling the crawler to lazily instantiate and use this generator during listing parsing. This involves careful context building for field templates, safe template rendering with jinja2, and defining an LLMRequest dataclass to standardize AI calls. I'll update imports, constructor signatures, and methods to handle these new capabilities while preserving existing functionality.
exec
bash -lc "sed -n '1,200p' agents/crawler/crawler.py" in /var/home/rick/Development/mega-directory succeeded in 41ms:
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, Iterable, List, Optional
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup

try:
    import requests_cache
except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    from types import SimpleNamespace

    class _ShimCachedSession(requests.Session):
        """Minimal drop-in replacement when requests-cache is unavailable."""

        def __init__(self, *args, **kwargs):
            super().__init__()
            self._cache: Dict[Any, requests.Response] = {}
file update:
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Planning code restructuring and additions**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -3,3 +3,3 @@
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
@@ -8,2 +8,3 @@
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
@@ -49,2 +50,3 @@
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
@@ -69,2 +71,172 @@
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..dfa60761b8e00f8a582e5ce87e1ab58885d40b15
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..dfa60761b8e00f8a582e5ce87e1ab58885d40b15
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Enhancing Crawler to integrate FieldGenerator**
file update
M /var/home/rick/Development/mega-directory/agents/crawler/crawler.py
@@ -248,2 +248,3 @@
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
@@ -255,2 +256,3 @@
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
@@ -264,2 +266,4 @@
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
@@ -316,2 +320,4 @@
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
@@ -326,3 +332,7 @@
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
@@ -332,2 +342,13 @@
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
@@ -386,4 +407,8 @@
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M agents/crawler/crawler.py
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))

file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Confirming FieldGenerator behavior and dependencies**
exec
bash -lc "python - <<'PY'
import jinja2
print(jinja2.__version__)
PY" in /var/home/rick/Development/mega-directory succeeded in 216ms:
3.1.6
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Preparing to run crawler tests**
exec
bash -lc 'pytest agents/crawler/tests' in /var/home/rick/Development/mega-directory succeeded in 1.79s:
============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/mega-directory
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 5 items

agents/crawler/tests/test_crawler.py .....                               [100%]

============================== 5 passed in 0.90s ===============================
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Updating plan statuses**
Plan update
  âœ“ Study crawler config/tests to infer LLM field requirements
  âœ“ Add failing tests covering jinja-based AI field generation and token context
  âœ“ Implement FieldGenerator + crawler wiring to satisfy tests
  âœ“ Run crawler test suite and verify
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Checking git status before finalizing**
exec
bash -lc 'git status -sb' in /var/home/rick/Development/mega-directory succeeded in 82ms:
## main...origin/main [ahead 2]
 M agents/crawler/crawler.py
 M codex/LOGS/codex_run_2025-11-11_04-26-54.log
 M codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log
?? agents/crawler/tests/
?? astro/src/components/DirectoryListing.astro
?? astro/src/components/DirectoryPage.astro
?? astro/src/data/
?? astro/src/lib/
?? astro/src/pages/listings/
?? astro/test/
?? codex/LOGS/tasklog_08_setup_featured_listings_logic_1762855542.log.done
?? codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log
?? codex/LOGS/tasklog_09_add_category_meta_and_seo_controls_1762856433.log.done
?? codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log
?? codex/LOGS/tasklog_10_scaffold_python_crawler_agent_1762856938.log.done
?? codex/LOGS/tasklog_11_llm_field_generation_via_jinja2_1762857329.log
?? codex/TASKS/work_list
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Investigating untracked test directory**
exec
bash -lc 'git status agents/crawler/tests/test_crawler.py' in /var/home/rick/Development/mega-directory succeeded in 74ms:
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	agents/crawler/tests/test_crawler.py

nothing added to commit but untracked files present (use "git add" to track)
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Clarifying file tracking and preparing line references**
exec
bash -lc "nl -ba agents/crawler/crawler.py | sed -n '1,260p'" in /var/home/rick/Development/mega-directory succeeded in 41ms:
     1	from __future__ import annotations
     2	
     3	from dataclasses import dataclass, field
     4	from typing import Any, Callable, Dict, Iterable, List, Optional
     5	from urllib.parse import quote_plus, urljoin
     6	
     7	import requests
     8	from bs4 import BeautifulSoup
     9	from jinja2 import Environment, StrictUndefined, TemplateError
    10	
    11	try:
    12	    import requests_cache
    13	except ModuleNotFoundError:  # pragma: no cover - shim is only needed locally
    14	    from types import SimpleNamespace
    15	
    16	    class _ShimCachedSession(requests.Session):
    17	        """Minimal drop-in replacement when requests-cache is unavailable."""
    18	
    19	        def __init__(self, *args, **kwargs):
    20	            super().__init__()
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Extending code view for parse_listings**
exec
bash -lc "nl -ba agents/crawler/crawler.py | sed -n '260,520p'" in /var/home/rick/Development/mega-directory succeeded in 52ms:
   260	        batches: List[CrawlerBatch] = []
   261	        for target in config.get("targets", []):
   262	            for location_batch in self._iter_location_batches(target):
   263	                listings = self.fetch_listings(target, location_batch)
   264	                limit = target.get("listings_per_location")
   265	                if isinstance(limit, int) and limit > 0:
   266	                    listings = listings[:limit]
   267	                if target.get("fields"):
   268	                    self._populate_fields(listings, target, location_batch)
   269	                meta = {
   270	                    "search_url": location_batch["search_url"],
   271	                    "keyword": location_batch.get("keyword"),
   272	                }
   273	                batches.append(
   274	                    CrawlerBatch(
   275	                        category=target.get("category", ""),
   276	                        location=location_batch["location"],
   277	                        subdomain=target.get("subdomain"),
   278	                        listings=listings,
   279	                        metadata=meta,
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Preparing references for summary**
exec
bash -lc 'nl -ba agents/crawler/tests/test_crawler.py' in /var/home/rick/Development/mega-directory succeeded in 54ms:
     1	from pathlib import Path
     2	from types import SimpleNamespace
     3	import sys
     4	
     5	import pytest
     6	
     7	AGENT_DIR = Path(__file__).resolve().parents[1]
     8	if str(AGENT_DIR) not in sys.path:
     9	    sys.path.insert(0, str(AGENT_DIR))
    10	
    11	import crawler
    12	from crawler import run_crawler
    13	
    14	
    15	SAMPLE_HTML = """
    16	<section>
    17	  <article data-listing>
    18	    <a class="listing-link" href="https://example.com/acme">
    19	      <span class="listing-title">ACME Electric</span>
    20	    </a>
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))


thinking
**Summarizing crawler code changes and tests**

I'm putting together a final structured summary highlighting the new `FieldGenerator` and `LLMRequest` additions with Jinja templating and token context, updated `Listing` with fields and extras, changes in `Crawler.run` to populate fields and require an `llm_client`, and the coverage from new tests in `agents/crawler/tests` including error handling. I'll include line references and the pytest command run, plus suggest logical next steps for hooking a real LLM client or extending serialization as per instructions.
codex
**Crawler Updates**
- Added `LLMRequest` and a reusable `FieldGenerator` that renders StrictUndefined Jinja2 templates with a rich token map, validates provider/model config, and funnels requests through an injected `llm_client`, so each field can be sourced from scrape data or AI output (`agents/crawler/crawler.py:72`, `agents/crawler/crawler.py:86`).
- Listings now expose a `fields` dict alongside scraped metadata and capture each anchorâ€™s link text inside `extras`, giving prompt templates reliable tokens such as `listing.link_text` (`agents/crawler/crawler.py:45`, `agents/crawler/crawler.py:318`).
- When a target defines `fields`, the crawler now populates them per listing via `_populate_fields`, and `run_crawler` accepts an optional `llm_client`, raising a clear runtime error when AI fields are configured without one (`agents/crawler/crawler.py:260`, `agents/crawler/crawler.py:343`, `agents/crawler/crawler.py:408`).

**Tests**
- Expanded coverage with scenarios that assert Jinja-driven AI fields, scrape-backed attributes, and the failure path when no `llm_client` is supplied (`agents/crawler/tests/test_crawler.py:146`, `agents/crawler/tests/test_crawler.py:186`).
- `pytest agents/crawler/tests`

**Next Steps**
- 1) Wire a concrete `llm_client` for OpenAI/OpenRouter/Gemini that consumes `LLMRequest` so real generations can run outside tests.
- 2) Decide how the new `listing.fields` payload should be transformed when POSTing to the API (e.g., map to `summary`, `websiteUrl`, etc.) and extend the serialization accordingly.
file update:
diff --git a/agents/crawler/crawler.py b/agents/crawler/crawler.py
index 1dd992dbbb8a1f2c44a434014566e39606f26761..5ab36db4e9d6040381bf7ae5fa3c224234e2db36
--- a/agents/crawler/crawler.py
+++ b/agents/crawler/crawler.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Callable, Dict, Iterable, List, Optional
 from urllib.parse import quote_plus, urljoin
 
 import requests
 from bs4 import BeautifulSoup
+from jinja2 import Environment, StrictUndefined, TemplateError
 
 try:
     import requests_cache
@@ -47,6 +48,7 @@
     url: str
     snippet: str
     extras: Dict[str, Any] = field(default_factory=dict)
+    fields: Dict[str, Any] = field(default_factory=dict)
 
 
 @dataclass
@@ -67,6 +69,176 @@
         }
 
 
+@dataclass
+class LLMRequest:
+    provider: str
+    model: str
+    prompt: str
+    field_name: str
+    options: Dict[str, Any] = field(default_factory=dict)
+    target: Optional[Dict[str, Any]] = None
+    listing: Optional[Dict[str, Any]] = None
+
+
+LLMClient = Callable[[LLMRequest], str]
+
+
+class FieldGenerator:
+    def __init__(self, llm_client: Optional[LLMClient] = None) -> None:
+        self.llm_client = llm_client
+        self._env = Environment(
+            autoescape=False,
+            undefined=StrictUndefined,
+            trim_blocks=True,
+            lstrip_blocks=True,
+        )
+
+    def generate(self, listing: Listing, target: Dict[str, Any], batch: Dict[str, Any]) -> Dict[str, Any]:
+        field_configs = target.get("fields") or {}
+        if not field_configs:
+            return {}
+
+        context = self._build_context(listing, target, batch)
+        generated: Dict[str, Any] = {}
+
+        for field_name, config in field_configs.items():
+            source = (config.get("source") or "scrape").lower()
+            if source == "scrape":
+                generated[field_name] = self._resolve_scrape_field(field_name, config, context)
+            elif source == "ai":
+                generated[field_name] = self._generate_ai_field(field_name, config, context, target)
+            else:
+                raise ValueError(f"Unsupported field source '{source}' for '{field_name}'")
+
+        return generated
+
+    def _build_context(
+        self,
+        listing: Listing,
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        listing_context: Dict[str, Any] = {
+            "title": listing.title,
+            "url": listing.url,
+            "snippet": listing.snippet,
+            "extras": dict(listing.extras),
+        }
+        for key, value in listing.extras.items():
+            if key not in listing_context:
+                listing_context[key] = value
+
+        batch_context = dict(batch)
+        base_context: Dict[str, Any] = {
+            "listing": listing_context,
+            "category": target.get("category"),
+            "location": batch_context.get("location"),
+            "keyword": batch_context.get("keyword"),
+            "subdomain": target.get("subdomain"),
+            "target": target,
+            "batch": batch_context,
+        }
+        base_context["tokens"] = self._build_tokens(listing_context, base_context, batch_context)
+        return base_context
+
+    @staticmethod
+    def _build_tokens(
+        listing_context: Dict[str, Any],
+        base_context: Dict[str, Any],
+        batch_context: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        tokens: Dict[str, Any] = {
+            "listing_title": listing_context.get("title", ""),
+            "listing_url": listing_context.get("url", ""),
+            "listing_snippet": listing_context.get("snippet", ""),
+            "category": base_context.get("category") or "",
+            "location": batch_context.get("location") or "",
+            "keyword": batch_context.get("keyword") or "",
+            "subdomain": base_context.get("subdomain") or "",
+        }
+        for key, value in listing_context.items():
+            if isinstance(value, str):
+                tokens[key] = value
+        return tokens
+
+    def _resolve_scrape_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+    ) -> Any:
+        attribute = config.get("attribute") or field_name
+        listing_data = context["listing"]
+        value = self._lookup_value(listing_data, attribute)
+        if value is not None:
+            return value
+        return self._lookup_value(context, attribute)
+
+    def _generate_ai_field(
+        self,
+        field_name: str,
+        config: Dict[str, Any],
+        context: Dict[str, Any],
+        target: Dict[str, Any],
+    ) -> Any:
+        if not self.llm_client:
+            raise RuntimeError("LLM field generation requires an llm_client instance")
+
+        provider = config.get("provider")
+        model = config.get("model")
+        prompt_template = config.get("prompt_template")
+        if not provider or not model or not prompt_template:
+            raise ValueError(
+                f"AI field '{field_name}' requires provider, model, and prompt_template"
+            )
+
+        render_context = self._prepare_render_context(context, config)
+        prompt = self._render_template(field_name, prompt_template, render_context)
+        request = LLMRequest(
+            provider=provider,
+            model=model,
+            prompt=prompt,
+            field_name=field_name,
+            options=config.get("options", {}),
+            target=target,
+            listing=context.get("listing"),
+        )
+        return self.llm_client(request)
+
+    def _prepare_render_context(
+        self,
+        context: Dict[str, Any],
+        config: Dict[str, Any],
+    ) -> Dict[str, Any]:
+        render_context = dict(context)
+        render_context["listing"] = context["listing"]
+        render_context["batch"] = context["batch"]
+        tokens = dict(context.get("tokens", {}))
+        extra_tokens = config.get("tokens") or {}
+        tokens.update(extra_tokens)
+        render_context["tokens"] = tokens
+        return render_context
+
+    def _render_template(self, field_name: str, template: str, context: Dict[str, Any]) -> str:
+        try:
+            compiled = self._env.from_string(template)
+            return compiled.render(**context)
+        except TemplateError as exc:  # pragma: no cover - defensive path
+            raise ValueError(f"Failed to render prompt for '{field_name}': {exc}") from exc
+
+    @staticmethod
+    def _lookup_value(source: Any, path: str) -> Any:
+        if not isinstance(source, dict) or not path:
+            return None
+        current: Any = source
+        for segment in path.split('.'):
+            if isinstance(current, dict) and segment in current:
+                current = current[segment]
+                continue
+            return None
+        return current
+
+
 class Crawler:
     def __init__(
         self,
@@ -74,6 +246,7 @@
         cache_name: str = "crawler_cache",
         expire_after: int = DEFAULT_CACHE_EXPIRE,
         request_timeout: int = DEFAULT_TIMEOUT,
+        llm_client: Optional[LLMClient] = None,
     ) -> None:
         self.session = session or requests_cache.CachedSession(
             cache_name=cache_name,
@@ -81,6 +254,7 @@
             expire_after=expire_after,
         )
         self.request_timeout = request_timeout
+        self.field_generator = FieldGenerator(llm_client=llm_client)
 
     def run(self, config: Dict[str, Any]) -> List[CrawlerBatch]:
         batches: List[CrawlerBatch] = []
@@ -90,6 +264,8 @@
                 limit = target.get("listings_per_location")
                 if isinstance(limit, int) and limit > 0:
                     listings = listings[:limit]
+                if target.get("fields"):
+                    self._populate_fields(listings, target, location_batch)
                 meta = {
                     "search_url": location_batch["search_url"],
                     "keyword": location_batch.get("keyword"),
@@ -142,6 +318,8 @@
             title = title_node.get_text(strip=True) if title_node else ""
             url = link_node["href"].strip() if link_node and link_node.has_attr("href") else ""
             snippet = desc_node.get_text(strip=True) if desc_node else ""
+            link_text = link_node.get_text(strip=True) if link_node else ""
+            link_text = link_text or title
 
             if not title and not url:
                 continue
@@ -152,12 +330,27 @@
                     title=title,
                     url=normalized_url,
                     snippet=snippet,
-                    extras={"location": location, "keyword": keyword},
+                    extras={
+                        "location": location,
+                        "keyword": keyword,
+                        "link_text": link_text,
+                    },
                 )
             )
 
         return listings
 
+    def _populate_fields(
+        self,
+        listings: List[Listing],
+        target: Dict[str, Any],
+        batch: Dict[str, Any],
+    ) -> None:
+        if not listings:
+            return
+        for listing in listings:
+            listing.fields = self.field_generator.generate(listing, target, batch)
+
     def _iter_location_batches(
         self,
         target: Dict[str, Any],
@@ -212,6 +405,10 @@
         return urljoin(base + "/", href.lstrip("/"))
 
 
-def run_crawler(config: Dict[str, Any], session: Optional[requests.Session] = None) -> List[CrawlerBatch]:
-    crawler = Crawler(session=session)
+def run_crawler(
+    config: Dict[str, Any],
+    session: Optional[requests.Session] = None,
+    llm_client: Optional[LLMClient] = None,
+) -> List[CrawlerBatch]:
+    crawler = Crawler(session=session, llm_client=llm_client)
     return crawler.run(config)
diff --git a/agents/crawler/tests/test_crawler.py b/agents/crawler/tests/test_crawler.py
index 3c2688a7628e49fafaf7cc39a46da6deee7c6cb2..296cd1da4c2f3c1562aede3236021ad5b2a355b1
--- a/agents/crawler/tests/test_crawler.py
+++ b/agents/crawler/tests/test_crawler.py
@@ -141,3 +141,67 @@
 
     assert captured["kwargs"]["expire_after"] > 0
     assert len(batches) == 1
+
+
+def test_run_crawler_populates_ai_fields_using_jinja_tokens() -> None:
+    llm_requests = []
+
+    def fake_llm_client(request):  # type: ignore[no-untyped-def]
+        llm_requests.append(request)
+        return f"LLM::{request.field_name}::{request.prompt}"
+
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "subdomain": "electric.example.com",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "title": {"source": "scrape"},
+                    "display_link": {"source": "scrape", "attribute": "link_text"},
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.link_text }} located in {{ location }} for {{ tokens.category }}.",
+                    },
+                },
+            }
+        ],
+    }
+
+    batches = run_crawler(config, session=DummySession(SAMPLE_HTML), llm_client=fake_llm_client)
+
+    listing = batches[0].listings[0]
+    assert listing.fields["title"] == "ACME Electric"
+    assert listing.fields["display_link"] == "ACME Electric"
+    assert listing.fields["description"].startswith("LLM::description::Describe ACME Electric located in Holland MI")
+    assert llm_requests, "expected llm_client to run at least once"
+    rendered_prompt = llm_requests[0].prompt
+    assert "Electricians" in rendered_prompt
+
+
+def test_run_crawler_errors_if_ai_field_without_llm_client() -> None:
+    config = {
+        "api_endpoint": "https://api.example.com",
+        "api_token": "token",
+        "targets": [
+            {
+                "category": "Electricians",
+                "locations": ["Holland MI"],
+                "fields": {
+                    "description": {
+                        "source": "ai",
+                        "provider": "openai",
+                        "model": "gpt-4",
+                        "prompt_template": "Describe {{ listing.title }}",
+                    }
+                },
+            }
+        ],
+    }
+
+    with pytest.raises(RuntimeError):
+        run_crawler(config, session=DummySession(SAMPLE_HTML))

tokens used
93,301
**Crawler Updates**
- Added `LLMRequest` and a reusable `FieldGenerator` that renders StrictUndefined Jinja2 templates with a rich token map, validates provider/model config, and funnels requests through an injected `llm_client`, so each field can be sourced from scrape data or AI output (`agents/crawler/crawler.py:72`, `agents/crawler/crawler.py:86`).
- Listings now expose a `fields` dict alongside scraped metadata and capture each anchorâ€™s link text inside `extras`, giving prompt templates reliable tokens such as `listing.link_text` (`agents/crawler/crawler.py:45`, `agents/crawler/crawler.py:318`).
- When a target defines `fields`, the crawler now populates them per listing via `_populate_fields`, and `run_crawler` accepts an optional `llm_client`, raising a clear runtime error when AI fields are configured without one (`agents/crawler/crawler.py:260`, `agents/crawler/crawler.py:343`, `agents/crawler/crawler.py:408`).

**Tests**
- Expanded coverage with scenarios that assert Jinja-driven AI fields, scrape-backed attributes, and the failure path when no `llm_client` is supplied (`agents/crawler/tests/test_crawler.py:146`, `agents/crawler/tests/test_crawler.py:186`).
- `pytest agents/crawler/tests`

**Next Steps**
- 1) Wire a concrete `llm_client` for OpenAI/OpenRouter/Gemini that consumes `LLMRequest` so real generations can run outside tests.
- 2) Decide how the new `listing.fields` payload should be transformed when POSTing to the API (e.g., map to `summary`, `websiteUrl`, etc.) and extend the serialization accordingly.
